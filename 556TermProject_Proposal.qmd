---
title: "556TermProject_Proposal"
authors: "Callan, Abbie, and Katie - ENV H 556"
format:
  html:
    df_print: "paged"
    fig_caption: yes
    toc: true
    toc_depth: 3
    number_sections: true
    self-contained: true #save images etc. in this file (vs folders)
execute:
  echo: false
  cache: false
  echo.comments: false
  message: false
  warning: false
  
---

This document was rendered on `r format(Sys.time(), '%B %d, %Y')`.
---

## Setting up

For our term project, we aim to characterize the temporal variations of traffic-related air pollution (TRAP). Specifically, we will assess how air pollution levels vary across seasons, days of the week, or hours of the day. Our project will use data from the Mobile Monitoring Campaign, as described by [Blanco et al.](https://pubmed.ncbi.nlm.nih.gov/35917479/). 

    

```{r setup, include=FALSE}

#-----setup-----

# clear work space of all objects and unload all extra (non-base) packages
rm(list = ls(all = TRUE))
if (!is.null(sessionInfo()$otherPkgs)) {
    res <- suppressWarnings(
        lapply(paste('package:', names(sessionInfo()$otherPkgs), sep=""),
               detach, character.only=TRUE, unload=TRUE, force=TRUE))
   
}

```

```{r load.libraries.pacman, echo=FALSE, include=FALSE, eval=TRUE}

#-----load libraries pacman-----

# Load pacman into memory, installing as needed
my_repo <- 'http://cran.r-project.org'
if (!require("pacman")) {install.packages("pacman", repos = my_repo)}

# Load the other packages, installing as needed.
pacman::p_load(knitr, kableExtra, tidyverse, lubridate, egg, multcomp, modelr, broom, EnvStats, Hmisc)

```


```{r, echo=FALSE, include=FALSE}
#-----read data from a website--------

# read in annual average air pollution concentrations from mobile monitoring
# download it from the web and save locallyif the file does not already exist
annual <- read.csv(file.path("https://zenodo.org/record/13761282/files/annual_data_and_predictions.csv?download=1"))
                              
mm_covariates <- read.csv(file.path("https://zenodo.org/records/13761282/files/dr0311_mobile_covariates.csv?download=1")) %>%
  rename("location" = native_id)
  
# combine files
annual <- left_join(annual, mm_covariates, by="location")

#I don't think the annual dataset is enough for our project aims. Downloading individual stop data with collection times and joining to the covariates. 

stop_data <- read.csv(file.path("https://zenodo.org/record/13761282/files/stop_data.csv?download=1"))
stop_data <- left_join(stop_data, mm_covariates, by="location")

head(stop_data) #it's 895 columns long!! going to have to eliminate some that aren't of interest

  
```



## Introduction

*Introduction to Mobile Monitoring Data set and variables*
  The Mobile Monitoring dat set comes from a study of air pollution and its association with aging. The study collected air samples from around the greater Seattle area (Washington, United States). A car, equipped with air quality monitoring and sampling equipment, drove along nine routes, stopping and taking ~ 309 samples across all nine routes. Each sample is a two-minute sample of air quality, including pollutant concentrations for nitrogen dioxide (NO2), PM2.5, particle number concentration (PNC), carbon dioxide (CO2), and black carbon (BC). This air sample data was used to associate with ageing outcomes in study participants. Our report will not look at any health outcomes, and will focus on characterizing the temporal patterns in the air pollution data. 
  
Over the course of a year, each site along the nine driving routes was visited about 25 times [Blanco et al., 2022].

Specific Stop-level Data


*specific aims/hypotheses*

  The sampling design for the Mobile Monitoring data set included variability in the day time, week, and season that samples were taken for a particular site. This means that each sampling location has sampling data spanning across distinct days, weeks, and months. Our report will utilize the multiple layers of temporal variation in samples to assess changes in air quality across days of the week and across seasons over the year. 
  
  We hypothesize that we will see higher levels of TRAP on weekdays during typical work days (M-F). Further, we anticipate that TRAP concentrations will be lower in the summer due to reduced commuter traffic stemming from summer travels [(LA Times 1990)](https://www.latimes.com/archives/la-xpm-1990-09-04-mn-665-story.html). 


## Methods & Statistical Approach

  We will describe TRAP by season (Fall, Winter, Spring, Summer) and by day of the week using descriptive summary statistics and figures to visualize the data distributions (e.g. box plots and histograms). 

  We will use ANOVA models to compare the mean concentration of log-transformed air pollutants across (1) seasons (i.e. Fall, Winter, Spring, and Summer); and (2) days of the week. As part of our day-of-the-week assessment, we will additionally test whether TRAP is associated more broadly with day type (i.e. week day or weekend) using a land-use regression model adjusted for season and distance to major roadways, which we anticipate could be precision variables.

*Statistical Approach for regression for association*

1. We aim to create a best-fit model of NOx/TRAP pollutant to assess the association of NOx concentrations across days of the week. 

    1b. We will characterize variation in NOx/TRAP across days of the week - within week day and between weekday

2. We aim to create a best fit model of NOx TRAP pollutant to assess NOx concentrations across the four seasons of the year. 

    2b. We will characterize variation in NOx/TRAP across season - within and between season variability

*Statistical Approach for regression for Prediction*

1. We aim to create a best-fit model of NOx/TRAP pollutant to predict NOx/TRAP concentrations for each season of the year. 

    1b. We will use 10-fold cross validation to assess model performances and choose the best-fit model based on out-of-sample RMSE and MSE-based R squared. All models will have an interaction term for season. 

2. We aim to create a best-fit model of NOx/TRAP pollutant to predict NOx/TRAP concentrations for the weekend and work days each week. 

    2b. We will use 10-fold cross validation to assess model performances and choose the best-fit model based on out-of-sample RMSE and MSE-based R squared. All models will have an interaction term for weekend/workweek.


## Exploratory/ Introductory Analysis

Each variable has 309 observations with no missing data for any observations (in the annual summary data). 

**Introduction to Sampling Location Annual Average Data**

```{r, echo = FALSE, warning = FALSE, message = FALSE}

kable(annual%>%
  group_by(variable) %>%
  summarise(
    GM = geoMean(value, na.rm = TRUE),
    GSD = geoSD(value, na.rm = TRUE),
    AM = mean(value, na.rm = TRUE),
    ASD = sd(value, na.rm = TRUE)),
  digit = 1,
  caption = "Table 1.0: Summary Statistics for TRAP Variables") %>%
  kable_styling()

```
```{r}
#---Q-Q plot---#


#plotting first on a native scale
ggplot(annual, aes(sample = value)) +
  stat_qq() + 
  stat_qq_line() +
  labs(title = "Figure 1a: Normal Q-Q Plot of TRAP",
       x = "Theoretical Quantiles from a Std Normal Distribution",
       y = "Concentration - native scale"
       ) +
  facet_wrap(~variable, scales = "free")

#Co2 and PM2.5 look close to normal on the native scale, although there are some slight deviations at the tails. 
#other trap have significant deviation, mostly in the upper quantiles

#creating log transformed concentration variable. 
annual <- annual %>% mutate(logvalue = log(value), .after = value)

#qq plots using log transformed values
ggplot(annual, aes(sample = logvalue)) +
  stat_qq() + 
  stat_qq_line() +
  labs(title = "Figure 1b: Normal Q-Q Plot of log(TRAP)",
       x = "Theoretical Quantiles from a Std LogNormal Distribution",
       y = "log(concentration)"
       ) +
  facet_wrap(~variable, scales = "free")

#CO2 nad PM2.5- also looks ok lognormal
#ma200 - looks much better lognormal
#No2, ns, pmdisc, pnc - still doesn't look normal, although the deviation is less severe than before transformation

```
The Q-Q plots show that CO2 concentrations in the data are relatively normally distributed, however black carbon, NO2 and partical number concentrations all deviate from normal with high concentrations. The log transformed variables have visibly reduced deviations from a normal distribution. This leads us to conclude that all of the TRAP variables except CO2 benefit from a log transformation.


```{r}
#histograms with smoother

#native scale
ggplot(annual, aes(value)) +
  geom_histogram(aes(y = ..density..), bins = 40, color = "lightblue", fill = "lightblue", alpha = 0.8) +
  geom_density(color = "purple") +
  facet_wrap(~variable, scales = "free") +
  labs(title = "Figure 2a: Distribution of TRAP",
       x = "Concentration on native scale",
       y = "density"
       )
  
#log transformed
ggplot(annual, aes(logvalue)) +
  geom_histogram(aes(y = ..density..), bins = 40, color = "darkgreen", fill = "darkgreen", alpha = 0.4) +
  geom_density(color = "green") +
  facet_wrap(~variable, scales = "free") +
  labs(title = "Figure 2b: Distribution of log(TRAP)",
       x = "Concentration on log scale",
       y = "density"
       )

#think we should use the log transformed data. It's not perfectly normal, but it has improved the super long right tails on some of the pollutants. 

```
The Histogram plots with smoother in Figures 2a and 2b also indicate a right tailed distribution or skew in most of the TRAP variables.


**Introduction to Stop-Level Data**


``` {r}
#repeating the above analysis with the individual stop data. 
kable(stop_data%>%
  group_by(variable) %>%
  summarise(
    n = length(median_value),   #using median value because it has been more extensively QC'd and was the summary measure used in the original study. 
    nmiss = sum(is.na(median_value)), 
    GM = geoMean(median_value, na.rm = TRUE),
    GSD = geoSD(median_value, na.rm = TRUE),
    AM = mean(median_value, na.rm = TRUE),
    ASD = sd(median_value, na.rm = TRUE)),
  digit = 1)

#cannot compute GM for N02, but there are no missing values, and it can compute CM. Negative values?


print(min(stop_data$median_value)) #Yes, there are negative values in the data set. Concentrations cannot be negative - need to investigate what happened here? Were they below LOD and became negative due to some calibration correction (subtracting off a certain value?)


sub_zero <- stop_data %>% filter(median_value <= 0) #9 measurements below 0
unique(stop_data$primary_instrument) # all of them were measured on a back-up instrument. Suggest that we drop backup instrument samples from the analysis

stop_data_primary <- stop_data %>% filter(primary_instrument == "Primary")

kable(stop_data_primary%>%
  group_by(variable) %>%
  summarise(
    n = length(median_value),   #using median value because it has been more extensively QC'd and was the summary measure used in the original study. 
    nmiss = sum(is.na(median_value)), 
    GM = geoMean(median_value, na.rm = TRUE),
    GSD = geoSD(median_value, na.rm = TRUE),
    AM = mean(median_value, na.rm = TRUE),
    ASD = sd(median_value, na.rm = TRUE)),
  digit = 1,
  caption = "Table 2.0: Stop-Level Data Summary" )%>%
  kable_styling()

#much better
# the count of each variable is different, but there are no NA values 

```
``` {r}
#---Q-Q plot of stop-level data---#

#same QQ plots as above

#using a qq plot to assess normality

#plotting first on a native scale
ggplot(stop_data_primary, aes(sample = median_value)) +
  stat_qq() + 
  stat_qq_line() +
  labs(title = " Figure 3a: Normal Q-Q Plot of TRAP Stop-level data",
       x = "Theoretical Quantiles from a Std Normal Distribution",
       y = "Concentration - native scale"
       ) +
  facet_wrap(~variable, scales = "free")

#it all looks crazy. Not normal!

#creating log transformed concentration variable. 
stop_data_primary <- stop_data_primary %>% mutate(log_med_value = log(median_value), .after = median_value)

#qq plots using log transformed values
ggplot(stop_data_primary, aes(sample = log_med_value)) +
  stat_qq() + 
  stat_qq_line() +
  labs(title = " Figure 3b: Normal Q-Q Plot of log(TRAP) Stop=-level data",
       x = "Theoretical Quantiles from a Std LogNormal Distribution",
       y = "log(concentration)"
       ) +
  facet_wrap(~variable, scales = "free")

#better, but not perfect. 

#co2 still has a lot of divergence, as do No2 and ma200. The rest have some dirgence, but look ok

```


The stop-level data shows similar results as the averaged annual data across sample locations. The data benefits from log transformations and this will likely be what we use in further analysis. CO2 has slightly less normal distribution at stop-level data compared to annual data. 


```{r}
#histograms and density plots

#histograms with smoother

#native scale
ggplot(stop_data_primary, aes(median_value)) +
  geom_histogram(aes(y = ..density..), bins = 40, color = "lightblue", fill = "lightblue", alpha = 0.8) +
  geom_density(color = "purple") +
  facet_wrap(~variable, scales = "free") +
  labs(title = "Figure 4a: Distribution of TRAP Stop-level data",
       x = "Concentration on native scale",
       y = "density"
       )
#everything has a really long right tail  

#log transformed
ggplot(stop_data_primary, aes(log_med_value)) +
  geom_histogram(aes(y = ..density..), bins = 40, color = "darkgreen", fill = "darkgreen", alpha = 0.4) +
  geom_density(color = "green") +
  facet_wrap(~variable, scales = "free") +
  labs(title = "Figure 4b: Distribution of TRAP (log transformed) - stop-level data",
       x = "Concentration on log scale",
       y = "density"
       )

#much more central, although there are still some long tails. Definitely should use log values for analysis. 

```
These histograms further the point of the Q-Q plot. In our introduction of this data in the term paper we will choose either the histograms or the Q-Q plots, and either the log-transformed or native scale variables to report. 

# Seasonal Characterization of the TRAP variables concentrations:

The stop-level data provided the date and week of the year that each sample was taken. To look at seasonal changes in the stop-level TRAP data, we need to separate the weeks of the year into four seasons. We did this by breaking up the roughly 52 weeks of the year into 4, ~13-week segments. 

Weeks 13 - 25 of the year are coded as spring
weeks 26 - 38 are coded as summer
weeks 39 - 50 are coded as autumn
and week 51,52 and 1-12 are coded as winter.

**Seasonal Stratification and summary of TRAP variables**

``` {r}
#creating a new season variable to explore time trends


#changing the date variables from character to dates
stop_data_primary$date <- as.Date(stop_data_primary$date)
stop_data_primary$time <- as_datetime(stop_data_primary$time)

#creating a new season variable. Doing this roughly based off week of year, but we can make it more detailed later if we use this in our analysis
seasons <- stop_data_primary %>%
  mutate(season = case_when(
    week(time) > 12 & week(time) < 26 ~ "Spring",
    week(time) > 25 & week(time) < 39 ~ "Summer",
    week(time) > 38 & week(time) < 51 ~ "Autumn",
    week(time) < 13 | week(time) > 50 ~ "Winter"
  )) %>%
  mutate(season = factor(season, levels = c("Spring", "Summer", "Autumn", "Winter")))

#head(seasons)

#summarizing by season
seasons <- seasons %>%
  mutate(variable = as.factor(variable))

describe(seasons$variable)

kable(seasons %>%
  group_by(variable, season) %>%
  summarise(
    n = sum(!is.na(median_value)),
    GM = geoMean(median_value, na.rm = TRUE),
    GSD = geoSD(median_value, na.rm = TRUE),
    AM = mean(median_value, na.rm = TRUE),
    ASD = sd(median_value, na.rm = TRUE)),
  digit = 1,
  caption = "Table 3.0: Seasonal Summary of stop-level TRAP variables") %>%
  kable_styling()

#can run anova later (or even ancova if there are covariates we want to include), but there definitely look like seasonal differences

#making a bar plot to visualize better

ggplot(seasons, aes(x = variable, y = median_value, fill = season)) +
  geom_bar(stat = "summary", fun = "mean", position = "dodge") +
  facet_wrap(~variable, scales = "free") +
  labs(y = "mean seasonal concentration", x = "") + 
  theme(axis.text.x = element_blank())

#pnc_screen wasn't measured in winter

#table and plot are kind of rough. Can be improved for readability if we use them, but I was trying to get a sense of the data. 

#most interesting for season difference seem to be ma200, neph_bscat, no2
#Co2 is the least interesting for seasonal differences
```




# Weekly Characterization of the TRAP variables concentrations:

**Weekday Stratification and Summary of TRAP variables**

```{r}
weekdays <- stop_data_primary %>%
  mutate(week_day = weekdays(stop_data_primary$date ))

weekdays <- weekdays %>%
  mutate(week_day = as.factor(week_day))


kable(weekdays %>%
  group_by(variable, week_day) %>%
  summarise(
    n = sum(!is.na(median_value)),
    GM = geoMean(median_value, na.rm = TRUE),
    GSD = geoSD(median_value, na.rm = TRUE),
    AM = mean(median_value, na.rm = TRUE),
    ASD = sd(median_value, na.rm = TRUE)),
  digit = 1,
  caption = "Table 3.0: Weekly Summary of stop-level TRAP variables") %>%
  kable_styling()

ggplot(weekdays, aes(x = variable, y = median_value, fill = week_day)) +
  geom_bar(stat = "summary", fun = "mean", position = "dodge") +
  facet_wrap(~variable, scales = "free") +
  labs(y = "mean Weekday concentration", x = "") + 
  theme(axis.text.x = element_blank())

```

It looks like BC, PNC, NS, and NO2 may be lower on the weekends than the weekdays. This could point towards work week commuting contributions to TRAP concentrations. 



This initial Summary of the annual averages and stop-level data from the Mobile Monitoring data set leads us to a few conclusions we will use to inform out term paper:



# Regression for Prediction: Seasonal Models

1. create cross validation functions
2. cross validation for out-of-sample statistics
  for summer, fall, winter, and spring models
3. select best-fit model 
  for summer, fall, winter, and spring models
  
```{r, echo = FALSE, message = FALSE}
##--- function sot be used---#

# This is a function to get the MSE, RMSE, MSE-based R2
get_MSE <- function(obs,pred) {
    
    obs_avg <- mean(obs)

    MSE_obs <- mean((obs-obs_avg)^2)
    
    MSE_pred <- mean((obs - pred)^2)

    result <- c(RMSE = sqrt(MSE_pred),
                MSE_based_R2 = max(1 - MSE_pred / MSE_obs, 0) 
                )
    
    return(result)
}

#-----define CV function-----
# Arguments:
  # data is the data frame
  # id is the unique variable for determining sort order of data frame
  # group is the grouping variable (a variable in the data frame)
  # formula is the formula to pass to lm
  # the function returns the dataset with a new variable called cvpreds
  # appended to the end; these are the out-of-sample predictions

  # do for each cluster in the  dataset
  # (Note the use of "[[ ]]" rather than "$" because group is input in the
  # function call as a quoted variable)

do_CV <- function(data = data, id = "id", group = "group", formula) {
  
  lapply(unique(data[[group]]), function(this_group){
    
    CV_lm <- lm(formula, data = data[data[[group]] != this_group,])
    
    data[data[[group]] == this_group,] %>%
      mutate(cvpreds = predict(CV_lm, newdata = .) %>% unname())
    

  }) %>% bind_rows() %>% arrange(.data[[id]])
  
}



```

```{r}
#-----forward selection SUMMER-----#
seasons <- seasons[seasons$variable == "no2",]

seasons <- seasons%>%
  mutate("ln_no2" = log(mean_value))

summer <- seasons %>%
  filter(season == "Summer")

null <- lm(ln_no2 ~ 1, data = summer)

summer <- summer %>%
  filter(!is.na(ln_no2))

covars_all <- str_subset(names(summer),"pop_|int_|open_|D2|A1_|A23_|m_to_a1")

# B: now turn this into a formula for the full model in stepwise regression:
full <- as.formula(paste("ln_no2 ~ ", paste(covars_all, collapse= "+")))

# Using k=2 is comparable to standard AIC.
# Using log(n), where n is the number of observations, is comparable to BIC.
forwardreg_summer <- step(null, 
                        scope = list(lower = null, upper = full), 
                        trace = 0, 
                        direction = "forward", 
                        k = 0)

covars_forward <- names(forwardreg_summer$coefficients) %>%
  setdiff('(Intercept)')

covars_forward

```

```{r}

#-----model order and CV-----

# apply along length of the vector of names from forward selection
res <- lapply(seq_along(covars_forward), function(i){
    
    # define the formula, updated to add a term each time
    fmla <- as.formula(
        paste("ln_no2 ~ 1 + ", paste(covars_forward[seq_len(i)], collapse = "+"))) 
    
    # in-sample model and estimates
    in_model <- lm(fmla, data = summer) 
    
    # out-of sample model and estimates
    out_ests <- do_CV(data = summer, id = "stop_id", group = "location", fmla)
    out_results <- get_MSE(out_ests$ln_nox, out_ests$cvpreds)
    
    # compile results
    tibble(n_pred = i,
           covar = covars_forward[i],
           in_RMSE = sqrt(mean(in_model$residuals^2)),
           in_R2 = summary(in_model)$r.squared,
           out_RMSE = out_results[["RMSE"]],
           out_R2 = out_results[["MSE_based_R2"]] 
           )
    
    }) %>% 
    bind_rows()

```





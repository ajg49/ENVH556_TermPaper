---
title: "556TermProject_Proposal"
authors: "Callan, Abbie, and Katie - ENV H 556"
format:
  html:
    df_print: "paged"
    fig_caption: yes
    toc: true
    toc_depth: 3
    number_sections: true
    self-contained: true #save images etc. in this file (vs folders)
execute:
  echo: true
  cache: false
  echo.comments: false
  message: false
  warning: false
  
---

This document was rendered on `r format(Sys.time(), '%B %d, %Y')`.
---

## Setting up

For our term project, we aim to assess the temporal variations of traffic-related air pollution (TRAP). Specifically, we will assess how air pollution levels vary across seasons, days of the week, or hours of the day. Our project will use data from the Mobile Monitoring Campaign, as described by [Blanco et al.](https://pubmed.ncbi.nlm.nih.gov/35917479/). 



```{r setup, include=FALSE}

#-----setup-----

# clear work space of all objects and unload all extra (non-base) packages
rm(list = ls(all = TRUE))
if (!is.null(sessionInfo()$otherPkgs)) {
    res <- suppressWarnings(
        lapply(paste('package:', names(sessionInfo()$otherPkgs), sep=""),
               detach, character.only=TRUE, unload=TRUE, force=TRUE))
   
}

```

```{r load.libraries.pacman, echo=FALSE, include=FALSE, eval=TRUE}

#-----load libraries pacman-----

# Load pacman into memory, installing as needed
my_repo <- 'http://cran.r-project.org'
if (!require("pacman")) {install.packages("pacman", repos = my_repo)}

# Load the other packages, installing as needed.
pacman::p_load(knitr, kableExtra, tidyverse, lubridate, egg, multcomp, modelr, broom, EnvStats, Hmisc)

```


```{r}
#-----read data from a website--------

# read in annual average air pollution concentrations from mobile monitoring
# download it from the web and save locallyif the file does not already exist
annual <- read.csv(file.path("https://zenodo.org/record/13761282/files/annual_data_and_predictions.csv?download=1"))
                              
mm_covariates <- read.csv(file.path("https://zenodo.org/records/13761282/files/dr0311_mobile_covariates.csv?download=1")) %>%
  rename("location" = native_id)
  
# combine files
annual <- left_join(annual, mm_covariates, by="location")

#I don't think the annual dataset is enough for our project aims. Downloading individual stop data with collection times and joining to the covariates. 

stop_data <- read.csv(file.path("https://zenodo.org/record/13761282/files/stop_data.csv?download=1"))
stop_data <- left_join(stop_data, mm_covariates, by="location")

head(stop_data) #it's 895 columns long!! going to have to eliminate some that aren't of interest

  
```



## Introduction

*Introduction to Mobile Monitoring Data set and variables*
  The Mobile Monitoring dat set comes from a study of air pollution and its association with aging. The study collected air samples from around the greater Seattle area (Washington, United States). A car, equipped with air quality monitoring and sampling equipment, drove along nine routes, stopping and taking ~ 309 samples across all nine routes. Each sample is a two-minute sample of air quality, including pollutant concentrations for nitrogen dioxide (NO2), PM2.5, particle number concentration (PNC), carbon dioxide (CO2), and black carbon (BC). This air sample data was used to associate with ageing outcomes in study participants. Our report will not look at any health outcomes, and will focus on characterizing the temporal patterns in the air pollution data. 
  
Over the course of a year, each site along the nine driving routes was visited about 25 times [Blanco et al., 2022].

*specific aims/hypotheses*

  The sampling design for the Mobile Monitoring data set included variability in the day time, week, and season that samples were taken for a particular site. This means that each sampling location has sampling data spanning across distinct days, weeks, and months. Our report will utilize the multiple layers of temporal variation in samples to assess changes in air quality across days of the week and across seasons over the year. 
  
  We hypothesize that we will see higher levels of TRAP on weekdays during typical days (M-F). Further, we anticipate that TRAP concentrations will be lower in the summer due to reduced commuter traffic stemming from summer travels [(LA Times 1990)](https://www.latimes.com/archives/la-xpm-1990-09-04-mn-665-story.html). 


## Methods & Statistical Approach

  We will describe TRAP by season (Fall, Winter, Spring, Summer) and by day of the week using descriptive summary statistics and figures to visualize the data distributions (e.g. box plots and histograms). 

  We will use ANOVA models to compare the mean concentration of log-transformed air pollutants across (1) seasons (i.e. Fall, Winter, Spring, and Summer); and (2) days of the week. As part of our day-of-the-week assessment, we will additionally test whether TRAP is associated more broadly with day type (i.e. week day or weekend) using a land-use regression model adjusted for season and distance to major roadways, which we anticipate could be precision variables.


## Exploratory/ Introductory Analysis

Table 1.0

```{r, echo = FALSE, warning = FALSE, message = FALSE}

kable(annual%>%
  group_by(variable) %>%
  summarise(
    n = length(value),  #every value is 309, can omit and summarize in text. 
    nmiss = sum(is.na(value)), #no missing, can omit and summarize in text.
    GM = geoMean(value, na.rm = TRUE),
    GSD = geoSD(value, na.rm = TRUE),
    AM = mean(value, na.rm = TRUE),
    ASD = sd(value, na.rm = TRUE)),
  digit = 1)

```
```{r}
#---Q-Q plot---#

#using a qq plot to assess normality

#plotting first on a native scale
ggplot(annual, aes(sample = value)) +
  stat_qq() + 
  stat_qq_line() +
  labs(title = "Normal Q-Q Plot of TRAP",
       x = "Theoretical Quantiles from a Std Normal Distribution",
       y = "Concentration - native scale"
       ) +
  facet_wrap(~variable, scales = "free")

#Co2 and PM2.5 look close to normal on the native scale, although there are some slight deviations at the tails. 
#other trap have significant deviation, mostly in the upper quantiles

#creating log transformed concentration variable. 
annual <- annual %>% mutate(logvalue = log(value), .after = value)

#qq plots using log transformed values
ggplot(annual, aes(sample = logvalue)) +
  stat_qq() + 
  stat_qq_line() +
  labs(title = "Normal Q-Q Plot of TRAP",
       x = "Theoretical Quantiles from a Std LogNormal Distribution",
       y = "log(concentration)"
       ) +
  facet_wrap(~variable, scales = "free")

#CO2 nad PM2.5- also looks ok lognormal
#ma200 - looks much better lognormal
#No2, ns, pmdisc, pnc - still doesn't look normal, although the deviation is less severe than before transformation

```
```{r}
#histograms with smoother

#native scale
ggplot(annual, aes(value)) +
  geom_histogram(aes(y = ..density..), bins = 40, color = "lightblue", fill = "lightblue", alpha = 0.8) +
  geom_density(color = "purple") +
  facet_wrap(~variable, scales = "free") +
  labs(title = "Distribution of TRAP",
       x = "Concentration on native scale",
       y = "density"
       )
  
#log transformed
ggplot(annual, aes(logvalue)) +
  geom_histogram(aes(y = ..density..), bins = 40, color = "darkgreen", fill = "darkgreen", alpha = 0.4) +
  geom_density(color = "green") +
  facet_wrap(~variable, scales = "free") +
  labs(title = "Distribution of TRAP",
       x = "Concentration on log scale",
       y = "density"
       )

#think we should use the log transformed data. It's not perfectly normal, but it has improved the super long right tails on some of the pollutants. 

```
``` {r}
#repeating the above analysis with the individual stop data. 
kable(stop_data%>%
  group_by(variable) %>%
  summarise(
    n = length(median_value),   #using median value because it has been more extensively QC'd and was the summary measure used in the original study. 
    nmiss = sum(is.na(median_value)), 
    GM = geoMean(median_value, na.rm = TRUE),
    GSD = geoSD(median_value, na.rm = TRUE),
    AM = mean(median_value, na.rm = TRUE),
    ASD = sd(median_value, na.rm = TRUE)),
  digit = 1)

#cannot compute GM for N02, but there are no missing values, and it can compute CM. Negative values?

print(min(stop_data$median_value)) #Yes, there are negative values in the data set. Concentrations cannot be negative - need to investigate what happened here? Were they below LOD and became negative due to some calibration correction (subtracting off a certain value?)


sub_zero <- stop_data %>% filter(median_value <= 0) #9 measurements below 0
unique(stop_data$primary_instrument) # all of them were measured on a back-up instrument. Suggest that we drop backup instrument samples from the analysis

stop_data_primary <- stop_data %>% filter(primary_instrument == "Primary")

kable(stop_data_primary%>%
  group_by(variable) %>%
  summarise(
    n = length(median_value),   #using median value because it has been more extensively QC'd and was the summary measure used in the original study. 
    nmiss = sum(is.na(median_value)), 
    GM = geoMean(median_value, na.rm = TRUE),
    GSD = geoSD(median_value, na.rm = TRUE),
    AM = mean(median_value, na.rm = TRUE),
    ASD = sd(median_value, na.rm = TRUE)),
  digit = 1)

#much better
# the count of each variable is different, but there are no NA values 

```
``` {r}
#---Q-Q plot---#

#same QQ plots as above

#using a qq plot to assess normality

#plotting first on a native scale
ggplot(stop_data_primary, aes(sample = median_value)) +
  stat_qq() + 
  stat_qq_line() +
  labs(title = "Normal Q-Q Plot of TRAP",
       x = "Theoretical Quantiles from a Std Normal Distribution",
       y = "Concentration - native scale"
       ) +
  facet_wrap(~variable, scales = "free")

#it all looks crazy. Not normal!

#creating log transformed concentration variable. 
stop_data_primary <- stop_data_primary %>% mutate(log_med_value = log(median_value), .after = median_value)

#qq plots using log transformed values
ggplot(stop_data_primary, aes(sample = log_med_value)) +
  stat_qq() + 
  stat_qq_line() +
  labs(title = "Normal Q-Q Plot of TRAP",
       x = "Theoretical Quantiles from a Std LogNormal Distribution",
       y = "log(concentration)"
       ) +
  facet_wrap(~variable, scales = "free")

#better, but not perfect. 

#co2 still has a lot of divergence, as do No2 and ma200. The rest have some dirgence, but look ok

```
```{r}
#histograms and density plots

#histograms with smoother

#native scale
ggplot(stop_data_primary, aes(median_value)) +
  geom_histogram(aes(y = ..density..), bins = 40, color = "lightblue", fill = "lightblue", alpha = 0.8) +
  geom_density(color = "purple") +
  facet_wrap(~variable, scales = "free") +
  labs(title = "Distribution of TRAP",
       x = "Concentration on native scale",
       y = "density"
       )
#everything has a really long right tail  

#log transformed
ggplot(stop_data_primary, aes(log_med_value)) +
  geom_histogram(aes(y = ..density..), bins = 40, color = "darkgreen", fill = "darkgreen", alpha = 0.4) +
  geom_density(color = "green") +
  facet_wrap(~variable, scales = "free") +
  labs(title = "Distribution of TRAP (log transformed)",
       x = "Concentration on log scale",
       y = "density"
       )

#much more central, although there are still some long tails. Definitely should use log values for analysis. 

```
``` {r}
#creating a new season variable to explore time trends


#changing the date variables from character to dates
stop_data_primary$date <- as.Date(stop_data_primary$date)
stop_data_primary$time <- as_datetime(stop_data_primary$time)

#creating a new season variable. Doing this roughly based off week of year, but we can make it more detailed later if we use this in our analysis
seasons <- stop_data_primary %>%
  mutate(season = case_when(
    week(time) > 12 & week(time) < 26 ~ "Spring",
    week(time) > 25 & week(time) < 39 ~ "Summer",
    week(time) > 38 & week(time) < 51 ~ "Autumn",
    week(time) < 13 | week(time) > 50 ~ "Winter"
  )) %>%
  mutate(season = factor(season, levels = c("Spring", "Summer", "Autumn", "Winter")))

head(seasons)

#summarizing by season

kable(seasons%>%
  group_by(variable, season) %>%
  summarise(
    n = sum(!is.na(median_value)),
    GM = geoMean(median_value, na.rm = TRUE),
    GSD = geoSD(median_value, na.rm = TRUE),
    AM = mean(median_value, na.rm = TRUE),
    ASD = sd(median_value, na.rm = TRUE)),
  digit = 1) %>% as.factor()

#can run anova later (or even ancova if there are covariates we want to include), but there definitely look like seasonal differences

#making a bar plot to visualize better

ggplot(seasons, aes(x = variable, y = median_value, fill = season)) +
  geom_bar(stat = "summary", fun = "mean", position = "dodge") +
  facet_wrap(~variable, scales = "free") +
  labs(y = "mean seasonal concentration", x = "") + 
  theme(axis.text.x = element_blank())

#pnc_screen wasn't measured in winter

#table and plot are kind of rough. Can be improved for readability if we use them, but I was trying to get a sense of the data. 

#most interesting for season difference seem to be ma200, neph_bscat, no2
#Co2 is the least interesting for seasonal differences
```


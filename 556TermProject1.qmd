---
title: "CA NOx Term Project"
authors: "Callan, Abbie, and Katie - ENV H 556"
format:
  html:
    df_print: "paged"
    fig_caption: yes
    toc: true
    toc_depth: 3
    number_sections: true
    self-contained: true #save images etc. in this file (vs folders)
execute:
  echo: false
  cache: false
  echo.comments: false
  message: false
  warning: false
  
---

This document was rendered on `r format(Sys.time(), '%B %d, %Y')`.
---

#Introduction and Purpose

Nitrogen oxides (NOx) are gaseous compounds commonly found in air pollution. Although NOx are generated through both natural and anthropogenic sources, human activities account for the majority of ambient NOx concentrations [(EPA 2024;)](https://www.epa.gov/no2-pollution/basic-information-about-no2) [Zhang et al. 2003;](https://www.pnas.org/doi/10.1073/pnas.252763799) [(ATSDR 2002;](https://www.atsdr.cdc.gov/toxfaqs/tfacts175.pdf) [(EPA 1999)](https://www3.epa.gov/ttncatc1/dir1/fnoxdoc.pdf). Mobile vehicles are a major source of anthropogenic NOx, accounting for approximately half of all emissions related to human activities [(EPA 1999)](https://www3.epa.gov/ttncatc1/dir1/fnoxdoc.pdf). Power plants also contribute a substantial fraction (~20%) of the total estimated annual anthropogenic NOx emissions [(EPA 1999)](https://www3.epa.gov/ttncatc1/dir1/fnoxdoc.pdf).

Traffic-related air pollutants (TRAP), such as NOx, can have diurnal and seasonal trends related to temporal vehicular traffic patterns [Blanco et al. (2023)](https://www.nature.com/articles/s41370-022-00470-5#MOESM1). Measurements of TRAP must therefore appropriately reflect the relevant temporal period that they are intended to represent. For example, if exposure assessment  The aim of our analysis was to compare the performance of temporally-restricted NOx prediction models with that of a "gold standard" temporally balanced model in Los Angeles county. Specifically, we aimed to assess how prediction surfaces varied based on the temporal variability included in the underlying dataset. Ultimately, these results can be used to inform future TRAP monitoring campaigns through signaling the importance of temporally-balanced sampling designs. 

#Methods 

**Dataset Description**

The California NOx dataset, first described by [Blanco et al. (2023)](https://www.nature.com/articles/s41370-022-00470-5#MOESM1), includes NOx concentration measurements collected from 69 California Air Quality System (AQS) sites in 2016. Measurements were collected every hour, enabling the evaluation of temporal trends in NOx concentrations across both short- and long-term timescales. Land-use, roadway proximity, and population density covariates (N=321 in total)(**There are actually 849 covariates total**) were additionally available in the dataset. 

**Overview of Statistical Approach** 

We geographically restricted our analysis to data from a single county to limit the impact of geospatial variability, as NOx concentration measurements likely have strong spatial correlation at the county-level based on shared land-use, policy, and urbanicity within counties. Los Angeles County was selected as the county of interest for our analysis based on its high population density (U.S. Census 2023) and because of its high proportion of sensors included in the underlying California NOx dataset. 

The distribution of NOx concentrations and distance-based model covariates were modeled on the natural log scale based on the methods described by Blanco et al. (2022) and based on our visual inspection of the variable distributions. (**NOTE: it looks like the Blanco paper did model covariates and concentration on the natural log scale, if i am interpretingn things correctly**)

*Sensor Selection*

Sensors from the California NOx dataset were selected for inclusion in our analysis according to the criteria described by Blanco et al. (2022). These criteria included: (1) limited missingness, such that the sensors included had annual data that was at least 66% complete; (2) limited data gaps, such that data gaps for a particular sensor were less than or equal to 45 days long; (3) sensors must have sampled for at least 40% of the time during the two-week period used in our temporally restricted sampling campaigns (described below)  (**May not use this last criterion?**); and (4) monitors must have positive readings (> 0 ppb) at least 60% of the time. (Negative readings were left in the data set, as we interpreted them as true reading close to 0 that come up as negative due to noise or "classical error" in the instrument readings). Restricted the percentage of negative readings in each sensor ensure that the sensors included have adequate variability in their readings and that annual average concentrations will be positive. 

*Model Selection*

Model covariates were selected for inclusion in NOx prediction models based on their scientific relevance to ambient NOx concentration prediction models, as described in peer-reviewed scientific literature. Specifically, we chose to include the covariates utilized by [Mercer et al. (2011)](https://pmc.ncbi.nlm.nih.gov/articles/PMC3146303/pdf/nihms299256.pdf), who measured ambient NOx concentrations in Los Angeles across varied temporal layers and built prediction models using land-use and geographic covariates. Mercer et al. (2011) selected their prediction model covariates from 65 possible covariates related to population density, land-use intensity, open space land, distance to the coast, distance to industrial sources, distances to various roadways, and lengths of roadways within a given buffer zone. From these potential covariates, Mercer et al. (2011) created a "common model" that had adequate performance in predicting NOx concentrations across all seasons. The covariates included in the Mercer et al. (2011) common model included: distance to commercial pollutant sources, distance to the coast, distance to A1 roadways, population size within 5000 meters, land use intensity within 3000 square kilometers, length of A1 roads within 50 meters, and length of A2 and A3 roads within 400 meters.

For our analyses, we selected the covariates most similar to the seven used by Mercer et al. (2011), based on the similarities in geographic region, pollutant of interest, land-use covariates, and modeling goals between the present analysis and that described by Mercer et al. (2011). These covariates included: meters to closest commercial and services area, distance to the closest coastline, population density within 5,000 meters, distance to nearest A1 roadway, length of A1 roadway within 500 meters, length of A2 roadway within 1500 meters, length of A3 roadway within 400 meters, and the proportion of mixed urban of built-up land within a 1500 square kilometer buffer. Note that, in some cases, distances and buffers varied between those used by Mercer et al. (2011) and those used by Blanco et al. (2022); in these cases, the closest available covariate distance or buffer was selected. For land-use intensity, the proportion of mixed urban or built-up land within 1500 square kilometers was used in the present analysis, as it was deemed to be likely the most similar to the Mercer et al. (2011) land use covariate. 

*Gold Standard Sampling*

To make our temporally-balanced "gold standard" sampling dataset, we included all measurements from all Los Angeles County monitors that met our pre-defined inclusion criteria. The gold standard dataset therefore reflects an idealistic sampling mechanism, representing sample collection at every hour on every day in all seasons. As noted by Blanco et al. (2022), the gold standard dataset is reasonably representative of the measurements one might obtain from annual stationary air monitoring. 

We predicted the annual average NOx concentration using our gold standard sampling dataset with 10-fold cross-validation. We computed the R2 and RMSE characterizing the model fit and accuracy within this dataset. 

*Short-Term Sampling*

We created short-term sampling datasets intended to represent mobile monitoring campaigns within temporally-restricted periods. Our short-term sampling datasets included season-restricted and day-of-the-week restricted subsets of the data (*NOTE: WE CAN CHANGE THIS AS NEEDED*). For each temporally restricted period, we filtered the dataset to include only samples from the specified timeframe and randomly sampled 28 measurements. 28 measurements were selected based on findings in Blanco et al. (2022), which reported that 28 measurements could accurately estimate the annual average of a site (less than or equal to 25% error) and reasonably represented the sampling design of a single mobile monitoring campaigns, which collect repeated samples from a limited number of locations. The random sampling process was repeated 30 total times to provide additional robustness through simulating multiple mobile monitoring campaigns, as described by Blanco et al. (2022). 

For each short-term sampling dataset, we predicted the expected annual average NOx concentration with 10-fold cross-validation based on the results from all 30 sampling iterations. We additionally computed the expected R2 and RMSE based on all 30 sampling iterations. 

*Comparison of Sampling Approaches*

We compared the estimated model performance parameters (RMSE, R2) from temporally-restricted, short-term sampling with those from the gold standard, long-term estimates. We additionally compared the cross-validated annual average estimates from the temporally-restricted sampling datasets with that estimated using the gold standard sampling dataset. 

#Results

In total, 69 sensors in Los Angeles County met the specified inclusion criteria and were included in our NOx prediction analysis (**is this true? I think this is all of CA, not just LA**). **Correct, there are 69 monitors in CA that meet the criteria. 12 in LA**

```{r setup, include=FALSE}

#-----setup-----

# clear work space of all objects and unload all extra (non-base) packages
rm(list = ls(all = TRUE))
if (!is.null(sessionInfo()$otherPkgs)) {
    res <- suppressWarnings(
        lapply(paste('package:', names(sessionInfo()$otherPkgs), sep=""),
               detach, character.only=TRUE, unload=TRUE, force=TRUE))
   
}

```

```{r load.libraries.pacman, echo=FALSE, include=FALSE, eval=TRUE}

#-----load libraries pacman-----

# Load pacman into memory, installing as needed
my_repo <- 'http://cran.r-project.org'
if (!require("pacman")) {install.packages("pacman", repos = my_repo)}

# Load the other packages, installing as needed.
pacman::p_load(knitr, kableExtra, tidyverse, lubridate, egg, multcomp, modelr, broom, EnvStats, Hmisc,dplyr, tidyr, purrr, ggplot2, stringr, sf, lme4, VCA)


```


```{r read.data, echo=FALSE, include=FALSE}
#-----read data from a website--------

# create data directory if it does not exist
dir.create(file.path("Datasets"),
           showWarnings = FALSE,
           recursive = TRUE)

# read in ca nox air pollution concentrations from mobile monitoring
# download it from the web and save locallyif the file does not already exist
# read data; download if necessary
cal_nox <- read_rds(file.path("https://zenodo.org/records/14166411/files/nox_hourly.rda?download=1", 
                                 output_file_path = file.path("Datasets", "nox_hourly.rda"))) %>% rename_with(~ tolower(gsub(".","_", .x, fixed=TRUE)))

#view the data
glimpse(cal_nox)
summary(cal_nox)

length(unique(cal_nox$native_id)) #73 locations

unique(cal_nox$parameter_name) #3 different measurements NO, NO2, NOX
                              
ca_covariates <- read_rds(file.path("https://zenodo.org/records/14166411/files/site_covariates.rda?download=1"))

ca_covariates <- st_as_sf(ca_covariates, coords = c("longitude", "latitude"))

#glimpse(ca_covariates) #too long!

#going to do the attaching later because it creates such a huge df
  


```



```{r create.strata, echo =F, include = F}

cal_nox <- cal_nox %>%
  mutate(day_time = case_when(
    hour(date) > 4 & hour(date) <= 9 ~ "Morning",
    hour(date) > 9 & hour(date) <= 16 ~ "Midday",
    hour(date) > 16 & hour(date) <= 21 ~ "Evening",
    hour(date) > 21 | hour(date) <= 4 ~ "Night"
  ), .after = hour) %>%
  mutate(season = factor(season, levels = c("Morning", "Midday", "Evening", "Night")))

#I made these cuts off very rough estimates based on data distribution. We probably want to fine tune our scientific rationale for doing so and readjust if warranted. KW

#fixing the season variable because there were a lot of NA

cal_nox <- cal_nox %>%
  mutate(season = case_when(
    week(date) > 12 & week(date) <= 25 ~ "Spring",
    week(date) > 25 & week(date) <= 38 ~ "Summer",
    week(date) > 38 & week(date) <= 50 ~ "Fall",
    week(date) > 50 | week(date) <= 12 ~ "Winter"
  )) %>%
  mutate(season = factor(season, levels = c("Spring", "Summer", "Fall", "Winter")))


#summarizing by Day of the Week
cal_nox<- cal_nox %>%
  mutate(weekday2 = as.character(weekday) %>%
           str_replace(pattern = "TRUE",
                       replacement = "Weekday") %>%
         str_replace(pattern = "FALSE",
                     replacement = "Weekend"), .after = weekday)


```
#Attaching covariates to the data with all the temporal variables added. Use this if needed for model selection
```{r, add.covars}
# combine files
cal_nox_covars <- left_join(cal_nox, ca_covariates, by="native_id")
```


#Cleaning the data according to the criteria in Blanco paper

```{r cleaning.data}

#cleaning the data for sampling per the Blanco et al. paper

#create a df that is only NOX

nox_only <- cal_nox %>% filter(parameter_name == "Oxides of nitrogen (NOx)")

length(unique(nox_only$native_id)) #There are 69 monitors here, so we lost a couple when we filtered it to NOX only (there were 73 monitors when NO and NO2 were also included). 

#determining missingness
lapply(nox_only, function(i){ 
   tibble( 
          # sum missing
          n_miss = sum(is.na(i)), 
          
          # percent missing
          perc_miss = round(n_miss/length(i) * 100, 1)
          )
   }) %>% 
   # bind list
   bind_rows(.id = "variable")
# There are no NA values, but not sure if there is a complete record for each monitor

#calculate the total number of expected measurements per monitor

#check the range of the data
min(nox_only$date)
max(nox_only$date)
#The data ranges from "2016-01-01 00:00:00 PST" to "2016-12-31 23:00:00 PST"

#confirm 2016 is a leap year
leap_year(2016) # it is, and there are 366 DOY

#expected number of measurements per monitor: 366 days * 24 measurements/day
meas <- 366 *24 #expect 8784 measurements/monitor if there are no gaps for the year

meas_summary <- nox_only %>% 
  group_by(native_id) %>% 
  summarise(
    count = n(), 
    pct_total = n()/meas*100, 
    pct_pos = sum(sample_measurement > 0)/n()*100)

min(meas_summary$pct_total) #77.8% of the year
max(meas_summary$pct_total) #96.9% of the year

#all of this meets the first criteria in the Blanco paper, which is to have reading >66% of the year

#second criteria from Blanco: data gaps <= 45 days long

# Initialize a dataframe
max_gap <- data.frame(native_id=unique(nox_only$native_id), max_gap=NA)

# Loop over the monitors to calculate the maximum gap in measurements in days
for(i in 1:nrow(max_gap)){
  data.i <- subset(nox_only, native_id == max_gap$native_id[i])
  data.i <- data.i[order(data.i$doy), ]  # Ensure data is sorted by 'doy'
  
  gaps <- diff(data.i$doy)  # Calculate the differences between consecutive days
  max_gap$max_gap[i] <- max(gaps, na.rm = TRUE)  # Store the maximum gap
}

max(max_gap$max_gap) #maximum number of consecutive days w/o measurements is 36

#based on blanco criteria there is no need to eliminate any monitors

#Third Blanco criteria is that the monitor sampled for 40% of the time during the two week period used in "common design" sampling models. Will implement this later if we choose to do a 2 weeks sampling window. 

#Fourth Blanco critera is the monitor is >0 60% of the time. 

min(meas_summary$pct_pos) #70.6%
max(meas_summary$pct_pos) #100%

#no need to eliminate any monitors based on this criteria. 

#No monitors were eliminated based on this cleaning. 
```


#Function to do random sampling from the data - gold standard dataset
```{r,random.sampling fxn}
#per Blanco, we will pick 28 samples per monitor. The Gold Standard sample, will be distributed across the whole year. We will then do temporally restricted samples to see how they compare

#create factor to make subsetting in the loop easier for resample
nox_only$native_id_fact <- factor(nox_only$native_id)

size = 28 #per Blanco, the number of samples taken from each monitor
monitors = length(unique(nox_only$native_id)) #number of monitors in the data

# Initialize a dataframe to store the sample that is the same columns as the original nox_only df and enough rows for the desired sampling
sample_df <- data.frame(matrix(NA, nrow = size*monitors, ncol = ncol(nox_only)))
colnames(sample_df) <- colnames(nox_only)

set.seed(72) #seed for reproducability
# Initialize a row index for sample_df
row_index <- 1

for(i in 1:monitors) {
  data.i <- subset(nox_only, native_id_fact == unique(nox_only$native_id_fact)[i])
  sample_indices <- sample(1:nrow(data.i), size, replace = FALSE) # Take 'size' random samples from each monitor
  
  for(j in sample_indices) {
    sample_df[row_index, ] <- data.i[j, ]
    row_index <- row_index + 1
  }
}
sample_df$date <- as_datetime(sample_df$date) #make sure the date column stays in the right format in the new df. 

#head(sample_df) #Worked!

#Parameter name was a factor, so it only saved the number and not the name, but who cares (in the original df as imported before filtering 1 = NO2, 2 = NO, 3 = NOX)

#now writing it as function that will be easy to use

random_sample <- function(data, size, seed) {  #specify the data frame and the number of samples to take from each monitor
  data$native_id_fact <- factor(data$native_id)
  monitors <- length(unique(data$native_id)) # number of monitors in the data
  
  # Initialize a dataframe to store the sample that has the same columns as the original data and enough rows for the desired sampling
  sample_df <- data.frame(matrix(NA, nrow = size * monitors, ncol = ncol(data)))
  colnames(sample_df) <- colnames(data)
  
  set.seed(seed) # seed for reproducibility
  # Initialize a row index for sample_df
  row_index <- 1
  
  for(i in 1:monitors) {
    data.i <- subset(data, native_id == unique(data$native_id)[i])
    sample_indices <- sample(1:nrow(data.i), size, replace = FALSE) # Take 'size' random samples from each monitor
    
    for(j in sample_indices) {
      sample_df[row_index, ] <- data.i[j, ]
      row_index <- row_index + 1
    }
  }
  
  sample_df$date <- as_datetime(sample_df$date) # make sure the date column stays in the right format in the new df.
  
  return(sample_df)
}

test <- random_sample(nox_only, 28, 72)

#comparing test and sample_df to make sure they are the same

check = sample_df$sample_measurement - test$sample_measurement
max(check) #0
min(check) #0
#The function works. 

```

#Writing a function to aggregate data to the annual level - gold standard dataset
```{r annual.avg, echo = FALSE}

#function to take the annual average concentration of any dataframe
annual_avg <- function(data) {
  annual <- data %>% group_by(native_id) %>% summarise(annual_avg = mean(sample_measurement), log_avg = log(mean(sample_measurement)))
  return(annual)
}

#testing the function does what i want to to
test1 <- test %>% group_by(native_id) %>% summarise(annual_avg = mean(sample_measurement), log_avg = log(mean(sample_measurement)))

test2 <- annual_avg(test)

check = test1$annual_avg - test2$annual_avg
max(check) #0
min(check) #0
#The function works. 

check2 = test1$log_avg - test2$log_avg
max(check2) #0
min(check2) #0


```


#taking a bunch of random samples for the short-term balanced data
# short term random samples meant to represent a balanced mobile monitoring campaign.
```{r gold.standard}
# Initialize the short-term balanced data frame
st_balanced <- data.frame()

n_reps <- 30  # number of random df simulations to run

for (i in 1:n_reps) {
  seed <- i * 3  # generating a different seed for each simulation
  sampled_data <- random_sample(nox_only, 28, seed)
  annual_avg_data <- annual_avg(sampled_data)
  
  # Add seed and rep_number columns
  annual_avg_data$seed <- seed
  annual_avg_data$rep_number <- i
  
  # Append to gold_standard
  st_balanced <- rbind(st_balanced, annual_avg_data)
}

# Checking Abbie's Understanding:
# this chunk randomly grabs 28 samples from each of the sensors. 
# it computes an annual average based on only these 28 samples for all 69 sensors. 
# this is repeated 30 times, so there are 30  annual averages based on thirty different 
# sets of samples for each of the 69 sensors.
# katie please check if I interpreted this right!

## Yes, that is exactly right! - KW


```


#Creating Gold Standard Data - long-term and short-term balanced datasets
```{r}

#Gold Standard Data
long_term_gold <- annual_avg(nox_only)

#summary statistics
lt_gold_sum <- long_term_gold %>% summarise(min = min(annual_avg), 
                                            Q25 = quantile(annual_avg, probs = 0.25), 
                                            Q50 = quantile(annual_avg, probs = 0.5),
                                            Q75 = quantile(annual_avg, probs = 0.75),
                                            max = max(annual_avg),
                                            SD = sd(annual_avg)
                                            )
lt_gold_sum
      #This matches the number reported in Blanco supplement S5

#summary stats for st_balanced
st_balanced_sum <- st_balanced %>% summarise(min = min(annual_avg), 
                                            Q25 = quantile(annual_avg, probs = 0.25), 
                                            Q50 = quantile(annual_avg, probs = 0.5),
                                            Q75 = quantile(annual_avg, probs = 0.75),
                                            max = max(annual_avg),
                                            SD = sd(annual_avg)
                                            )
st_balanced_sum #this would not be expected to match Blanco because we used different seeds
#summarizing the data from the random samples
names <- c("Long-term", "short-term")
kable(cbind(names , rbind(lt_gold_sum, st_balanced_sum)), digits = 3,
      caption = "Balanced, Long-term & Short-term Sampling Campaign Summaries")

```



#writing a function for calculating summary statistics of an annual average df. 
```{r, summary stats}
#function to make summary stats easier
sum_stats <- function(data) {
  summary <- data %>% summarise(min = min(annual_avg), 
                                            Q25 = quantile(annual_avg, probs = 0.25), 
                                            Q50 = quantile(annual_avg, probs = 0.5),
                                            Q75 = quantile(annual_avg, probs = 0.75),
                                            max = max(annual_avg),
                                            SD = sd(annual_avg)
                                            )
  return(summary)
  
}

#testing
sum_stats(st_balanced)

#yay!!

```

```{r gold.standard.variability}
#graphing the distribution of annual averages

ggplot(st_balanced, aes(x = native_id, y = annual_avg)) +
  geom_point() +
  ggtitle("Figure 1: Gold Standard Short Term Sensor Annual Averages \n30 sets of 28 samples per sensor")

ggplot(st_balanced, aes(x = native_id, y = annual_avg)) +
  geom_boxplot()+
  ggtitle("Figure 1: Gold Standard Short Term Sensor Annual Averages \n30 sets of 28 samples per sensor")

ggplot(st_balanced, aes(x = native_id, y = log_avg)) +
  geom_boxplot()+
  ggtitle("Figure 1: Gold Standard Short Term Sensor Log-transformed Annual Averages \n30 sets of 28 samples per sensor")

#Kinda crazy how much variability there is using unrestricted random samples
#We can choose which type of plot we like best - AG

with(st_balanced, hist(annual_avg, breaks = 30, col = "blue", xlab = "NOx ppm" ))
with(st_balanced, hist(log_avg, breaks = 30, col = "seagreen", xlab = "log(NOx ppm)" ))

with(long_term_gold, hist(annual_avg, breaks = 20, col = "blue", xlab = "NOx ppm" ))
with(long_term_gold, hist(log_avg, breaks = 20, col = "seagreen", xlab = "log(NOx ppm)" ))

#makes much more sense to work with the data on the log scale


```

#Break up the histograms by season, look at NOx distribution by seasons, day of the week, etc.

```{r histogram}

#---------------------- Stratified by season----------------------------------#
#native scale
#histograms with smoother
nox_only %>%
ggplot(aes(sample_measurement)) +
  geom_histogram(aes(y = ..density..), bins = 40, color = "lightblue", fill = "lightblue", alpha = 0.8) +
  geom_density(color = "purple") +
  facet_wrap(~season, scales = "free") +
  labs(title = "Distribution of NOx by Season",
       x = "Concentration on native scale",
       y = "density"
       )

#---------stratified by day of the week ---------------------------------------#
#histograms with smoother
nox_only %>%
ggplot(aes(sample_measurement)) +
  geom_histogram(aes(y = ..density..), bins = 40, color = "lightblue", fill = "lightblue", alpha = 0.8) +
  geom_density(color = "purple") +
  facet_wrap(~dow, scales = "free") +
  labs(title = "Distribution of NOx by Day of the Week",
       x = "Concentration on native scale",
       y = "density"
       )


```

```{r, eval = F}

#log transformed by season
nox_only %>%
ggplot( aes(log(sample_measurement))) +
  geom_histogram(aes(y = ..density..), bins = 40, color = "darkgreen", fill = "darkgreen", alpha = 0.4) +
  geom_density(color = "green") +
  facet_wrap(~season, scales = "free") +
  labs(title = "Distribution of log(NOx) by Season",
       x = "Concentration on log scale",
       y = "density"
       )


  
#log transformed by day of week
nox_only %>%
ggplot( aes(log(sample_measurement))) +
  geom_histogram(aes(y = ..density..), bins = 40, color = "darkgreen", fill = "darkgreen", alpha = 0.4) +
  geom_density(color = "green") +
  facet_wrap(~dow, scales = "free") +
  labs(title = "Distribution of log(NOx) by Day of the week",
       x = "Concentration on log scale",
       y = "density"
       )


```


# Seasonal Characterization of NOx concentrations:

The data provided the date and week of the year that each sample was taken. To look at seasonal changes in the data, we need to separate the weeks of the year into four seasons. We did this by breaking up the roughly 52 weeks of the year into 4, ~13-week segments. 

    Weeks 13 - 25 of the year are coded as spring
    weeks 26 - 38 are coded as summer
    weeks 39 - 50 are coded as autumn
    and week 51,52 and 1-12 are coded as winter.
    
  ** Limitations of this approach. This is based only on diving the year into roughly equal portions based on numerical week. There is no distinct scientific or meteorological significance to the cutoff. Could look at other ways of defining season for more accuracy --- could use specific calendrical definitions, or could look at cut points based on temp or precip. Could also use social cutoffs (e.g. summer is when school is out), as any seasonal differences could be a combination of both social and meteorological phenomena.

  We are also interested in a day of the week prediction model, comparing NOx concnetrations across the days of the week. We created a variable called "dow" which indicates the days of the week that the sample was taken on.
  
  We chose to use these two strata to investigate a temporal component to NOx concentrations in the state of california because these two models portray two different kinds of temporal fluctuation on very different time scales and because the strata for these variables is relatively balanced. By looking at prediction models stratified by season, we can investigate whether seasonal weather changes or tempurature averages could play a role in NOx concentrations, and by looking at days of the week, we can see if more short-term activities have an impact of NOx concentrations. 

# Adding Covariates from MErcer et al Common Model
  The Mercer et al 2011 paper ha the following covariates determined in its "common model" with which they assessed seasonal prediction models.
 Covariates:  D2A1 (distance to A1 road), A1_50m (length of A1 road in 50 m buffer), A23_400m (length of A2 or A3 roads within 400 m buffer), Pop_5k (total population within 5,000 m buffer), D2C (Distance to the coast in meters), Int_3k (intense land use within 3,000 m), D2Comm (distance to commercial land).

```{r covariates}
# I want to confirm that these are the right covariates, the names are different than Mercer et al 2011

common_model_cov <- ca_covariates %>%
  dplyr::select("native_id", "m_to_a1", "m_to_coast", "m_to_comm","pop_s05000", "ll_a1_s00050", "ll_a2_s00400", "ll_a3_s00400", "lu_industcomm_p05000", "County", "geometry")

nox_only <- left_join(nox_only,common_model_cov, by = "native_id" )

#adding the covariates to the gold standard data to do some predictions
long_term_gold <- left_join(long_term_gold,common_model_cov, by = "native_id" )
st_balanced <- left_join(st_balanced,common_model_cov, by = "native_id" )



```
#Specify the common model
```{r}
# build regression formula

covars_common <- c("m_to_a1", "m_to_coast", "m_to_comm","pop_s05000", "ll_a1_s00050", "ll_a2_s00400", "ll_a3_s00400", "lu_industcomm_p05000")
frml <- as.formula(paste("log_avg ~", paste(covars_common, collapse = "+")) )

```
#Running the prediction model on the lt_gold 
```{r}

# in-sample predictions and fit summary
summary(lm_gold_standard <- lm(frml, data = long_term_gold))


```
#Create cross-validation groups that can be used for all the data
```{r cv}
#-----generate cross validation group for all analyses-----

# set the seed to make reproducible
set.seed(123)

# create vector of CV groups
  # first generate a vector with values 1:10 equal to the number of dataset rows
  # then randomize (with argument replace = FALSE)
CV_grp <- rep(1:10, length.out = nrow(long_term_gold)) %>% 
  sample(replace = FALSE)


# now append it to the cal_nox_clean data frame
long_term_gold <- mutate(long_term_gold, CV_grp = CV_grp)


```

**Model Prediction Functions**

```{r define get_MSE}
#-----define get_MSE function-----

# This is a function to get the MSE, RMSE, MSE-based R2
get_MSE <- function(obs,pred) {
    # obs is the outcome variable
    # pred is the prediction from a model
     
    # mean of obs
    obs_avg <- mean(obs)
    
    # MSE of obs (for R2 denominator)
    MSE_obs <- mean((obs-obs_avg)^2)
    
    # MSE of predictions
    MSE_pred <- mean((obs - pred)^2)
    
    # compile output
    result <- c(RMSE = sqrt(MSE_pred),
                MSE_based_R2 = max(1 - MSE_pred / MSE_obs, 0) 
                )
    
    # explicit return (optional)
    return(result)
}

```



```{r define.CV.function}
#-----define CV function-----

do_CV <- function(data, id = "id", group = "group", formula) {
 
  lapply(unique(data[[group]]), function(this_group){
    
    # fit the "common" model to the training set (without this group)
    CV_lm <- lm(formula, data = data[data[[group]] != this_group,])
    
    # generate predictions for this group using training model
    data[data[[group]] == this_group,] %>%
      mutate(cvpreds = predict(CV_lm, newdata = .) %>% unname())
    
    # recombine data from all clusters and sort by ID column
    # note use of ".data[[ ]]" to return the value of variable id
  }) %>% bind_rows() %>% arrange(.data[[id]])
  
  # return the dataset (the last-evaluated object is always returned by default)
}

```
#Doing CV on the gold_standard model
```{r}
gold_CV <- do_CV(long_term_gold, id = "native_id", group = "CV_grp", formula = frml)

gold_MSE <- get_MSE(gold_CV$log_avg, gold_CV$cvpreds)

gold_MSE

#RMSE = 0.49
#MSE-R2 = 0.26

#plotting the results

# get range for plot
r <- gold_CV %>% dplyr::select(log_avg, cvpreds) %>% range()


ggplot(data = gold_CV, aes(log_avg, cvpreds)) +
    geom_point(shape = "o", alpha = 0.8) +
    lims(x= r, y = r) +
    coord_fixed() +
    geom_smooth(method = 'lm', se = F, aes(color = "red")) +
    geom_abline(aes(slope = 1, intercept = 0, color = "blue")) +
    labs(title = "Model predictions vs. observed ln(NOx)\n Cross-validated ", 
         subtitle = "Gold Standard Data",
         x = "Observed ln(NOx) (ln(ppb))",
         y = "Predicted ln(NOx) (ln(ppb))",
         caption = "RMSE = 0.49 MSE-based R2 = 0.26") + 
    scale_colour_manual(name='',
                      labels = c("1:1 Line", "Best-Fit Line"), 
                      values=c("blue", "red")) +
    theme_bw()


#model we have selected doesn't give the greatest predictions, but that might be fine. We changing the covariates in the model above won't necessitate changing any of the other code. 
```
#Repeating above to compare predictions from the st_balanced to predictions from the gold standard

```{r}

#need to group the st_balanced data by "campaign" before doing regression

#apply the save CV groups as used in the gold standard model
st_balanced <- st_balanced %>% group_by(rep_number) %>% mutate(CV_grp = CV_grp) %>% ungroup()

#in sample-predictions
fitted_models = st_balanced %>% group_by(rep_number) %>% do(model = lm(frml, data = .))

#cross-validated predictions treating each sample "campaign" separately
st_balanced_CV <- st_balanced %>% group_by(rep_number) %>% do(model = do_CV(data = ., id = "native_id", group = "CV_grp", formula = frml))
#produces a df for each result

#next step is to restructure the result back into a single long_df

st_balanced_CV <- st_balanced_CV %>% rename(model_run = rep_number) #change column name so there isn't a conflict when I unnest it

st_balanced_CV <- st_balanced_CV %>%
  unnest(cols = c(model))

#appending the predictions from the gold_standard to the st_balanced_CV df for comparison
gold_preds <- gold_CV %>%
  dplyr::select("native_id", "cvpreds") %>% rename(gold_preds = cvpreds)
st_balanced_CV <- left_join(st_balanced_CV, gold_preds, by = "native_id" )


#calculating the RMSE and R2 two ways
  #a) comparing the campaign predictions to the observations from the gold standard 
  #b) comparing the predictions from each campaign to the observations from that campaign

#need to append the observations from the gold-standard to do method a

gold_obs <- gold_CV %>%
  dplyr::select("native_id", "log_avg") %>% rename(gold_obs = log_avg)
st_balanced_CV <- left_join(st_balanced_CV, gold_obs, by = "native_id" )

# Initialize empty lists to store the results for both methods
results_a <- list()
results_b <- list()

# Loop through each unique model_run
for (run in unique(st_balanced_CV$model_run)) {
  # Filter the data for the current model_run
  data_subset <- st_balanced_CV %>% filter(model_run == run)
  
  # Calculate the MSE for method A
  mse_result_a <- get_MSE(data_subset$gold_obs, data_subset$cvpreds)
  results_a[[run]] <- mse_result_a
  
  # Calculate the MSE for method B
  mse_result_b <- get_MSE(data_subset$log_avg, data_subset$cvpreds)
  results_b[[run]] <- mse_result_b
}

# Convert the lists to a data frame with separate columns for each method
st_balanced_MSE <- data.frame(
  model_run = unique(st_balanced_CV$model_run),
  RMSE_a = sapply(results_a, `[`, 1),
  R2_a = sapply(results_a, `[`, 2),
  RMSE_b = sapply(results_b, `[`, 1),
  R2_b = sapply(results_b, `[`, 2)
)

#pivot longer for plotting
st_balanced_MSE_long <- st_balanced_MSE %>%
  pivot_longer(
    cols = c(RMSE_a, R2_a, RMSE_b, R2_b),
    names_to = c("metric", "method"),
    names_sep = "_",
    values_to = "value"
  ) 

#Plotting (replicate figure 1 in blanco paper)
# get range for plot
r <- st_balanced_CV %>% dplyr::select(gold_preds, cvpreds) %>% range()


ggplot(data = st_balanced_CV, aes(x = gold_preds, y = cvpreds, color = as.factor(model_run))) +
    geom_point(shape = "o", alpha = 0.7) +
    lims(x= r, y = r) +
    coord_fixed() +
    geom_smooth(method = 'lm', se = F, alpha = 0.5) +
    geom_abline(slope = 1, intercept = 0, color = "darkgray", linetype = "dashed") +
    labs(title = "Campaign vs. Gold Standard Cross-validated Predictions", 
         subtitle = "Balanced Campaigns",
         x = "Gold Standard predicted ln(NOx) (ln(ppb))",
         y = " Campaign Predicted ln(NOx) (ln(ppb))",
         caption = "1:1 line is dashed") +
    theme_bw()


#This needs some aesthetic adjustments: choose between dots or lines, pick a color scale that's helpful, get rid of the funky legend, but it's cool!!
    #looks a lot like the balanced predictions from the Blanco paper. 

#replicate blanco figure 3
ggplot(st_balanced_MSE_long, aes(x = method, y = value, color = method)) +
  geom_boxplot() +
  geom_point(data = data.frame(method = c("a", "b"), value = gold_MSE[[1]], metric = "RMSE"), aes(x = method, y = value, color = method), size = 3, shape = 5) +
  geom_point(data = data.frame(method = c("a", "b"), value = gold_MSE[[2]], metric = "R2"), aes(x = method, y = value, color = method), size = 3, shape = 5) +
  facet_wrap(~metric) +
  labs(title = "RMSE and MSE_R2 Comparison",
       caption = "Method A = Campaign Predictions Compared to Gold Standard Obs\n
       Method B = Campaign Predictions Compared to Campaign Observations\n
       Square points shows the long-term campaigns performance") +
  theme_bw()

#This is also not bad. It will need some fancification, as well as adjustment when we start to add other campaigns, but it's an ok start. 

#Need to also add the in-sample R2, which I can do later.

#probably want to turn some of the above stuff into functions, because it is going to be a lot of cut and paste to do this for different data sets multiple times. 


```




# Season Specific Prediction Models

**Seasonal Stratification of NOx for season-specific model generation**

``` {r seasonal strat}

summer <- nox_only[nox_only$season == "Summer",]
unique(summer$native_id)

fall <- nox_only[nox_only$season == "Fall",]

winter <- nox_only[nox_only$season == "Winter",] 

spring <- nox_only[nox_only$season == "Spring",]



```


#Writing a function to aggregate data to the seasonal level -seasonal dataset
```{r annual.avg, echo = FALSE}
#function to take the annual average concentration of any dataframe
seasonal_avg <- function(data) {
  seasonal <- data %>% group_by(native_id) %>% summarise(seasonal_avg = mean(sample_measurement), log_avg = log(mean(sample_measurement)))
  return(seasonal)
}



#there are seasonal averages equal to zero, this makes the log transformed measure inf. 


```


#creating short term seasonal data sets
```{r seasonal datasets}


#random_sample() (data, size, seed)
#----------summer----------------------------------------------
# Initialize the short-term balanced data frame
st_summer <- data.frame()

n_reps <- 30  # number of random df simulations to run

for (i in 1:n_reps) {
  seed <- i * 3  # generating a different seed for each simulation
  sampled_data <- random_sample(summer, 28, seed)
  summer_avg_data <- seasonal_avg(sampled_data)
  
  # Add seed and rep_number columns
  summer_avg_data$seed <- seed
  summer_avg_data$rep_number <- i
  
  # Append to gold_standard
  st_summer <- rbind(st_summer, summer_avg_data)
}

#----------fall----------------------------------------------
# Initialize the short-term balanced data frame
st_fall <- data.frame()

n_reps <- 30  # number of random df simulations to run

for (i in 1:n_reps) {
  seed <- i * 3  # generating a different seed for each simulation
  sampled_data <- random_sample(fall, 28, seed)
  fall_avg_data <- seasonal_avg(sampled_data)
  
  # Add seed and rep_number columns
  fall_avg_data$seed <- seed
  fall_avg_data$rep_number <- i
  
  # Append to gold_standard
  st_fall <- rbind(st_fall, fall_avg_data)
}

#------------Winter--------------------------------------------
st_winter <- data.frame()

n_reps <- 30  # number of random df simulations to run

for (i in 1:n_reps) {
  seed <- i * 3  # generating a different seed for each simulation
  sampled_data <- random_sample(winter, 28, seed)
  winter_avg_data <- seasonal_avg(sampled_data)
  
  # Add seed and rep_number columns
  winter_avg_data$seed <- seed
  winter_avg_data$rep_number <- i
  
  # Append to gold_standard
  st_winter <- rbind(st_winter, winter_avg_data)
}

#------------Spring--------------------------------------------
st_spring <- data.frame()

n_reps <- 30  # number of random df simulations to run

for (i in 1:n_reps) {
  seed <- i * 3  # generating a different seed for each simulation
  sampled_data <- random_sample(spring, 28, seed)
  spring_avg_data <- seasonal_avg(sampled_data)
  
  # Add seed and rep_number columns
  spring_avg_data$seed <- seed
  spring_avg_data$rep_number <- i
  
  # Append to gold_standard
  st_spring <- rbind(st_spring, spring_avg_data)
}

# Checking Abbie's Understanding:
# this chunk randomly grabs 28 samples from the season's data, from each of the sensors. 
# it computes a seasonal average based on only these 28 samples for all 69 sensors. 
# this is repeated 30 times, so there are 30 seasonal averages based on thirty different 
# sets of samples for each of the 69 sensors.
# katie please check if I interpreted this right!

## Yes, that is exactly right! - KW\
# this should be the same length as the st_balanced dataframe
#True


```



#quick summary short-term seasonal datasets
```{r}

#summary stats for st_summer
st_summer_sum <- st_summer %>% summarise(min = min(seasonal_avg), 
                                            Q25 = quantile(seasonal_avg, probs = 0.25), 
                                            Q50 = quantile(seasonal_avg, probs = 0.5),
                                            Q75 = quantile(seasonal_avg, probs = 0.75),
                                            max = max(seasonal_avg),
                                            SD = sd(seasonal_avg)
                                            )
st_summer_sum #this would not be expected to match Blanco because we used different seeds
#summarizing the data from the random samples

st_fall_sum <- st_fall %>% summarise(min = min(seasonal_avg), 
                                            Q25 = quantile(seasonal_avg, probs = 0.25), 
                                            Q50 = quantile(seasonal_avg, probs = 0.5),
                                            Q75 = quantile(seasonal_avg, probs = 0.75),
                                            max = max(seasonal_avg),
                                            SD = sd(seasonal_avg)
                                            )
st_fall_sum #this would not be expected to match Blanco because we used different seeds
#summarizing the data from the random samples

st_winter_sum <- st_winter %>% summarise(min = min(seasonal_avg), 
                                            Q25 = quantile(seasonal_avg, probs = 0.25), 
                                            Q50 = quantile(seasonal_avg, probs = 0.5),
                                            Q75 = quantile(seasonal_avg, probs = 0.75),
                                            max = max(seasonal_avg),
                                            SD = sd(seasonal_avg)
                                            )
st_winter_sum #this would not be expected to match Blanco because we used different seeds
#summarizing the data from the random samples

st_spring_sum <- st_spring %>% summarise(min = min(seasonal_avg), 
                                            Q25 = quantile(seasonal_avg, probs = 0.25), 
                                            Q50 = quantile(seasonal_avg, probs = 0.5),
                                            Q75 = quantile(seasonal_avg, probs = 0.75),
                                            max = max(seasonal_avg),
                                            SD = sd(seasonal_avg)
                                            )
st_spring_sum #this would not be expected to match Blanco because we used different seeds
#summarizing the data from the random samples





names2 <- c("summer", "fall", "winter", "spring", "balanced")
kable(cbind(names2 , rbind(st_summer_sum, st_fall_sum, st_winter_sum, st_spring_sum, st_balanced_sum)), digits = 2,
      caption = " Short-term Sampling Campaign NOx Summaries")

```






#May not use, instead use common model covariates
CALLAN NOTE: I cannot get the below code to run and I'm not sure why.

"m_to_a1", "m_to_coast", "m_to_comm","pop_s05000", "ll_a1_s00050" , "lu_industcomm_p05000", "geometry"
```{r model selection}
#-----forward selection for full general model covariates-----

# Null model: intercept only
null <- lm(sample_measurement ~ 1, data = nox_only)

# find covariates using string matching
covars_all <- str_subset(names(nox_only), "pop_|ll_|lu_|m_to_")

# create the formula for the full model
full_formula <- as.formula(paste("sample_measurement ~ ", paste(covars_all, collapse = "+")))

# Run the forward stepwise regression using AIC (k = 2)
forwardreg_generalmod <- step(null, 
                              scope = list(lower = null, upper = full_formula), 
                              trace = 0, 
                              direction = "forward", 
                              k = 2)  # k = 2 for AIC (comparable to BIC with log(n) for large datasets)


covars_forward2 <- names(forwardreg_generalmod$coefficients) %>%
  setdiff('(Intercept)')

covars_forward2 #different list than above when just run for summer

#

```


**Summer -  Model Selection **

Using bias-variance trade off and ranked forward selection. We created random clusters on which do do cross validation.

```{r cv}
#-----generate groups-----

# set the seed to make reproducible
set.seed(123)

# create vector of CV groups
  # first generate a vector with values 1:10 equal to the number of dataset rows
  # then randomize (with argument replace = FALSE)
CV_grp <- rep(1:10, length.out = nrow(summer)) %>% 
  sample(replace = FALSE)
#Did this on the whole DF, which I think make sense if we use week_day or type_day as interaction terms in the model
#would not make sense if we end up doing seperate models for the types of days. 

# now append it to the summer data frame
summer <- mutate(summer, rando_CV_grp = CV_grp)

#-----forward selection using interaction-----#
null <- lm(sample_measurement ~ 1, data = summer)

covars_all <- covars_forward2

# B: now turn this into a formula for the full model in stepwise regression:
full <- as.formula(paste("sample_measurement ~ ", paste(covars_all, collapse= "+")))

# Using k=2 is comparable to standard AIC.
# Using log(n), where n is the number of observations, is comparable to BIC.
# remove NA's from teh dataset first
forwardreg_summer <- step(null, 
                        scope = list(lower = null, upper = full), 
                        trace = 0, 
                        direction = "forward", 
                        k = 2) #made K = 2 for true AIC calculation

covars_forward2 <- names(forwardreg_summer$coefficients) %>%
  setdiff('(Intercept)')

covars_forward2 #different list than above when just run for summer

#forward selection is what we used the the lab, but she also said it wasn't how you would normally really do it. Do we want to try backwards selection or something else?

```

#cross validation for each model and plotting bias/variance tradeoff
```{r summer CV}
#-----model order and CV-----

# apply along length of the vector of names from forward selection
res1 <- lapply(seq_along(covars_forward2), function(i){
    
    # define the formula, updated to add a term each time
    fmla <- as.formula(
        paste("sample_measurement ~ 1 + ", paste(covars_forward2[seq_len(i)], collapse = "+"))) 
    
    # in-sample model and estimates
    in_model <- lm(fmla, data = summer) 
    
    # out-of sample model and estimates
    out_ests <- do_CV(data = summer, id = "native_id", group = "rando_CV_grp", fmla) # I don't have a great sense of what the location variable represents in this data, but there probably is some spacial value to it. Also it's way more than 10 groups, and not random. KW
    out_results <- get_MSE(out_ests$sample_measurement, out_ests$cvpreds)
    
    # compile results
   tibble(n_pred = i,
           covar = covars_forward2[i],
           in_RMSE = sqrt(mean(in_model$residuals^2)),
           in_R2 = summary(in_model)$r.squared,
           out_RMSE = out_results[["RMSE"]],
           out_R2 = out_results[["MSE_based_R2"]] 
           )
    
    }) %>% 
    bind_rows()

head(res1)

```

^ from the plot and functions above we pick a summer model. repeat for fall, winter, and spring?


**General Model Cross-Validation Group** 

Callan adding: Per Lianne's feedback, I am making one cross validation group across the full dataset for use in prediction validation. 



``` {r} 
# creating a formula with temporal variables
cal_nox_clean <- cal_nox_clean %>%      #filter to only one species category
  filter(parameter_name == "Oxides of nitrogen (NOx)") %>%
  mutate(dow = as.factor(dow),
         weekday2 = as.factor(weekday2))

#CALLAN CHANGED: taking out dow and leaving just weekday2 to prevent overfitting. adding in a sensitivity analysis with dow below. 
Full <- as.formula(log_conc ~ day_time + weekday2 + season )

```



```{r}
# linear regression model of full model (weekday2)

full_temporal <- lm(Full, data = cal_nox_clean)

#---spatial distribution---
cal_nox_clean %>%
  group_by(County) %>%
  summarise(
             n = sum(!is.na(County)))

cal_nox_clean %>%
  group_by(County) %>%
  summarise(
             n = sum(!is.na(County)))


cal_nox_clean %>%
  group_by(County) %>%
  summarise(
             n = length(unique(native_id)))

#26 total counties. use this as the clustering variable? Code below makes random clusters but this could be a clustering variable

```
This plot allows you to see trends in the data without artificially stratifying the data by season, day, or hour.


# Regression for Prediction: Seasonal Models

1. create cross validation functions
2. cross validation for out-of-sample statistics
  for summer, fall, winter, and spring models
3. select best-fit model 
  for summer, fall, winter, and spring models
  
```{r, echo = FALSE, message = FALSE}

#-----manual 10-fold CV---------------------------------------------------
#--generate random groups--

# set the seed to make reproducible
set.seed(283)

# create vector of CV groups
  # first generate a vector with values 1:10 equal to the number of dataset rows
  # then randomize (with argument replace = FALSE)
CV_grp <- rep(1:10, length.out = nrow(cal_nox_clean)) %>% 
  sample(replace = FALSE)

#\ now append it to the cal_nox_clean data frame
cal_nox_clean <- mutate(cal_nox_clean, CV_grp = CV_grp)

# create a numeric variable for CV predictions (using -999 as a placeholder)
cv_pred <- cal_nox_clean %>% mutate(preds = -999)

# loop over the 10 clusters
for (i in 1:10){

    # define the current cluster variable as a logical vector
    is_cluster <- cv_pred$CV_grp == i

    # fit the "common" model to the training set by omitting cluster i
    CV_lm <- lm(Full, data = cv_pred, subset = !is_cluster)

    # generate predictions using CV_lm
    preds <- predict(CV_lm, cal_nox_clean)

    # add results to cv_pred dataframe
    cv_pred$preds[is_cluster] <- preds[is_cluster]
}

# now calculate the MSE, RMSE, MSE-based R2

# mean of observations
log_conc_avg <- mean(cv_pred$log_conc)

# MSE of predictions
MSE_pred <- mean((cv_pred$log_conc - cv_pred$preds)^2)

# MSE of observations (for R2 denominator)
MSE_obs <- mean((cv_pred$log_conc - log_conc_avg)^2)

# print the results not rounded
kable(rbind(paste("RMSE:  ", round(sqrt(MSE_pred),3)),
paste("MSE-based R2:  ", round(max(1 - MSE_pred/MSE_obs, 0),3))),
  caption = "10-fold CV with random groups") %>%
  kable_styling()


```
#These are not very good performance statistics!! Do we want to try and figure out a different model with some geographic covariates?

# ANOVA components of Variance analysis
Now lets look to see what covariates explain the most temporal variation


```{r}
#-----VCA implementation-----

# different ways to estimate variance components
fit_MOM <- anovaVCA(Full, Data = as.data.frame(cal_nox_clean) )
kable(fit_MOM$aov.tab,
      caption = "Table of MOM ANOVA",
      digits = 2) %>%
  kable_styling()

#fit_REML <- fitVCA(Full, Data = as.data.frame(cal_nox_clean), method = "REML")
#kable(fit_REML$aov.tab,
#      caption = "Table of REML ANOVA",
#      digits = 2) %>%
#  kable_styling()


```


# SENSITIVITY ANALYSIS: DAY OF THE WEEK 

```{r full model sensitivity analysis}

#creating a formula to check that the simpler version of weekday vs. weekend (rather than day of the week) is appropriate in our model: 

Full2 <-as.formula(log_conc~day_time + dow + season)

```

```{r linear regression model for dow sensitivity analysis}
#creating linear regression model for model with day of the week rather than weekend vs. weekday

full_temporal2 <- lm(Full2, data = cal_nox_clean)

```

```{r comparison of dow vs. weekend or weekday models}

#model with weekday2 covariate: 
summary(full_temporal)


#model with dow covariate: 
summary(full_temporal2)

#these appear reasonably comparable, though notably (and predictably) the day of the week model is more complex than the weekend/weekday model

#calculate AIC for both models

AIC(full_temporal)

AIC(full_temporal2)


#AIC looks to be considerably lower for the full DOW model than for the simpler model. Let's check BIC: 

BIC(full_temporal)

BIC(full_temporal2)

#BIC also looks better for the full DOW model. 


#let's check the deviance for both models:
deviance(full_temporal)

deviance(full_temporal2)

#deviance also looks better for full DOW model. 


#NOTE: could make a table with all these values (plus rmse and r2) comparing the model results. will need to decide how to balance complexity and performance. 
```

```{r repeat cross-validation for sensitivity analysis}
 
```
```{r anova for sensitivity analysis}

#-----VCA implementation-----

# different ways to estimate variance components
fit_MOM <- anovaVCA(Full2, Data = as.data.frame(cal_nox_clean) )
kable(fit_MOM$aov.tab,
      caption = "Table of MOM ANOVA",
      digits = 2) %>%
  kable_styling()


```

``` {r sensitivity analysis 2: cross-validation by county clusters}


# create vector of CV groups by county
cal_nox_clean <- cal_nox_clean %>%
  group_by(County) %>%
  mutate(CV_grp_co = cur_group_id()) %>%
  ungroup()



# create a numeric variable for CV predictions (using -999 as a placeholder)
cv_pred1 <- cal_nox_clean %>% mutate(preds = -999)

num_counties <- length(unique(cal_nox_clean$CV_grp_co))

print(num_counties)

# loop over the 10 clusters
for (i in 1:num_counties){

    # define the current cluster variable as a logical vector
    is_cluster <- cv_pred1$CV_grp_co == i

    # fit the "common" model to the training set by omitting cluster i
    CV_lm <- lm(Full, data = cv_pred1, subset = !is_cluster)

    # generate predictions using CV_lm
    preds1 <- predict(CV_lm, cal_nox_clean)

    # add results to cv_pred dataframe
    cv_pred1$preds[is_cluster] <- preds1[is_cluster]
}

# now calculate the MSE, RMSE, MSE-based R2

# mean of observations
log_conc_avg <- mean(cv_pred1$log_conc)

# MSE of predictions
MSE_pred <- mean((cv_pred1$log_conc - cv_pred1$preds)^2)

# MSE of observations (for R2 denominator)
MSE_obs <- mean((cv_pred1$log_conc - log_conc_avg)^2)

# print the results not rounded
kable(rbind(paste("RMSE:  ", round(sqrt(MSE_pred),3)),
paste("MSE-based R2:  ", round(max(1 - MSE_pred/MSE_obs, 0),3))),
  caption = "10-fold CV with random groups") %>%
  kable_styling()



```


# Discussion and Conclusion





# Works Cited 

United States Census. 2023. QuickFacts Los Angeles County, California. https://www.census.gov/quickfacts/fact/table/losangelescountycalifornia,losangelescitycalifornia,CA/PST045223 Accessed 11/30/2024





## Appendix 

#---------------------------Reference Code-------------------------------------#

# Useless chunk
This section of code shouldn't be run. Cleaning the data in a different way. Leaving it just in case there is something we want to grab out of it in the future.
```{r, useless, echo = F, include = F, message = F, eval = F}
#data is going to need a little cleaning. There are monitors with negative values recorded. 

#filter out the negative values and repeat the table. Justify that it's scientifically impossible to have neg. values on native scale.
# this table is tratified by season, and another table is stratified by week day.

#per lianne, we don't want to just ditch the negative values
#cal_nox_clean <- cal_nox %>% filter(sample_measurement > 0) #29,521 rows removed!!


#create a new variable called "log_conc" for the log transformed concentrations
#cal_nox_clean <- cal_nox_clean %>%
#  mutate(log_conc = log(sample_measurement)) %>%
#  relocate(log_conc, .after = sample_measurement)

#we have negative numbers again, but because of the log transformation

#kable(cal_nox_clean[cal_nox_clean$parameter_name == "Oxides of nitrogen (NOx)",] %>%
  group_by(season) %>%
  summarise(
    count = n(),
    min = min(sample_measurement),
    max = max(sample_measurement),
    median = median(sample_measurement),
    GM = geoMean(sample_measurement, na.rm = TRUE),
    GSD = geoSD(sample_measurement, na.rm = TRUE),
    AM = mean(sample_measurement, na.rm = TRUE),
    ASD = sd(sample_measurement, na.rm = TRUE)),
  digit = 2,
  caption = "Table 1.0: Summary Statistics for Nitric Oxides (NOx) by Season") %>%
  kable_styling()

#kable(cal_nox_clean[cal_nox_clean$parameter_name == "Oxides of nitrogen (NOx)",] %>%
  group_by(dow) %>%
  summarise(
    count = n(),
    min = min(sample_measurement),
    max = max(sample_measurement),
    median = median(sample_measurement),
    GM = geoMean(sample_measurement, na.rm = TRUE),
    GSD = geoSD(sample_measurement, na.rm = TRUE),
    AM = mean(sample_measurement, na.rm = TRUE),
    ASD = sd(sample_measurement, na.rm = TRUE)),
  digit = 2,
  caption = "Table 2.0: Summary Statistics for Nitric Oxides (NOx) by Week day") %>%
  kable_styling()
```


```{r forward select, warning = FALSE, message = FALSE}

#-----model order and CV-----

# apply along length of the vector of names from forward selection
res1 <- lapply(seq_along(covars_forward), function(i){
    
    # define the formula, updated to add a term each time
    fmla <- as.formula(
        paste("ln_no2 ~ 1 + ", paste(covars_forward[seq_len(i)], collapse = "+"))) 
    
    # in-sample model and estimates
    in_model <- lm(fmla, data = summer) 
    
    # out-of sample model and estimates
    out_ests <- do_CV(data = summer, id = "stop_id", group = "location", fmla) # I don't have a great sense of what the location variable represents in this data, but there probably is some spacial value to it. Also it's way more than 10 groups, and not random. KW
    out_results <- get_MSE(out_ests$ln_no2, out_ests$cvpreds)
    
    # compile results
   tibble(n_pred = i,
           covar = covars_forward[i],
           in_RMSE = sqrt(mean(in_model$residuals^2)),
           in_R2 = summary(in_model)$r.squared,
           out_RMSE = out_results[["RMSE"]],
           out_R2 = out_results[["MSE_based_R2"]] 
           )
    
    }) %>% 
    bind_rows()

head(res1)

#length(unique(stop_data_primary$location)). # There are 309 CV groups using this method. (basically each stop location is a group as I understand it.

```

```{r bias.plots}
#-----bias-var combined plots-----


#there are 23 terms in the model. 
max(res1$out_RMSE[1:23])
min(res1$out_RMSE[1:23])

y_lim <- 0.8 #need to find what is actually useful for out data. Set at 0.8 for now just to include everything

# create temporary dataframe for plot
temp <- res1 %>% 
  
  # make long dataframe
  pivot_longer(cols = c(ends_with("_RMSE"), ends_with("_R2")), 
               names_to = "Source_Estimate", 
               values_to = "value" ) %>%
  
  # separate the "Source" column for in and out of sample
  separate(col = Source_Estimate, into = c("Source", "Estimate") ) %>% 

  # set high RMSE values to NA, then filter out these values before plotting
  mutate(value = ifelse(Estimate == "RMSE" & value > y_lim, NA, value)) %>%
  filter(!is.na(value)) 

#plot looked to be missing some stuff, so I filled it in - KW

combined_plot <- ggplot(data = temp) +
  geom_point(aes(x = n_pred, y = value, color = Source)) +
  geom_line(aes(x = n_pred, y = value, color = Source)) +
  xlab("Model Complexity (# of terms)") +
  ylab("") +
  scale_x_continuous(breaks = c(seq(0, 63, 5))) +
  facet_wrap(~ Estimate, scales = "free_y", ncol = 1, strip.position = "right") +
  theme_bw() 

#show plot
combined_plot
```
This is good to have as a template, but we should think about the CV groups as well as if we want season spefic or interaction model. 



```{r generate.random.cv.groups KW}
#-----generate groups-----

# set the seed to make reproducible
set.seed(123)

# create vector of CV groups
  # first generate a vector with values 1:10 equal to the number of dataset rows
  # then randomize (with argument replace = FALSE)
CV_grp <- rep(1:10, length.out = nrow(no2_only)) %>% 
  sample(replace = FALSE)
#Did this on the whole DF, which I think make sense if we use week_day or type_day as interaction terms in the model
#would not make sense if we end up doing seperate models for the types of days. 

# now append it to the fall data frame
no2_only <- mutate(no2_only, rando_CV_grp = CV_grp)

```

```{r}
#KW
#repeating the above selection and  cross-validation using the random groups for week_day (still using No2, but can switch out for pM2.5 later)

#-----forward selection using interaction-----#


null <- lm(ln_no2 ~ 1*day_type, data = no2_only) #I don't actually know if you can define a null model like this for interaction terms (got this warning on the results: Warning: variable 'day_type' is absent, its contrast will be ignored)


covars_all <- str_subset(names(no2_only),"pop_|int_|open_|D2|A1_|A23_|m_to_a1")

# B: now turn this into a formula for the full model in stepwise regression:
full <- as.formula(paste("ln_no2 ~ ", paste(covars_all, collapse= "+")))

# Using k=2 is comparable to standard AIC.
# Using log(n), where n is the number of observations, is comparable to BIC.
forwardreg_day <- step(null, 
                        scope = list(lower = null, upper = full), 
                        trace = 0, 
                        direction = "forward", 
                        k = 0)

covars_forward2 <- names(forwardreg_day$coefficients) %>%
  setdiff('(Intercept)')

covars_forward2 #different list than above when just run for summer

#forward selection is what we used the the lab, but she also said it wasn't how you would normally really do it. Do we want to try backwards selection or something else?

```

```{r fit for weekdays, warning = FALSE, message = FALSE}

#-----model order and CV-----

# apply along length of the vector of names from forward selection
res2 <- lapply(seq_along(covars_forward2), function(i){
    
    # define the formula, updated to add a term each time
    fmla <- as.formula(
        paste("ln_no2 ~ + ", paste(covars_forward2[seq_len(i)], collapse = "+"))) 
    
    # in-sample model and estimates
    in_model <- lm(fmla, data = no2_only) 
    
    # out-of sample model and estimates
    out_ests <- do_CV(data = no2_only, id = "location", group = "rando_CV_grp", fmla)  #changed the id to location, because that is the unique identifier for WHERE the stop is, which is what I think we want to be able to predict. STOP ID, is unique to the actual time that they stoped at that location each time around. Maybe I'm confused here, but it seems like we want to predict by location?
    out_results <- get_MSE(out_ests$ln_no2, out_ests$cvpreds)
    
    # compile results
   tibble(n_pred = i,
           covar = covars_forward2[i],
           in_RMSE = sqrt(mean(in_model$residuals^2)),
           in_R2 = summary(in_model)$r.squared,
           out_RMSE = out_results[["RMSE"]],
           out_R2 = out_results[["MSE_based_R2"]] 
           )
    
    }) %>% 
    bind_rows()

head(res2)



```

```{r bias.plots.weekday kw}
#-----bias-var combined plots-----


#there are 23 terms in the model. 
max(res2$out_RMSE[1:23])
min(res2$out_RMSE[1:23])

y_lim <- 0.8

# create temporary dataframe for plot
temp2 <- res2 %>% 
  
  # make long dataframe
  pivot_longer(cols = c(ends_with("_RMSE"), ends_with("_R2")), 
               names_to = "Source_Estimate", 
               values_to = "value" ) %>%
  
  # separate the "Source" column for in and out of sample
  separate(col = Source_Estimate, into = c("Source", "Estimate") ) %>% 

  # set high RMSE values to NA, then filter out these values before plotting
  mutate(value = ifelse(Estimate == "RMSE" & value > y_lim, NA, value)) %>%
  filter(!is.na(value)) 


combined_plot2 <- ggplot(data = temp2) +
  geom_point(aes(x = n_pred, y = value, color = Source)) +
  geom_line(aes(x = n_pred, y = value, color = Source)) +
  xlab("Model Complexity (# of terms)") +
  ylab("") +
  labs(title = "Bias-Variance TradeOff For Randomly Cross-Validated CV Groups",
       subtitle = "Type of Day Model") +
  scale_x_continuous(breaks = c(seq(0, 63, 5))) +
  facet_wrap(~ Estimate, scales = "free_y", ncol = 1, strip.position = "right") +
  theme_bw() 

#show plot
combined_plot2
```

```{r}
#KW experimenting with backwards selection


#-----backward selection-----#


null <- lm(ln_no2 ~ 1, data = no2_only) 


covars_all <- str_subset(names(no2_only),"pop_|int_|open_|D2|A1_|A23_|m_to_a1")

# B: now turn this into a formula for the full model in stepwise regression:
full <- as.formula(paste("ln_no2 ~ ", paste(covars_all, collapse= "+")))

# Fit the full model
full_model <- lm(full, data = no2_only)

# Using k=2 is comparable to standard AIC. (which is the default here)

backward_reg <- step(full_model, direction='backward', scope=formula(full_model), trace=0)

covars_backward <- names(backward_reg$coefficients) %>%
  setdiff('(Intercept)')

covars_backward 

backward_reg$anova

backward_reg$coefficients

# I don't really know much about how to interpret this. May not be that useful in its current state. 


```


```{r appendix, results='hide'}

#*Statistical Approach for regression for association*

#1. We aim to create a best-fit model of NOx/TRAP pollutant to assess the association of NOx concentrations across days of the week. 

#1b. We will characterize variation in NOx/TRAP across days of the week - within week day and between weekday

#2. We aim to create a best fit model of NOx TRAP pollutant to assess NOx concentrations across the four seasons of the year. 

#2b. We will characterize variation in NOx/TRAP across season - within and between season variability


## Methods & Statistical Approach

#Descriptive statistics

#  We will characterize the distribution of PM2.5 concentrations according to season (Fall, Winter, Spring, Summer) and day of the week using descriptive summary statistics, box plots and/or histograms. 

 # We will use ANOVA models to compare the mean concentration of log-transformed air pollutants across (1) seasons (i.e. Fall, Winter, Spring, and Summer); and (2) days of the week. 
  

#As part of our day-of-the-week assessment, we will additionally test whether TRAP is associated more broadly with day type (i.e. week day or weekend) using a land-use regression model adjusted for season and distance to major roadways, which we anticipate could be precision variables.



```

```{r}

season_summary <- stop_data_primary %>% group_by(season, location) %>% summarise(count = length(median_value))

range(season_summary$count)

day_summary <- stop_data_primary %>% group_by(day_type, location) %>% summarise(count = length(median_value))

range(day_summary$count)

weekday_summary <- stop_data_primary %>% group_by(week_day, location) %>% summarise(count = length(median_value))

range(weekday_summary$count)


```




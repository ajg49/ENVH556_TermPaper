---
title: "CA NOx Term Project"
authors: "Callan, Abbie, and Katie - ENV H 556"
format:
  html:
    df_print: "paged"
    fig_caption: yes
    toc: true
    toc_depth: 3
    number_sections: true
    self-contained: true #save images etc. in this file (vs folders)
execute:
  echo: false
  cache: false
  echo.comments: false
  message: false
  warning: false
  
---

This document was rendered on `r format(Sys.time(), '%B %d, %Y')`.
---

#Introduction and Purpose

Nitrogen oxides (NOx) are gaseous compounds commonly found in air pollution. Although NOx are generated through both natural and anthropogenic sources, human activities account for the majority of ambient NOx concentrations [(EPA 2024;)](https://www.epa.gov/no2-pollution/basic-information-about-no2) [Zhang et al. 2003;](https://www.pnas.org/doi/10.1073/pnas.252763799) [(ATSDR 2002;](https://www.atsdr.cdc.gov/toxfaqs/tfacts175.pdf) [(EPA 1999)](https://www3.epa.gov/ttncatc1/dir1/fnoxdoc.pdf). Mobile vehicles are a major source of anthropogenic NOx, accounting for approximately half of all emissions related to human activities [(EPA 1999)](https://www3.epa.gov/ttncatc1/dir1/fnoxdoc.pdf). Power plants also contribute a substantial fraction (~20%) of the total estimated annual anthropogenic NOx emissions [(EPA 1999)](https://www3.epa.gov/ttncatc1/dir1/fnoxdoc.pdf).

Traffic-related air pollutants (TRAP), such as NOx, can have diurnal and seasonal trends related to temporal vehicular traffic patterns [Blanco et al. (2023)](https://www.nature.com/articles/s41370-022-00470-5#MOESM1).


#Methods 

*Dataset Description*

The California NOx dataset, first described by [Blanco et al. (2023)](https://www.nature.com/articles/s41370-022-00470-5#MOESM1), includes NOx concentration measurements collected from 69 California Air Quality System (AQS) sites in 2016. Measurements were collected every hour, enabling the evaluation of temporal trends in NOx concentrations across both short- and long-term timescales.


*Overview of Statistical Approach* 


-log transformed covariates based on distribution checking




**


#Results


```{r setup, include=FALSE}

#-----setup-----

# clear work space of all objects and unload all extra (non-base) packages
rm(list = ls(all = TRUE))
if (!is.null(sessionInfo()$otherPkgs)) {
    res <- suppressWarnings(
        lapply(paste('package:', names(sessionInfo()$otherPkgs), sep=""),
               detach, character.only=TRUE, unload=TRUE, force=TRUE))
   
}

```

```{r load.libraries.pacman, echo=FALSE, include=FALSE, eval=TRUE}

#-----load libraries pacman-----

# Load pacman into memory, installing as needed
my_repo <- 'http://cran.r-project.org'
if (!require("pacman")) {install.packages("pacman", repos = my_repo)}

# Load the other packages, installing as needed.
pacman::p_load(knitr, kableExtra, tidyverse, lubridate, egg, multcomp, modelr, broom, EnvStats, Hmisc,dplyr, tidyr, purrr, ggplot2, stringr, sf, lme4, VCA)


```


```{r read.data, echo=FALSE, include=FALSE}
#-----read data from a website--------

# create data directory if it does not exist
dir.create(file.path("Datasets"),
           showWarnings = FALSE,
           recursive = TRUE)

# read in ca nox air pollution concentrations from mobile monitoring
# download it from the web and save locallyif the file does not already exist
# read data; download if necessary
cal_nox <- read_rds(file.path("https://zenodo.org/records/14166411/files/nox_hourly.rda?download=1", 
                                 output_file_path = file.path("Datasets", "nox_hourly.rda"))) %>% rename_with(~ tolower(gsub(".","_", .x, fixed=TRUE)))

#view the data
glimpse(cal_nox)
summary(cal_nox)

length(unique(cal_nox$native_id)) #73 locations
                              
ca_covariates <- read_rds(file.path("https://zenodo.org/records/14166411/files/site_covariates.rda?download=1")) 

ca_covariates <- st_as_sf(ca_covariates, coords = c("longitude", "latitude"))

#glimpse(ca_covariates) #too long!
  
# combine files
cal_nox <- left_join(cal_nox, ca_covariates, by="native_id")

unique(cal_nox$parameter_name)  

#subset the cal nox dataset to only one parameter - NOx

```



```{r create.strata, echo =F, include = F}

cal_nox <- cal_nox %>%
  mutate(day_time = case_when(
    hour(date) > 4 & hour(date) <= 9 ~ "Morning",
    hour(date) > 9 & hour(date) <= 16 ~ "Midday",
    hour(date) > 16 & hour(date) <= 21 ~ "Evening",
    hour(date) > 21 | hour(date) <= 4 ~ "Night"
  ), .after = hour) %>%
  mutate(season = factor(season, levels = c("Morning", "Midday", "Evening", "Night")))

#I made these cuts off very rough estimates based on data distribution. We probably want to fine tune our scientific rationale for doing so and readjust if warranted. KW

#fixing the season variable because there were a lot of NA

cal_nox <- cal_nox %>%
  mutate(season = case_when(
    week(date) > 12 & week(date) <= 25 ~ "Spring",
    week(date) > 25 & week(date) <= 38 ~ "Summer",
    week(date) > 38 & week(date) <= 50 ~ "Fall",
    week(date) > 50 | week(date) <= 12 ~ "Winter"
  )) %>%
  mutate(season = factor(season, levels = c("Spring", "Summer", "Fall", "Winter")))


#summarizing by Day of the Week
cal_nox<- cal_nox %>%
  mutate(weekday2 = as.character(weekday) %>%
           str_replace(pattern = "TRUE",
                       replacement = "Weekday") %>%
         str_replace(pattern = "FALSE",
                     replacement = "Weekend"), .after = weekday)


```

```{r summarize Nox, echo = FALSE, warning = FALSE, message = FALSE}


#data is going to need a little cleaning. There are monitors with negative values recorded. 

#filter out the negative values and repeat the table. Justify that it's scientifically impossible to have neg. values on native scale.
# this table is tratified by season, and another table is stratified by week day.

cal_nox_clean <- cal_nox %>% filter(sample_measurement > 0) #29,521 rows removed!!


#create a new variable called "log_conc" for the log transformed concentrations
cal_nox_clean <- cal_nox_clean %>%
  mutate(log_conc = log(sample_measurement)) %>%
  relocate(log_conc, .after = sample_measurement)

#we have negative numbers again, but because of the log transformation

kable(cal_nox_clean[cal_nox_clean$parameter_name == "Oxides of nitrogen (NOx)",] %>%
  group_by(season) %>%
  summarise(
    count = n(),
    min = min(sample_measurement),
    max = max(sample_measurement),
    median = median(sample_measurement),
    GM = geoMean(sample_measurement, na.rm = TRUE),
    GSD = geoSD(sample_measurement, na.rm = TRUE),
    AM = mean(sample_measurement, na.rm = TRUE),
    ASD = sd(sample_measurement, na.rm = TRUE)),
  digit = 2,
  caption = "Table 1.0: Summary Statistics for Nitric Oxides (NOx) by Season") %>%
  kable_styling()

kable(cal_nox_clean[cal_nox_clean$parameter_name == "Oxides of nitrogen (NOx)",] %>%
  group_by(dow) %>%
  summarise(
    count = n(),
    min = min(sample_measurement),
    max = max(sample_measurement),
    median = median(sample_measurement),
    GM = geoMean(sample_measurement, na.rm = TRUE),
    GSD = geoSD(sample_measurement, na.rm = TRUE),
    AM = mean(sample_measurement, na.rm = TRUE),
    ASD = sd(sample_measurement, na.rm = TRUE)),
  digit = 2,
  caption = "Table 2.0: Summary Statistics for Nitric Oxides (NOx) by Week day") %>%
  kable_styling()



```

#Break up the histograms by season, look at NOx distribution by seasons, day of the week, etc.

```{r histogram}

#---------------------- Stratified by season----------------------------------#
#native scale
#histograms with smoother
cal_nox_clean[cal_nox_clean$parameter_name == "Oxides of nitrogen (NOx)",] %>%
ggplot(aes(sample_measurement)) +
  geom_histogram(aes(y = ..density..), bins = 40, color = "lightblue", fill = "lightblue", alpha = 0.8) +
  geom_density(color = "purple") +
  facet_wrap(~season, scales = "free") +
  labs(title = "Distribution of NOx by Season",
       x = "Concentration on native scale",
       y = "density"
       )
  
#log transformed
cal_nox_clean[cal_nox_clean$parameter_name == "Oxides of nitrogen (NOx)",] %>%
ggplot( aes(log(sample_measurement))) +
  geom_histogram(aes(y = ..density..), bins = 40, color = "darkgreen", fill = "darkgreen", alpha = 0.4) +
  geom_density(color = "green") +
  facet_wrap(~season, scales = "free") +
  labs(title = "Distribution of log(NOx) by Season",
       x = "Concentration on log scale",
       y = "density"
       )

#Log transformed is definitely better.


#---------stratified by day of the week ---------------------------------------#
#histograms with smoother
cal_nox_clean[cal_nox_clean$parameter_name == "Oxides of nitrogen (NOx)",] %>%
ggplot(aes(sample_measurement)) +
  geom_histogram(aes(y = ..density..), bins = 40, color = "lightblue", fill = "lightblue", alpha = 0.8) +
  geom_density(color = "purple") +
  facet_wrap(~dow, scales = "free") +
  labs(title = "Distribution of NOx by Day of the Week",
       x = "Concentration on native scale",
       y = "density"
       )
  
#log transformed
cal_nox_clean[cal_nox_clean$parameter_name == "Oxides of nitrogen (NOx)",] %>%
ggplot( aes(log(sample_measurement))) +
  geom_histogram(aes(y = ..density..), bins = 40, color = "darkgreen", fill = "darkgreen", alpha = 0.4) +
  geom_density(color = "green") +
  facet_wrap(~dow, scales = "free") +
  labs(title = "Distribution of log(NOx) by Day of the week",
       x = "Concentration on log scale",
       y = "density"
       )




```


# Seasonal Characterization of NOx concentrations:

The data provided the date and week of the year that each sample was taken. To look at seasonal changes in the data, we need to separate the weeks of the year into four seasons. We did this by breaking up the roughly 52 weeks of the year into 4, ~13-week segments. 

    Weeks 13 - 25 of the year are coded as spring
    weeks 26 - 38 are coded as summer
    weeks 39 - 50 are coded as autumn
    and week 51,52 and 1-12 are coded as winter.
    
  ** Limitations of this approach. This is based only on diving the year into roughly equal portions based on numerical week. There is no distinct scientific or meteorological significance to the cutoff. Could look at other ways of defining season for more accuracy --- could use specific calendrical definitions, or could look at cut points based on temp or precip. Could also use social cutoffs (e.g. summer is when school is out), as any seasonal differences could be a combination of both social and meteorological phenomena.

  We are also interested in a day of the week prediction model, comparing NOx concnetrations across the days of the week. We created a variable called "dow" which indicates the days of the week that the sample was taken on.
  
  We chose to use these two strata to investigate a temporal component to NOx concentrations in the state of california because these two models portray two different kinds of temporal fluctuation on very different time scales and because the strata for these variables is relatively balanced. By looking at prediction models stratified by season, we can investigate whether seasonal weather changes or tempurature averages could play a role in NOx concentrations, and by looking at days of the week, we can see if more short-term activities have an impact of NOx concentrations. 

**Week day Summary of NOx**

```{r weeklytrends, echo = F}

#plotting by time

cal_nox_clean %>% filter(parameter_name == "Oxides of nitrogen (NOx)") %>% ggplot(aes(x = as.factor(dow), y = log_conc)) +
         geom_boxplot() +
  ggtitle( label = "Ln(NOx) Distributions Across the Days of the Week") +
  xlab("Day of the week") +
  ylab("log(NOx) log(ppm)")

```

# Season Specific Prediction Models

**Seasonal Stratification of NOx for season-specific model generation**

``` {r seasonal strat}

summer <- cal_nox_clean[cal_nox_clean$season == "Summer",]
fall <- cal_nox_clean[cal_nox_clean$season == "Fall",]
winter <- cal_nox_clean[cal_nox_clean$season == "Winter",]
spring <- cal_nox_clean[cal_nox_clean$season == "Spring",]


```


**Model Prediction Functions**

```{r define get_MSE}
#-----define get_MSE function-----

# This is a function to get the MSE, RMSE, MSE-based R2
get_MSE <- function(obs,pred) {
    # obs is the outcome variable
    # pred is the prediction from a model
     
    # mean of obs
    obs_avg <- mean(obs)
    
    # MSE of obs (for R2 denominator)
    MSE_obs <- mean((obs-obs_avg)^2)
    
    # MSE of predictions
    MSE_pred <- mean((obs - pred)^2)
    
    # compile output
    result <- c(RMSE = sqrt(MSE_pred),
                MSE_based_R2 = max(1 - MSE_pred / MSE_obs, 0) 
                )
    
    # explicit return (optional)
    return(result)
}

```



```{r define.CV.function}
#-----define CV function-----

do_CV <- function(data, id = "id", group = "group", formula) {
 
  lapply(unique(data[[group]]), function(this_group){
    
    # fit the "common" model to the training set (without this group)
    CV_lm <- lm(formula, data = data[data[[group]] != this_group,])
    
    # generate predictions for this group using training model
    data[data[[group]] == this_group,] %>%
      mutate(cvpreds = predict(CV_lm, newdata = .) %>% unname())
    
    # recombine data from all clusters and sort by ID column
    # note use of ".data[[ ]]" to return the value of variable id
  }) %>% bind_rows() %>% arrange(.data[[id]])
  
  # return the dataset (the last-evaluated object is always returned by default)
}

```

**General Model Selection** 

Callan adding: here I am using forward selection to pick covariates for the "general model" that we can use across all seasons. 

```{r cv}
#-----generate cross validation group for all analyses-----

# set the seed to make reproducible
set.seed(123)

# create vector of CV groups
  # first generate a vector with values 1:10 equal to the number of dataset rows
  # then randomize (with argument replace = FALSE)
CV_grp <- rep(1:10, length.out = nrow(cal_nox_clean)) %>% 
  sample(replace = FALSE)


# now append it to the cal_nox_clean data frame
cal_nox_clean <- mutate(cal_nox_clean, rando_CV_grp = CV_grp)

```


CALLAN NOTE: I cannot get the below code to run and I'm not sure why.
```{r model selection}
#-----forward selection for full general model covariates-----

cal_nox_clean <- cal_nox_clean %>% 
  dplyr::select(-m_to_6oil, -m_to_oil, -m_to_main_cityhall, -m_to_local_cityhall)


# Null model: intercept only
null <- lm(log_conc ~ 1, data = cal_nox_clean)

# find covariates using string matching
covars_all <- str_subset(names(cal_nox_clean), "pop_|int_|open_|D2|A1_|A23_|m_to_")

# create the formula for the full model
full_formula <- as.formula(paste("log_conc ~ ", paste(covars_all, collapse = "+")))

# Run the forward stepwise regression using AIC (k = 2)
forwardreg_generalmod <- step(null, 
                              scope = list(lower = null, upper = full_formula), 
                              trace = 0, 
                              direction = "forward", 
                              k = 2)  # k = 2 for AIC (comparable to BIC with log(n) for large datasets)


covars_forward2 <- names(forwardreg_generalmod$coefficients) %>%
  setdiff('(Intercept)')

covars_forward2 #different list than above when just run for summer

#

```

**General Model Cross-Validation Group** 

Callan adding: Per Lianne's feedback, I am making one cross validation group across the full dataset for use in prediction validation. 








**Summer -  Model Selection **

Using bias-variance trade off and ranked forward selection. We created random clusters on which do do cross validation.

```{r}

#------remove variables with NA's--------------
summer <- summer %>% 
  dplyr::select(-m_to_6oil, -m_to_oil, -m_to_main_cityhall, -m_to_local_cityhall)

#describe(summer)
#need to remove NA's
```

```{r cv}
#-----generate groups-----

# set the seed to make reproducible
set.seed(123)

# create vector of CV groups
  # first generate a vector with values 1:10 equal to the number of dataset rows
  # then randomize (with argument replace = FALSE)
CV_grp <- rep(1:10, length.out = nrow(summer)) %>% 
  sample(replace = FALSE)
#Did this on the whole DF, which I think make sense if we use week_day or type_day as interaction terms in the model
#would not make sense if we end up doing seperate models for the types of days. 

# now append it to the summer data frame
summer <- mutate(summer, rando_CV_grp = CV_grp)

#-----forward selection using interaction-----#
null <- lm(log_conc ~ 1, data = summer)

covars_all <- str_subset(names(summer),"pop_|int_|open_|D2|A1_|A23_|m_to_")

# B: now turn this into a formula for the full model in stepwise regression:
full <- as.formula(paste("log_conc ~ ", paste(covars_all, collapse= "+")))

# Using k=2 is comparable to standard AIC.
# Using log(n), where n is the number of observations, is comparable to BIC.
# remove NA's from teh dataset first
forwardreg_summer <- step(null, 
                        scope = list(lower = null, upper = full), 
                        trace = 0, 
                        direction = "forward", 
                        k = 2) #made K = 2 for true AIC calculation

covars_forward2 <- names(forwardreg_day$coefficients) %>%
  setdiff('(Intercept)')

covars_forward2 #different list than above when just run for summer

#forward selection is what we used the the lab, but she also said it wasn't how you would normally really do it. Do we want to try backwards selection or something else?

```

#cross validation for each model and plotting bias/variance tradeoff
```{r}
#-----model order and CV-----

# apply along length of the vector of names from forward selection
res1 <- lapply(seq_along(covars_forward), function(i){
    
    # define the formula, updated to add a term each time
    fmla <- as.formula(
        paste("log_conc ~ 1 + ", paste(covars_forward[seq_len(i)], collapse = "+"))) 
    
    # in-sample model and estimates
    in_model <- lm(fmla, data = summer) 
    
    # out-of sample model and estimates
    out_ests <- do_CV(data = summer, id = "stop_id", group = "location", fmla) # I don't have a great sense of what the location variable represents in this data, but there probably is some spacial value to it. Also it's way more than 10 groups, and not random. KW
    out_results <- get_MSE(out_ests$ln_no2, out_ests$cvpreds)
    
    # compile results
   tibble(n_pred = i,
           covar = covars_forward[i],
           in_RMSE = sqrt(mean(in_model$residuals^2)),
           in_R2 = summary(in_model)$r.squared,
           out_RMSE = out_results[["RMSE"]],
           out_R2 = out_results[["MSE_based_R2"]] 
           )
    
    }) %>% 
    bind_rows()

head(res1)

#-----bias-var combined plots-----


#there are 23 terms in the model. 
max(res2$out_RMSE[1:23])
min(res2$out_RMSE[1:23])

y_lim <- 0.8

# create temporary dataframe for plot
temp2 <- res2 %>% 
  
  # make long dataframe
  pivot_longer(cols = c(ends_with("_RMSE"), ends_with("_R2")), 
               names_to = "Source_Estimate", 
               values_to = "value" ) %>%
  
  # separate the "Source" column for in and out of sample
  separate(col = Source_Estimate, into = c("Source", "Estimate") ) %>% 

  # set high RMSE values to NA, then filter out these values before plotting
  mutate(value = ifelse(Estimate == "RMSE" & value > y_lim, NA, value)) %>%
  filter(!is.na(value)) 


combined_plot2 <- ggplot(data = temp2) +
  geom_point(aes(x = n_pred, y = value, color = Source)) +
  geom_line(aes(x = n_pred, y = value, color = Source)) +
  xlab("Model Complexity (# of terms)") +
  ylab("") +
  labs(title = "Bias-Variance TradeOff For Randomly Cross-Validated CV Groups from Summer Data",
       subtitle = "Type of Day Model") +
  scale_x_continuous(breaks = c(seq(0, 63, 5))) +
  facet_wrap(~ Estimate, scales = "free_y", ncol = 1, strip.position = "right") +
  theme_bw() 

#show plot
combined_plot2
```

^ from the plot and functions above we pick a summer model. repeat for fall, winter, and spring?




``` {r} 
# creating a formula with temporal variables
cal_nox_clean <- cal_nox_clean %>%      #filter to only one species category
  filter(parameter_name == "Oxides of nitrogen (NOx)") %>%
  mutate(dow = as.factor(dow),
         weekday2 = as.factor(weekday2))

#CALLAN CHANGED: taking out dow and leaving just weekday2 to prevent overfitting. adding in a sensitivity analysis with dow below. 
Full <- as.formula(log_conc ~ day_time + weekday2 + season )

```



```{r}
# linear regression model of full model (weekday2)

full_temporal <- lm(Full, data = cal_nox_clean)

#---spatial distribution---
cal_nox_clean %>%
  group_by(County) %>%
  summarise(
             n = sum(!is.na(County)))

#26 total counties. use this as the clustering variable? Code below makes random clusters but this could be a clustering variable

```
This plot allows you to see trends in the data without artificially stratifying the data by season, day, or hour.


# Regression for Prediction: Seasonal Models

1. create cross validation functions
2. cross validation for out-of-sample statistics
  for summer, fall, winter, and spring models
3. select best-fit model 
  for summer, fall, winter, and spring models
  
```{r, echo = FALSE, message = FALSE}

#-----manual 10-fold CV---------------------------------------------------
#--generate random groups--

# set the seed to make reproducible
set.seed(283)

# create vector of CV groups
  # first generate a vector with values 1:10 equal to the number of dataset rows
  # then randomize (with argument replace = FALSE)
CV_grp <- rep(1:10, length.out = nrow(cal_nox_clean)) %>% 
  sample(replace = FALSE)

#\ now append it to the cal_nox_clean data frame
cal_nox_clean <- mutate(cal_nox_clean, CV_grp = CV_grp)

# create a numeric variable for CV predictions (using -999 as a placeholder)
cv_pred <- cal_nox_clean %>% mutate(preds = -999)

# loop over the 10 clusters
for (i in 1:10){

    # define the current cluster variable as a logical vector
    is_cluster <- cv_pred$CV_grp == i

    # fit the "common" model to the training set by omitting cluster i
    CV_lm <- lm(Full, data = cv_pred, subset = !is_cluster)

    # generate predictions using CV_lm
    preds <- predict(CV_lm, cal_nox_clean)

    # add results to cv_pred dataframe
    cv_pred$preds[is_cluster] <- preds[is_cluster]
}

# now calculate the MSE, RMSE, MSE-based R2

# mean of observations
log_conc_avg <- mean(cv_pred$log_conc)

# MSE of predictions
MSE_pred <- mean((cv_pred$log_conc - cv_pred$preds)^2)

# MSE of observations (for R2 denominator)
MSE_obs <- mean((cv_pred$log_conc - log_conc_avg)^2)

# print the results not rounded
kable(rbind(paste("RMSE:  ", round(sqrt(MSE_pred),3)),
paste("MSE-based R2:  ", round(max(1 - MSE_pred/MSE_obs, 0),3))),
  caption = "10-fold CV with random groups") %>%
  kable_styling()


```
#These are not very good performance statistics!! Do we want to try and figure out a different model with some geographic covariates?

# ANOVA components of Variance analysis
Now lets look to see what covariates explain the most temporal variation


```{r}
#-----VCA implementation-----

# different ways to estimate variance components
fit_MOM <- anovaVCA(Full, Data = as.data.frame(cal_nox_clean) )
kable(fit_MOM$aov.tab,
      caption = "Table of MOM ANOVA",
      digits = 2) %>%
  kable_styling()

#fit_REML <- fitVCA(Full, Data = as.data.frame(cal_nox_clean), method = "REML")
#kable(fit_REML$aov.tab,
#      caption = "Table of REML ANOVA",
#      digits = 2) %>%
#  kable_styling()


```


# SENSITIVITY ANALYSIS: DAY OF THE WEEK 

```{r full model sensitivity analysis}

#creating a formula to check that the simpler version of weekday vs. weekend (rather than day of the week) is appropriate in our model: 

Full2 <-as.formula(log_conc~day_time + dow + season)

```

```{r linear regression model for dow sensitivity analysis}
#creating linear regression model for model with day of the week rather than weekend vs. weekday

full_temporal2 <- lm(Full2, data = cal_nox_clean)

```

```{r comparison of dow vs. weekend or weekday models}

#model with weekday2 covariate: 
summary(full_temporal)


#model with dow covariate: 
summary(full_temporal2)

#these appear reasonably comparable, though notably (and predictably) the day of the week model is more complex than the weekend/weekday model

#calculate AIC for both models

AIC(full_temporal)

AIC(full_temporal2)


#AIC looks to be considerably lower for the full DOW model than for the simpler model. Let's check BIC: 

BIC(full_temporal)

BIC(full_temporal2)

#BIC also looks better for the full DOW model. 


#let's check the deviance for both models:
deviance(full_temporal)

deviance(full_temporal2)

#deviance also looks better for full DOW model. 


#NOTE: could make a table with all these values (plus rmse and r2) comparing the model results. will need to decide how to balance complexity and performance. 
```

```{r repeat cross-validation for sensitivity analysis}
 
```
```{r anova for sensitivity analysis}

#-----VCA implementation-----

# different ways to estimate variance components
fit_MOM <- anovaVCA(Full2, Data = as.data.frame(cal_nox_clean) )
kable(fit_MOM$aov.tab,
      caption = "Table of MOM ANOVA",
      digits = 2) %>%
  kable_styling()


```

``` {r sensitivity analysis 2: cross-validation by county clusters}


# create vector of CV groups by county
cal_nox_clean <- cal_nox_clean %>%
  group_by(County) %>%
  mutate(CV_grp_co = cur_group_id()) %>%
  ungroup()



# create a numeric variable for CV predictions (using -999 as a placeholder)
cv_pred1 <- cal_nox_clean %>% mutate(preds = -999)

num_counties <- length(unique(cal_nox_clean$CV_grp_co))

print(num_counties)

# loop over the 10 clusters
for (i in 1:num_counties){

    # define the current cluster variable as a logical vector
    is_cluster <- cv_pred1$CV_grp_co == i

    # fit the "common" model to the training set by omitting cluster i
    CV_lm <- lm(Full, data = cv_pred1, subset = !is_cluster)

    # generate predictions using CV_lm
    preds1 <- predict(CV_lm, cal_nox_clean)

    # add results to cv_pred dataframe
    cv_pred1$preds[is_cluster] <- preds1[is_cluster]
}

# now calculate the MSE, RMSE, MSE-based R2

# mean of observations
log_conc_avg <- mean(cv_pred1$log_conc)

# MSE of predictions
MSE_pred <- mean((cv_pred1$log_conc - cv_pred1$preds)^2)

# MSE of observations (for R2 denominator)
MSE_obs <- mean((cv_pred1$log_conc - log_conc_avg)^2)

# print the results not rounded
kable(rbind(paste("RMSE:  ", round(sqrt(MSE_pred),3)),
paste("MSE-based R2:  ", round(max(1 - MSE_pred/MSE_obs, 0),3))),
  caption = "10-fold CV with random groups") %>%
  kable_styling()



```


# Discussion and Conclusion





# Works Cited 







## Appendix 

#---------------------------Reference Code-------------------------------------#

```{r forward select, warning = FALSE, message = FALSE}

#-----model order and CV-----

# apply along length of the vector of names from forward selection
res1 <- lapply(seq_along(covars_forward), function(i){
    
    # define the formula, updated to add a term each time
    fmla <- as.formula(
        paste("ln_no2 ~ 1 + ", paste(covars_forward[seq_len(i)], collapse = "+"))) 
    
    # in-sample model and estimates
    in_model <- lm(fmla, data = summer) 
    
    # out-of sample model and estimates
    out_ests <- do_CV(data = summer, id = "stop_id", group = "location", fmla) # I don't have a great sense of what the location variable represents in this data, but there probably is some spacial value to it. Also it's way more than 10 groups, and not random. KW
    out_results <- get_MSE(out_ests$ln_no2, out_ests$cvpreds)
    
    # compile results
   tibble(n_pred = i,
           covar = covars_forward[i],
           in_RMSE = sqrt(mean(in_model$residuals^2)),
           in_R2 = summary(in_model)$r.squared,
           out_RMSE = out_results[["RMSE"]],
           out_R2 = out_results[["MSE_based_R2"]] 
           )
    
    }) %>% 
    bind_rows()

head(res1)

#length(unique(stop_data_primary$location)). # There are 309 CV groups using this method. (basically each stop location is a group as I understand it.

```

```{r bias.plots}
#-----bias-var combined plots-----


#there are 23 terms in the model. 
max(res1$out_RMSE[1:23])
min(res1$out_RMSE[1:23])

y_lim <- 0.8 #need to find what is actually useful for out data. Set at 0.8 for now just to include everything

# create temporary dataframe for plot
temp <- res1 %>% 
  
  # make long dataframe
  pivot_longer(cols = c(ends_with("_RMSE"), ends_with("_R2")), 
               names_to = "Source_Estimate", 
               values_to = "value" ) %>%
  
  # separate the "Source" column for in and out of sample
  separate(col = Source_Estimate, into = c("Source", "Estimate") ) %>% 

  # set high RMSE values to NA, then filter out these values before plotting
  mutate(value = ifelse(Estimate == "RMSE" & value > y_lim, NA, value)) %>%
  filter(!is.na(value)) 

#plot looked to be missing some stuff, so I filled it in - KW

combined_plot <- ggplot(data = temp) +
  geom_point(aes(x = n_pred, y = value, color = Source)) +
  geom_line(aes(x = n_pred, y = value, color = Source)) +
  xlab("Model Complexity (# of terms)") +
  ylab("") +
  scale_x_continuous(breaks = c(seq(0, 63, 5))) +
  facet_wrap(~ Estimate, scales = "free_y", ncol = 1, strip.position = "right") +
  theme_bw() 

#show plot
combined_plot
```
This is good to have as a template, but we should think about the CV groups as well as if we want season spefic or interaction model. 



```{r generate.random.cv.groups KW}
#-----generate groups-----

# set the seed to make reproducible
set.seed(123)

# create vector of CV groups
  # first generate a vector with values 1:10 equal to the number of dataset rows
  # then randomize (with argument replace = FALSE)
CV_grp <- rep(1:10, length.out = nrow(no2_only)) %>% 
  sample(replace = FALSE)
#Did this on the whole DF, which I think make sense if we use week_day or type_day as interaction terms in the model
#would not make sense if we end up doing seperate models for the types of days. 

# now append it to the fall data frame
no2_only <- mutate(no2_only, rando_CV_grp = CV_grp)

```

```{r}
#KW
#repeating the above selection and  cross-validation using the random groups for week_day (still using No2, but can switch out for pM2.5 later)

#-----forward selection using interaction-----#


null <- lm(ln_no2 ~ 1*day_type, data = no2_only) #I don't actually know if you can define a null model like this for interaction terms (got this warning on the results: Warning: variable 'day_type' is absent, its contrast will be ignored)


covars_all <- str_subset(names(no2_only),"pop_|int_|open_|D2|A1_|A23_|m_to_a1")

# B: now turn this into a formula for the full model in stepwise regression:
full <- as.formula(paste("ln_no2 ~ ", paste(covars_all, collapse= "+")))

# Using k=2 is comparable to standard AIC.
# Using log(n), where n is the number of observations, is comparable to BIC.
forwardreg_day <- step(null, 
                        scope = list(lower = null, upper = full), 
                        trace = 0, 
                        direction = "forward", 
                        k = 0)

covars_forward2 <- names(forwardreg_day$coefficients) %>%
  setdiff('(Intercept)')

covars_forward2 #different list than above when just run for summer

#forward selection is what we used the the lab, but she also said it wasn't how you would normally really do it. Do we want to try backwards selection or something else?

```

```{r fit for weekdays, warning = FALSE, message = FALSE}

#-----model order and CV-----

# apply along length of the vector of names from forward selection
res2 <- lapply(seq_along(covars_forward2), function(i){
    
    # define the formula, updated to add a term each time
    fmla <- as.formula(
        paste("ln_no2 ~ + ", paste(covars_forward2[seq_len(i)], collapse = "+"))) 
    
    # in-sample model and estimates
    in_model <- lm(fmla, data = no2_only) 
    
    # out-of sample model and estimates
    out_ests <- do_CV(data = no2_only, id = "location", group = "rando_CV_grp", fmla)  #changed the id to location, because that is the unique identifier for WHERE the stop is, which is what I think we want to be able to predict. STOP ID, is unique to the actual time that they stoped at that location each time around. Maybe I'm confused here, but it seems like we want to predict by location?
    out_results <- get_MSE(out_ests$ln_no2, out_ests$cvpreds)
    
    # compile results
   tibble(n_pred = i,
           covar = covars_forward2[i],
           in_RMSE = sqrt(mean(in_model$residuals^2)),
           in_R2 = summary(in_model)$r.squared,
           out_RMSE = out_results[["RMSE"]],
           out_R2 = out_results[["MSE_based_R2"]] 
           )
    
    }) %>% 
    bind_rows()

head(res2)



```

```{r bias.plots.weekday kw}
#-----bias-var combined plots-----


#there are 23 terms in the model. 
max(res2$out_RMSE[1:23])
min(res2$out_RMSE[1:23])

y_lim <- 0.8

# create temporary dataframe for plot
temp2 <- res2 %>% 
  
  # make long dataframe
  pivot_longer(cols = c(ends_with("_RMSE"), ends_with("_R2")), 
               names_to = "Source_Estimate", 
               values_to = "value" ) %>%
  
  # separate the "Source" column for in and out of sample
  separate(col = Source_Estimate, into = c("Source", "Estimate") ) %>% 

  # set high RMSE values to NA, then filter out these values before plotting
  mutate(value = ifelse(Estimate == "RMSE" & value > y_lim, NA, value)) %>%
  filter(!is.na(value)) 


combined_plot2 <- ggplot(data = temp2) +
  geom_point(aes(x = n_pred, y = value, color = Source)) +
  geom_line(aes(x = n_pred, y = value, color = Source)) +
  xlab("Model Complexity (# of terms)") +
  ylab("") +
  labs(title = "Bias-Variance TradeOff For Randomly Cross-Validated CV Groups",
       subtitle = "Type of Day Model") +
  scale_x_continuous(breaks = c(seq(0, 63, 5))) +
  facet_wrap(~ Estimate, scales = "free_y", ncol = 1, strip.position = "right") +
  theme_bw() 

#show plot
combined_plot2
```

```{r}
#KW experimenting with backwards selection


#-----backward selection-----#


null <- lm(ln_no2 ~ 1, data = no2_only) 


covars_all <- str_subset(names(no2_only),"pop_|int_|open_|D2|A1_|A23_|m_to_a1")

# B: now turn this into a formula for the full model in stepwise regression:
full <- as.formula(paste("ln_no2 ~ ", paste(covars_all, collapse= "+")))

# Fit the full model
full_model <- lm(full, data = no2_only)

# Using k=2 is comparable to standard AIC. (which is the default here)

backward_reg <- step(full_model, direction='backward', scope=formula(full_model), trace=0)

covars_backward <- names(backward_reg$coefficients) %>%
  setdiff('(Intercept)')

covars_backward 

backward_reg$anova

backward_reg$coefficients

# I don't really know much about how to interpret this. May not be that useful in its current state. 


```


```{r appendix, results='hide'}

#*Statistical Approach for regression for association*

#1. We aim to create a best-fit model of NOx/TRAP pollutant to assess the association of NOx concentrations across days of the week. 

#1b. We will characterize variation in NOx/TRAP across days of the week - within week day and between weekday

#2. We aim to create a best fit model of NOx TRAP pollutant to assess NOx concentrations across the four seasons of the year. 

#2b. We will characterize variation in NOx/TRAP across season - within and between season variability


## Methods & Statistical Approach

#Descriptive statistics

#  We will characterize the distribution of PM2.5 concentrations according to season (Fall, Winter, Spring, Summer) and day of the week using descriptive summary statistics, box plots and/or histograms. 

 # We will use ANOVA models to compare the mean concentration of log-transformed air pollutants across (1) seasons (i.e. Fall, Winter, Spring, and Summer); and (2) days of the week. 
  

#As part of our day-of-the-week assessment, we will additionally test whether TRAP is associated more broadly with day type (i.e. week day or weekend) using a land-use regression model adjusted for season and distance to major roadways, which we anticipate could be precision variables.



```

```{r}

season_summary <- stop_data_primary %>% group_by(season, location) %>% summarise(count = length(median_value))

range(season_summary$count)

day_summary <- stop_data_primary %>% group_by(day_type, location) %>% summarise(count = length(median_value))

range(day_summary$count)

weekday_summary <- stop_data_primary %>% group_by(week_day, location) %>% summarise(count = length(median_value))

range(weekday_summary$count)


```




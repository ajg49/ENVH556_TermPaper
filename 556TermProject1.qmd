---
title: "CA NOx Term Project"
authors: "Callan, Abbie, and Katie - ENV H 556"
format:
  html:
    df_print: "paged"
    fig_caption: yes
    toc: true
    toc_depth: 3
    number_sections: true
    self-contained: true #save images etc. in this file (vs folders)
execute:
  echo: false
  cache: false
  echo.comments: false
  message: false
  warning: false
  
---

This document was rendered on `r format(Sys.time(), '%B %d, %Y')`.
---

# Introduction and Purpose

Nitrogen oxides (NOx) are gaseous compounds commonly found in air pollution. Although NOx are generated through both natural and anthropogenic sources, human activities account for the majority of ambient NOx concentrations [(EPA 2024;)](https://www.epa.gov/no2-pollution/basic-information-about-no2) [Zhang et al. 2003;](https://www.pnas.org/doi/10.1073/pnas.252763799) [(ATSDR 2002;](https://www.atsdr.cdc.gov/toxfaqs/tfacts175.pdf) [(EPA 1999)](https://www3.epa.gov/ttncatc1/dir1/fnoxdoc.pdf). Mobile vehicles are a major source of anthropogenic NOx, accounting for approximately half of all emissions related to human activities [(EPA 1999)](https://www3.epa.gov/ttncatc1/dir1/fnoxdoc.pdf). Power plants also contribute a substantial fraction (~20%) of the total estimated annual anthropogenic NOx emissions [(EPA 1999)](https://www3.epa.gov/ttncatc1/dir1/fnoxdoc.pdf).

Traffic-related air pollutants (TRAP), such as NOx, can have diurnal and seasonal trends related to temporal vehicular traffic patterns [(Blanco et al. 2022)](https://www.nature.com/articles/s41370-022-00470-5#MOESM1). Measurements of TRAP must therefore appropriately reflect the relevant temporal period that they are intended to represent. Traditionally, air pollution is measured through stationary area monitors, which provide continuous air concentration data over long time scales from a single location [(CDPHE 2024)](https://cdphe.colorado.gov/air-toxics/monitoring#:~:text=Mobile%20monitoring%20can%20capture%20%E2%80%9Cspatial,wind%20compared%20to%20stationary%20monitoring.). Mobile monitoring is another effective air pollution measurement strategy that can be used to evaluate air concentrations across a wide set of geographic areas. During mobile monitoring, vehicles equipped with air sampling equipment drive through areas and provide real-time measurements of air pollution across a series of stops (CDPHE 2024). Mobile monitoring is an especially advantageous air monitoring strategy due to its spatial flexibility, which can provide data on areas not well-represented by stationary monitoring sites (CDPHE 2024). Despite its spatial advantages, mobile monitoring is more resource intensive than stationary monitoring and therefore provides a more limited set of air monitoring data (CDPHE 2024). 

In this analysis, we use the California NOx dataset described by Blanco et al. (2022) to simulate a series of temporally-restricted mobile monitoring campaigns and compare their prediction performances against that of a long-term, temporally-balanced "gold standard" dataset representing what might be achieved with a stationary air monitor. We aimed to assess how well NOx predictions from short-term mobile monitoring campaigns aligned with the "gold standard" dataset and whether the inclusion or omission of specific temporal windows impacted prediction performance. In support of this aim, we compared the performance of a "gold standard" temporally-balanced statewide dataset with a (1) short-term, temporally-balanced statewide dataset; and (2) short-term, seasonally-restricted statewide datasets. Additionally, we repeated this analysis when all data were instead restricted to Los Angeles county to assess how the comparisons differed in the presence of reduced geospatial variability. Ultimately, these results can be used to inform future TRAP monitoring campaigns through signaling the importance of temporally-balanced sampling designs and underscoring which key temporal windows must be included in mobile monitoring sampling events. 

# Methods 

**Dataset Description**

The California NOx dataset, first described by [Blanco et al. (2023)](https://www.nature.com/articles/s41370-022-00470-5#MOESM1), includes NOx concentration measurements collected from 69 California Air Quality System (AQS) sites in 2016. Measurements were collected every hour, enabling the evaluation of temporal trends in NOx concentrations across both short- and long-term timescales. Land-use, roadway proximity, and population density covariates (N=321 in total) were additionally available in the dataset. 

**Overview of Statistical Approach** 

We developed a general model for NOx predictions to be used across all analyses. We evaluated NOx prediction performances using the general model for our "gold standard" temporally-balanced dataset, our short-term temporally balanced dataset, and our short-term temporally restricted dataset. Details regarding the creation and characteristics of these datasets are provided in subsequent sections. 

For all analyses, NOx concentrations were modeled on the natural log scale based on the methods described by Blanco et al. (2022) and based on our visual inspection of the variable distributions. 

*Sensor Selection*

Sensors from the California NOx dataset were selected for inclusion in our analysis according to the criteria described by Blanco et al. (2022). These criteria included: (1) limited missingness, such that the sensors included had annual data that was at least 66% complete; (2) limited data gaps, such that data gaps for a particular sensor were less than or equal to 45 days long; (3) sensors must have sampled for at least 40% of the time during the two-week period used in our temporally restricted sampling campaigns (described below)  (**May not use this last criterion?**); and (4) monitors must have positive readings (> 0 ppb) at least 60% of the time. Negative concentration readings were left in the data set, as we interpreted them as true reading close to 0 that reads as negative due to noise or "classical error" in the instrument readings. We restricted the percentage of negative readings in each sensor to ensure that the sensors included had adequate variability in their readings and that annual average concentrations would be positive. 

*Model Selection*

Covariates were selected for inclusion into our general model for NOx predictions based on their scientific relevance to ambient NOx concentration prediction models, as described in peer-reviewed scientific literature. Specifically, we chose to include the covariates utilized by [Mercer et al. (2011)](https://pmc.ncbi.nlm.nih.gov/articles/PMC3146303/pdf/nihms299256.pdf), who measured ambient NOx concentrations in Los Angeles across varied temporal layers and built prediction models using land-use and geographic covariates. Mercer et al. (2011) selected their prediction model covariates from 65 possible covariates related to population density, land-use intensity, open space land, distance to the coast, distance to industrial sources, distances to various roadways, and lengths of roadways within a given buffer zone. From these potential covariates, Mercer et al. (2011) created a "common model" that had adequate performance in predicting NOx concentrations across all seasons. The covariates included in the Mercer et al. (2011) common model included: distance to commercial pollutant sources, distance to the coast, distance to A1 roadways, population size within 5000 meters, land use intensity within 3000 square kilometers, length of A1 roads within 50 meters, and length of A2 and A3 roads within 400 meters.

For our analyses, we used forward stepwise regression to select covariates for inclusion into our model from those most similar to the seven used by Mercer et al. (2011). The Mercer et al. (2011) covariates were deemed appropriate to use based on the similarities in geographic region, pollutant of interest, land-use covariates, and modeling goals between the present analysis and that described by Mercer et al. (2011). These covariates included: meters to closest commercial and services area, distance to the closest coastline, population density within 5,000 meters, distance to nearest A1 roadway, length of A1 roadway within 500 meters, length of A2 roadway within 1500 meters, length of A3 roadway within 400 meters, and the proportion of mixed urban of built-up land within a 1500 square kilometer buffer. Note that, in some cases, distances and buffers varied between those used by Mercer et al. (2011) and those used by Blanco et al. (2022); in these cases, the closest available covariate distance or buffer was selected. For land-use intensity, the proportion of mixed urban or built-up land within 1500 square kilometers was used in the present analysis, as it was deemed to be likely the most similar to the Mercer et al. (2011) land use intensity covariate. 

*Gold Standard Sampling Dataset*

To make our temporally-balanced "gold standard" sampling dataset, we included all measurements from monitors that met our pre-defined inclusion criteria. The gold standard dataset therefore reflected an ideal sampling mechanism, representing sample collection at every hour on every day in all seasons. As noted by Blanco et al. (2022), the gold standard dataset is reasonably representative of the measurements one might obtain from annual stationary air monitoring. 

We predicted the annual average NOx concentration using our gold standard sampling dataset with 10-fold cross-validation. We computed the R2 and RMSE characterizing the model fit and accuracy within this dataset. 

*Short-Term Sampling Dataset*

We created short-term sampling datasets intended to represent mobile monitoring campaigns within short-term sampling periods. Our short-term sampling datasets included temporally balanced and temporally restricted subsets of the data. The temporally balanced subset included randomly selected measurements from all measurements in the gold standard dataset. For our temporally-restricted datasets, we stratified our dataset by season and restricted measurements for each monitoring campaign to a single season; this was intended to simulate a mobile monitoring campaign collecting all data on an expedited timeframe with little seasonal variability in the collected samples. 

For each short-term sampling period, we filtered the dataset to include only samples from the specified timeframe, randomly sampled 28 measurements from each monitor, and repeated this random selection 30 times. 28 measurements were selected based on findings in Blanco et al. (2022), which reported that 28 measurements could accurately estimate the annual average of a site (less than or equal to 25% error) and reasonably represented the sampling design of a single mobile monitoring campaigns, which collect repeated samples from a limited number of locations. The random sampling process was repeated 30 total times to provide additional robustness through simulating multiple mobile monitoring campaigns, as described by Blanco et al. (2022). 

For each short-term sampling dataset, we predicted the expected annual average NOx concentration with 10-fold cross-validation based on the results from all 30 sampling iterations. We additionally computed the expected value of R2 [*WHAT KIND OF R SQUARED*] and expected RMSE based on all 30 sampling iterations. 

*Comparison of Sampling Approaches*

We compared the estimated model performance parameters (RMSE, R2) from the short-term sampling datasets with those from the gold standard dataset. We additionally compared the cross-validated annual average estimates from the temporally-restricted sampling datasets with that estimated using the gold standard sampling dataset (**NOTE: is this last sentence true?**). 

*Sensitivy Analysis: Los Angeles County*

In our sensitivty analysis, we repeated our analyses with geographically restricted data from a single county to limit the impact of geospatial variability, as NOx concentration measurements likely have strong spatial correlation at the county-level based on shared land-use, policy, and urbanicity within counties. Los Angeles County was selected as the county of interest for our analysis based on its high population density (U.S. Census 2023) and because of its high proportion of sensors included in the underlying California NOx dataset. Because there were fewer sensors in Los Angeles County relative to the statewide dataset, 10-fold cross validation was not possible in this sensitivity analysis. Instead, we used leave-one-out cross validation to validate Los Angeles-specific NOx predictions. We still subset our data into 30 sampling iterations. 



# Results

In total, 69 sensors in the state of California met the specified inclusion criteria and were included in our NOx prediction analysis. For our analyses specific to Los Angeles county, 12 sensors meeting the inclusion criteria were included.

```{r setup, include=FALSE}

#-----setup-----

# clear work space of all objects and unload all extra (non-base) packages
rm(list = ls(all = TRUE))
if (!is.null(sessionInfo()$otherPkgs)) {
    res <- suppressWarnings(
        lapply(paste('package:', names(sessionInfo()$otherPkgs), sep=""),
               detach, character.only=TRUE, unload=TRUE, force=TRUE))
   
}

```

```{r load.libraries.pacman, echo=FALSE, include=FALSE, eval=TRUE}

#-----load libraries pacman-----

# Load pacman into memory, installing as needed
my_repo <- 'http://cran.r-project.org'
if (!require("pacman")) {install.packages("pacman", repos = my_repo)}

# Load the other packages, installing as needed.
pacman::p_load(knitr, kableExtra, tidyverse, lubridate, egg, multcomp, modelr, broom, EnvStats, Hmisc,dplyr, tidyr, purrr, ggplot2, stringr, sf, lme4, VCA)


```


```{r read.data, echo=FALSE, include=FALSE}
#-----read data from a website--------

# create data directory if it does not exist
dir.create(file.path("Datasets"),
           showWarnings = FALSE,
           recursive = TRUE)

# read in ca nox air pollution concentrations from mobile monitoring
# download it from the web and save locallyif the file does not already exist
# read data; download if necessary
cal_nox <- read_rds(file.path("https://zenodo.org/records/14166411/files/nox_hourly.rda?download=1", 
                                 output_file_path = file.path("Datasets", "nox_hourly.rda"))) %>% rename_with(~ tolower(gsub(".","_", .x, fixed=TRUE)))

#view the data
glimpse(cal_nox)
summary(cal_nox)

length(unique(cal_nox$native_id)) #73 locations

unique(cal_nox$parameter_name) #3 different measurements NO, NO2, NOX
                              
ca_covariates <- read_rds(file.path("https://zenodo.org/records/14166411/files/site_covariates.rda?download=1"))

ca_covariates <- st_as_sf(ca_covariates, coords = c("longitude", "latitude"))

#glimpse(ca_covariates) #too long!

#going to do the attaching later because it creates such a huge df
  


```



```{r create.strata, echo =F, include = F}

cal_nox <- cal_nox %>%
  mutate(day_time = case_when(
    hour(date) > 4 & hour(date) <= 9 ~ "Morning",
    hour(date) > 9 & hour(date) <= 16 ~ "Midday",
    hour(date) > 16 & hour(date) <= 21 ~ "Evening",
    hour(date) > 21 | hour(date) <= 4 ~ "Night"
  ), .after = hour) %>%
  mutate(season = factor(season, levels = c("Morning", "Midday", "Evening", "Night")))

#I made these cuts off very rough estimates based on data distribution. We probably want to fine tune our scientific rationale for doing so and readjust if warranted. KW

#fixing the season variable because there were a lot of NA

cal_nox <- cal_nox %>%
  mutate(season = case_when(
    week(date) > 12 & week(date) <= 25 ~ "Spring",
    week(date) > 25 & week(date) <= 38 ~ "Summer",
    week(date) > 38 & week(date) <= 50 ~ "Fall",
    week(date) > 50 | week(date) <= 12 ~ "Winter"
  )) %>%
  mutate(season = factor(season, levels = c("Spring", "Summer", "Fall", "Winter")))


#summarizing by Day of the Week
cal_nox<- cal_nox %>%
  mutate(weekday2 = as.character(weekday) %>%
           str_replace(pattern = "TRUE",
                       replacement = "Weekday") %>%
         str_replace(pattern = "FALSE",
                     replacement = "Weekend"), .after = weekday)


```
#Attaching covariates to the data with all the temporal variables added. Use this if needed for model selection
```{r, add.covars}
# combine files
cal_nox_covars <- left_join(cal_nox, ca_covariates, by="native_id")
```


#Cleaning the data according to the criteria in Blanco paper

```{r cleaning.data}

#cleaning the data for sampling per the Blanco et al. paper

#create a df that is only NOX

nox_only <- cal_nox %>% filter(parameter_name == "Oxides of nitrogen (NOx)")


length(unique(nox_only$native_id)) #There are 69 monitors here, so we lost a couple when we filtered it to NOX only (there were 73 monitors when NO and NO2 were also included). 

#determining missingness
lapply(nox_only, function(i){ 
   tibble( 
          # sum missing
          n_miss = sum(is.na(i)), 
          
          # percent missing
          perc_miss = round(n_miss/length(i) * 100, 1)
          )
   }) %>% 
   # bind list
   bind_rows(.id = "variable")
# There are no NA values, but not sure if there is a complete record for each monitor

#calculate the total number of expected measurements per monitor

#check the range of the data
min(nox_only$date)
max(nox_only$date)
#The data ranges from "2016-01-01 00:00:00 PST" to "2016-12-31 23:00:00 PST"

#confirm 2016 is a leap year
leap_year(2016) # it is, and there are 366 DOY

#expected number of measurements per monitor: 366 days * 24 measurements/day
meas <- 366 *24 #expect 8784 measurements/monitor if there are no gaps for the year

meas_summary <- nox_only %>% 
  group_by(native_id) %>% 
  summarise(
    count = n(), 
    pct_total = n()/meas*100, 
    pct_pos = sum(sample_measurement > 0)/n()*100)

min(meas_summary$pct_total) #77.8% of the year
max(meas_summary$pct_total) #96.9% of the year

#all of this meets the first criteria in the Blanco paper, which is to have reading >66% of the year

#second criteria from Blanco: data gaps <= 45 days long

# Initialize a dataframe
max_gap <- data.frame(native_id=unique(nox_only$native_id), max_gap=NA)

# Loop over the monitors to calculate the maximum gap in measurements in days
for(i in 1:nrow(max_gap)){
  data.i <- subset(nox_only, native_id == max_gap$native_id[i])
  data.i <- data.i[order(data.i$doy), ]  # Ensure data is sorted by 'doy'
  
  gaps <- diff(data.i$doy)  # Calculate the differences between consecutive days
  max_gap$max_gap[i] <- max(gaps, na.rm = TRUE)  # Store the maximum gap
}

max(max_gap$max_gap) #maximum number of consecutive days w/o measurements is 36

#based on blanco criteria there is no need to eliminate any monitors

#Third Blanco criteria is that the monitor sampled for 40% of the time during the two week period used in "common design" sampling models. Will implement this later if we choose to do a 2 weeks sampling window. 

#Fourth Blanco critera is the monitor is >0 60% of the time. 

min(meas_summary$pct_pos) #70.6%
max(meas_summary$pct_pos) #100%

#no need to eliminate any monitors based on this criteria. 

#No monitors were eliminated based on this cleaning. 
```


#Function to do random sampling from the data - gold standard dataset
```{r,random.sampling fxn}
#per Blanco, we will pick 28 samples per monitor. The Gold Standard sample, will be distributed across the whole year. We will then do temporally restricted samples to see how they compare

#create factor to make subsetting in the loop easier for resample
nox_only$native_id_fact <- factor(nox_only$native_id)

size = 28 #per Blanco, the number of samples taken from each monitor
monitors = length(unique(nox_only$native_id)) #number of monitors in the data

# Initialize a dataframe to store the sample that is the same columns as the original nox_only df and enough rows for the desired sampling
sample_df <- data.frame(matrix(NA, nrow = size*monitors, ncol = ncol(nox_only)))
colnames(sample_df) <- colnames(nox_only)

set.seed(72) #seed for reproducability
# Initialize a row index for sample_df
row_index <- 1

for(i in 1:monitors) {
  data.i <- subset(nox_only, native_id_fact == unique(nox_only$native_id_fact)[i])
  sample_indices <- sample(1:nrow(data.i), size, replace = FALSE) # Take 'size' random samples from each monitor
  
  for(j in sample_indices) {
    sample_df[row_index, ] <- data.i[j, ]
    row_index <- row_index + 1
  }
}
sample_df$date <- as_datetime(sample_df$date) #make sure the date column stays in the right format in the new df. 

#head(sample_df) #Worked!

#Parameter name was a factor, so it only saved the number and not the name, but who cares (in the original df as imported before filtering 1 = NO2, 2 = NO, 3 = NOX)

#now writing it as function that will be easy to use

random_sample <- function(data, size, seed) {  #specify the data frame and the number of samples to take from each monitor
  data$native_id_fact <- factor(data$native_id)
  monitors <- length(unique(data$native_id)) # number of monitors in the data
  
  # Initialize a dataframe to store the sample that has the same columns as the original data and enough rows for the desired sampling
  sample_df <- data.frame(matrix(NA, nrow = size * monitors, ncol = ncol(data)))
  colnames(sample_df) <- colnames(data)
  
  set.seed(seed) # seed for reproducibility
  # Initialize a row index for sample_df
  row_index <- 1
  
  for(i in 1:monitors) {
    data.i <- subset(data, native_id == unique(data$native_id)[i])
    sample_indices <- sample(1:nrow(data.i), size, replace = FALSE) # Take 'size' random samples from each monitor
    
    for(j in sample_indices) {
      sample_df[row_index, ] <- data.i[j, ]
      row_index <- row_index + 1
    }
  }
  
  sample_df$date <- as_datetime(sample_df$date) # make sure the date column stays in the right format in the new df.
  
  return(sample_df)
}

test <- random_sample(nox_only, 28, 72)

#comparing test and sample_df to make sure they are the same

check = sample_df$sample_measurement - test$sample_measurement
max(check) #0
min(check) #0
#The function works. 

```

#Writing a function to aggregate data to the annual level - gold standard dataset
```{r annual.avg, echo = FALSE}

#function to take the annual average concentration of any dataframe
annual_avg <- function(data) {
  annual <- data %>% group_by(native_id) %>% summarise(annual_avg = mean(sample_measurement), log_avg = log(mean(sample_measurement)))
  return(annual)
}

#testing the function does what i want to to
test1 <- test %>% group_by(native_id) %>% summarise(annual_avg = mean(sample_measurement), log_avg = log(mean(sample_measurement)))

test2 <- annual_avg(test)

check = test1$annual_avg - test2$annual_avg
max(check) #0
min(check) #0
#The function works. 

check2 = test1$log_avg - test2$log_avg
max(check2) #0
min(check2) #0


```


#taking a bunch of random samples for the short-term balanced data
# short term random samples meant to represent a balanced mobile monitoring campaign.
```{r st.balanced}
# Initialize the short-term balanced data frame
st_balanced <- data.frame()

n_reps <- 30  # number of random df simulations to run

for (i in 1:n_reps) {
  seed <- i * 4  # generating a different seed for each simulation
  sampled_data <- random_sample(nox_only, 28, seed)
  annual_avg_data <- annual_avg(sampled_data)
  
  # Add seed and rep_number columns
  annual_avg_data$seed <- seed
  annual_avg_data$rep_number <- i
  
  # Append to dataframe
  st_balanced <- rbind(st_balanced, annual_avg_data)
}

# Checking Abbie's Understanding:
# this chunk randomly grabs 28 samples from each of the sensors. 
# it computes an annual average based on only these 28 samples for all 69 sensors. 
# this is repeated 30 times, so there are 30  annual averages based on thirty different 
# sets of samples for each of the 69 sensors.
# katie please check if I interpreted this right!

## Yes, that is exactly right! - KW


```


#Creating Gold Standard Data - long-term and short-term balanced datasets
```{r}

#Gold Standard Data
long_term_gold <- annual_avg(nox_only)

#summary statistics
lt_gold_sum <- long_term_gold %>% summarise(min = min(annual_avg), 
                                            Q25 = quantile(annual_avg, probs = 0.25), 
                                            Q50 = quantile(annual_avg, probs = 0.5),
                                            Q75 = quantile(annual_avg, probs = 0.75),
                                            max = max(annual_avg),
                                            SD = sd(annual_avg)
                                            )
lt_gold_sum
      #This matches the number reported in Blanco supplement S5

#summary stats for st_balanced
st_balanced_sum <- st_balanced %>% summarise(min = min(annual_avg), 
                                            Q25 = quantile(annual_avg, probs = 0.25), 
                                            Q50 = quantile(annual_avg, probs = 0.5),
                                            Q75 = quantile(annual_avg, probs = 0.75),
                                            max = max(annual_avg),
                                            SD = sd(annual_avg)
                                            )
st_balanced_sum #this would not be expected to match Blanco because we used different seeds
#summarizing the data from the random samples
names <- c("Long-term", "short-term")
kable(cbind(names , rbind(lt_gold_sum, st_balanced_sum)), digits = 2,
      caption = "Table 1.0: Balanced, Long-term & Short-term Sampling Campaign Summaries") %>% kable_styling()

```
  Table 1.0 has a short summary of the data sets for our Gold Standard and short-term balanced models. **maybe this gets moved into the appendix?**


#writing a function for calculating summary statistics of an annual average df. 
```{r, summary stats}
#function to make summary stats easier
sum_stats <- function(data) {
  summary <- data %>% summarise(min = min(annual_avg), 
                                            Q25 = quantile(annual_avg, probs = 0.25), 
                                            Q50 = quantile(annual_avg, probs = 0.5),
                                            Q75 = quantile(annual_avg, probs = 0.75),
                                            max = max(annual_avg),
                                            SD = sd(annual_avg)
                                            )
  return(summary)
  
}

#testing
sum_stats(st_balanced)

#yay!!

```

```{r gold.standard.variability}
#graphing the distribution of annual averages


ggplot(st_balanced, aes(x = native_id, y = log_avg)) +
  geom_boxplot()+
  ggtitle("Figure 1:Short Term Balanced Sensor Log-transformed Annual Averages \n30 sets of 28 samples per sensor") +
  ylab("ln(NOx) ln(ppb)")

ggplot(long_term_gold, aes(x = native_id, y = log_avg)) +
  geom_point()+
  ggtitle("Figure 2: Gold Standard Sensor Log-transformed Annual Averages \n1 sample per sensor") +
  ylab("ln(NOx) ln(ppb)")

ggplot(nox_only, aes(x = native_id, y = log(sample_measurement))) +
  geom_boxplot()+
  ggtitle("Figure 3: Raw Data by sensor ID \nLog transformed") +
  ylab("ln(NOx) ln(ppb)")

#Kinda crazy how much variability there is using unrestricted random samples
#We can choose which type of plot we like best - AG

with(st_balanced, hist(annual_avg, breaks = 30, col = "blue", xlab = "NOx ppm" ))
with(st_balanced, hist(log_avg, breaks = 30, col = "seagreen", xlab = "log(NOx ppm)" ))

with(long_term_gold, hist(annual_avg, breaks = 20, col = "blue", xlab = "NOx ppm" ))
with(long_term_gold, hist(log_avg, breaks = 20, col = "seagreen", xlab = "log(NOx ppm)" ))

#makes much more sense to work with the data on the log scale


```

Figure 1.0 illustrates the variability in the short-term balanced data set created by randomly sampling across all eligible sensors in california. We see variability within each sensor and across sensors. 

Figure 2.0 shows the annual averages for each sensor in the Gold Standard data set. 

Figure 3 shows the variation in our data without any aggregation (but it is log transformed). There is more variability within each sensor's data than between sensors. 


#Break up the histograms by season, look at NOx distribution by seasons

```{r histogram}

#---------------------- Stratified by season----------------------------------#
#native scale
#histograms with smoother
nox_only %>%
ggplot(aes(sample_measurement)) +
  geom_histogram(aes(y = ..density..), bins = 40, color = "lightblue", fill = "lightblue", alpha = 0.8) +
  geom_density(color = "purple") +
  facet_wrap(~season, scales = "free") +
  labs(title = "Distribution of NOx by Season",
       x = "Concentration on native scale",
       y = "density"
       )

```

```{r, eval = F}

#log transformed by season
nox_only %>%
ggplot( aes(log(sample_measurement))) +
  geom_histogram(aes(y = ..density..), bins = 40, color = "darkgreen", fill = "darkgreen", alpha = 0.4) +
  geom_density(color = "green") +
  facet_wrap(~season, scales = "free") +
  labs(title = "Distribution of ln(NOx) by Season",
       x = "Concentration on log scale",
       y = "density"
       )

```
#Create CV groups that can be used for all the data
```{r cv}
#-----generate cross validation group for all analyses-----

# set the seed to make reproducible
set.seed(123)

# create vector of CV groups
  # first generate a vector with values 1:10 equal to the number of dataset rows
  # then randomize (with argument replace = FALSE)
CV_grp <- rep(1:10, length.out = nrow(long_term_gold)) %>% 
  sample(replace = FALSE)


# now append it to the cal_nox_clean data frame
long_term_gold <- mutate(long_term_gold, CV_grp = CV_grp)


```

#Model Prediction Functions
```{r define get_MSE}
#-----define get_MSE function-----

# This is a function to get the MSE, RMSE, MSE-based R2
get_MSE <- function(obs,pred) {
    # obs is the outcome variable
    # pred is the prediction from a model
     
    # mean of obs
    obs_avg <- mean(obs)
    
    # MSE of obs (for R2 denominator)
    MSE_obs <- mean((obs-obs_avg)^2)
    
    # MSE of predictions
    MSE_pred <- mean((obs - pred)^2)
    
    # compile output
    result <- c(RMSE = sqrt(MSE_pred),
                MSE_based_R2 = max(1 - MSE_pred / MSE_obs, 0) 
                )
    
    # explicit return (optional)
    return(result)
}

```


#Define the Cross Validation Function
```{r define.CV.function}
#-----define CV function-----

do_CV <- function(data, id = "id", group = "group", formula) {
 
  lapply(unique(data[[group]]), function(this_group){
    
    # fit the "common" model to the training set (without this group)
    CV_lm <- lm(formula, data = data[data[[group]] != this_group,])
    
    # generate predictions for this group using training model
    data[data[[group]] == this_group,] %>%
      mutate(cvpreds = predict(CV_lm, newdata = .) %>% unname())
    
    # recombine data from all clusters and sort by ID column
    # note use of ".data[[ ]]" to return the value of variable id
  }) %>% bind_rows() %>% arrange(.data[[id]])
  
  # return the dataset (the last-evaluated object is always returned by default)
}

```

#mercer common model covariates
```{r covariates}

common_model_cov <- ca_covariates %>%
  dplyr::select("native_id", "m_to_a1", "m_to_coast", "m_to_comm","pop_s05000", "ll_a1_s00050", "ll_a2_s00400", "ll_a3_s00400", "lu_industcomm_p05000", "County", "geometry")

#nox_only <- left_join(nox_only,common_model_cov, by = "native_id" )

#adding the covariates to the gold standard data to do some predictions           # commenting out so that we can try fowrard selection
#long_term_gold <- left_join(long_term_gold,common_model_cov, by = "native_id" )
#st_balanced <- left_join(st_balanced,common_model_cov, by = "native_id" )



```


# forward, stepwise selection with common model covariates

```{r forward selection - gold standard}

#-----forward selection using interaction-----#
long_term_gold <- left_join(long_term_gold,common_model_cov, by = "native_id" )


null <- lm(log_avg ~ 1, data = long_term_gold)


covars_all <- str_subset(names(long_term_gold),"pop_|int_|open_|ll_|m_to_")

# B: now turn this into a formula for the full model in stepwise regression:
full <- as.formula(paste("log_avg ~ ", paste(covars_all, collapse= "+")))

# Using k=2 is comparable to standard AIC.
forwardreg_day <- step(null, 
                        scope = list(lower = null, upper = full), 
                        trace = 0, 
                        direction = "forward", 
                        k = 2)

covars_forward2 <- names(forwardreg_day$coefficients) %>%
  setdiff('(Intercept)')

covars_forward2 #different list than above when just run for summer



```

#gold-standard dataset for model selection
```{r cv in forward selection, warning = FALSE, message = FALSE}

#-----model order and CV-----

# apply along length of the vector of names from forward selection
res2 <- lapply(seq_along(covars_forward2), function(i){
    
    # define the formula, updated to add a term each time
    fmla <- as.formula(
        paste("log_avg ~ + ", paste(covars_forward2[seq_len(i)], collapse = "+"))) 
    
    # in-sample model and estimates
    in_model <- lm(fmla, data = long_term_gold) 
    
    # out-of sample model and estimates
    out_ests <- do_CV(data = long_term_gold, id = "native_id", group = "CV_grp", fmla)  #changed the id to location, because that is the unique identifier for WHERE the stop is, which is what I think we want to be able to predict. STOP ID, is unique to the actual time that they stoped at that location each time around. Maybe I'm confused here, but it seems like we want to predict by location?
    out_results <- get_MSE(out_ests$log_avg, out_ests$cvpreds)
    
    # compile results
   tibble(n_pred = i,
           covar = covars_forward2[i],
           in_RMSE = sqrt(mean(in_model$residuals^2)),
           in_R2 = summary(in_model)$r.squared,
           out_RMSE = out_results[["RMSE"]],
           out_R2 = out_results[["MSE_based_R2"]] 
           )
    
    }) %>% 
    bind_rows()

head(res2)



```

```{r bias.plots.weekday kw}
#-----bias-var combined plots-----


#there are 23 terms in the model. 
max(res2$out_RMSE[1:23])
min(res2$out_RMSE[1:23])

y_lim <- 0.8

# create temporary dataframe for plot
temp2 <- res2 %>% 
  
  # make long dataframe
  pivot_longer(cols = c(ends_with("_RMSE"), ends_with("_R2")), 
               names_to = "Source_Estimate", 
               values_to = "value" ) %>%
  
  # separate the "Source" column for in and out of sample
  separate(col = Source_Estimate, into = c("Source", "Estimate") ) %>% 

  # set high RMSE values to NA, then filter out these values before plotting
  mutate(value = ifelse(Estimate == "RMSE" & value > y_lim, NA, value)) %>%
  filter(!is.na(value)) 


combined_plot2 <- ggplot(data = temp2) +
  geom_point(aes(x = n_pred, y = value, color = Source)) +
  geom_line(aes(x = n_pred, y = value, color = Source)) +
  xlab("Model Complexity (# of terms)") +
  ylab("") +
  labs(title = "Bias-Variance TradeOff For Randomly Cross-Validated CV Groups",
       subtitle = "Mercer Common Model Covariates") +
  scale_x_continuous(breaks = c(seq(0, 63, 5))) +
  facet_wrap(~ Estimate, scales = "free_y", ncol = 1, strip.position = "right") +
  theme_bw() 

#show plot
combined_plot2
```


# Adding Covariates from Mercer et al Common Model
  The Mercer et al 2011 paper has the following covariates determined in its "common model" with which they assessed seasonal prediction models.
 Covariates:  D2A1 (distance to A1 road), A1_50m (length of A1 road in 50 m buffer), A23_400m (length of A2 or A3 roads within 400 m buffer), Pop_5k (total population within 5,000 m buffer), D2C (Distance to the coast in meters), Int_3k (intense land use within 3,000 m), D2Comm (distance to commercial land).
  To assess the best fit of these covariates to our data, we ran forward, stepwise selection on the california nox covariates that matched these mercer et al (2011) covariates the best (a total of 8 covariates). The Bias Variance plot above shows us that the three-covariate model performed the best on our gold standard data. The covariates in this model are:
  
"pop_s05000", "m_to_coast", and "m_to_a1" .  

#Specify the common model from forward selection
```{r}
# build regression formula with common model covariates

covars_common <- c("pop_s05000" ,"m_to_coast" ,"m_to_a1")
frml <- as.formula(paste("log_avg ~", paste(covars_common, collapse = "+")) )

```

#Running the prediction model on the lt_gold  - mercer
```{r}

# in-sample predictions and fit summary
summary(lm_gold_standard <- lm(frml, data = long_term_gold))


```



#Doing CV on the gold_standard model
```{r}
gold_CV <- do_CV(long_term_gold, id = "native_id", group = "CV_grp", formula = frml)

gold_MSE <- get_MSE(gold_CV$log_avg, gold_CV$cvpreds)

gold_MSE

#RMSE = 0.49
#MSE-R2 = 0.26

#plotting the results

# get range for plot
r <- gold_CV %>% dplyr::select(log_avg, cvpreds) %>% range()


ggplot(data = gold_CV, aes(log_avg, cvpreds)) +
    geom_point(shape = "o", alpha = 0.8) +
    lims(x= r, y = r) +
    coord_fixed() +
    geom_smooth(method = 'lm', se = F, aes(color = "red")) +
    geom_abline(aes(slope = 1, intercept = 0, color = "blue")) +
    labs(title = "Figure 4.0: Model predictions vs. observed ln(NOx)\n Cross-validated ", 
         subtitle = "Gold Standard Data",
         x = "Observed ln(NOx) (ln(ppb))",
         y = "Predicted ln(NOx) (ln(ppb))",
         caption = "RMSE = 0.49 MSE-based R2 = 0.26") + 
    scale_colour_manual(name='',
                      labels = c("1:1 Line", "Best-Fit Line"), 
                      values=c("blue", "red")) +
    theme_bw()


#model we have selected doesn't give the greatest predictions, but that might be fine. We changing the covariates in the model above won't necessitate changing any of the other code. 
```
Figure 4.0 shows the results from our cross validation of our gold standard model and data set. Using annual averages to predict across all 69 sensors has pretty poor prediction performance.

#Repeating above to compare predictions from the st_balanced to predictions from the gold standard

```{r st_balanced}

#need to group the st_balanced data by "campaign" before doing regression
st_balanced <- left_join(st_balanced,common_model_cov , by = "native_id" )

#apply the save CV groups as used in the gold standard model
st_balanced <- st_balanced %>% group_by(rep_number) %>% mutate(CV_grp = CV_grp) %>% ungroup()

#in sample-predictions
fitted_models = st_balanced %>% group_by(rep_number) %>% do(model = lm(frml, data = .))

#cross-validated predictions treating each sample "campaign" separately
st_balanced_CV <- st_balanced %>% group_by(rep_number) %>% do(model = do_CV(data = ., id = "native_id", group = "CV_grp", formula = frml))
#produces a df for each result

#next step is to restructure the result back into a single long_df

st_balanced_CV <- st_balanced_CV %>% rename(model_run = rep_number) #change column name so there isn't a conflict when I unnest it

st_balanced_CV <- st_balanced_CV %>%
  unnest(cols = c(model))

#appending the predictions from the gold_standard to the st_balanced_CV df for comparison
gold_preds <- gold_CV %>%
  dplyr::select("native_id", "cvpreds") %>% rename(gold_preds = cvpreds)
st_balanced_CV <- left_join(st_balanced_CV, gold_preds, by = "native_id" )


#calculating the RMSE and R2 two ways
  #a) comparing the campaign predictions to the observations from the gold standard 
  #b) comparing the predictions from each campaign to the observations from that campaign

#need to append the observations from the gold-standard to do method a

gold_obs <- gold_CV %>%
  dplyr::select("native_id", "log_avg") %>% rename(gold_obs = log_avg)
st_balanced_CV <- left_join(st_balanced_CV, gold_obs, by = "native_id" )

# Initialize empty lists to store the results for both methods
results_a <- list()
results_b <- list()

# Loop through each unique model_run
for (run in unique(st_balanced_CV$model_run)) {
  # Filter the data for the current model_run
  data_subset <- st_balanced_CV %>% filter(model_run == run)
  
  # Calculate the MSE for method A
  mse_result_a <- get_MSE(data_subset$gold_obs, data_subset$cvpreds)
  results_a[[run]] <- mse_result_a
  
  # Calculate the MSE for method B
  mse_result_b <- get_MSE(data_subset$log_avg, data_subset$cvpreds)
  results_b[[run]] <- mse_result_b
}

# Convert the lists to a data frame with separate columns for each method
st_balanced_MSE <- data.frame(
  model_run = unique(st_balanced_CV$model_run),
  RMSE_a = sapply(results_a, `[`, 1),
  R2_a = sapply(results_a, `[`, 2),
  RMSE_b = sapply(results_b, `[`, 1),
  R2_b = sapply(results_b, `[`, 2)
)

#pivot longer for plotting
st_balanced_MSE_long <- st_balanced_MSE %>%
  pivot_longer(
    cols = c(RMSE_a, R2_a, RMSE_b, R2_b),
    names_to = c("metric", "method"),
    names_sep = "_",
    values_to = "value"
  ) 

#Plotting (replicate figure 1 in blanco paper)
# get range for plot
r <- st_balanced_CV %>% dplyr::select(gold_preds, cvpreds) %>% range()


ggplot(data = st_balanced_CV, aes(x = gold_preds, y = cvpreds, color = as.factor(model_run))) +
    geom_point(shape = "o", alpha = 0.7) +
    lims(x= r, y = r) +
    coord_fixed() +
    geom_smooth(method = 'lm', se = F, alpha = 0.5) +
    geom_abline(slope = 1, intercept = 0, color = "darkgray", linetype = "dashed") +
    labs(title = "Campaign vs. Gold Standard Cross-validated Predictions", 
         subtitle = "Balanced Campaigns",
         x = "Gold Standard predicted ln(NOx) (ln(ppb))",
         y = " Campaign Predicted ln(NOx) (ln(ppb))",
         caption = "1:1 line is dashed") +
    theme_bw()


#This needs some aesthetic adjustments: choose between dots or lines, pick a color scale that's helpful, get rid of the funky legend, but it's cool!!
    #looks a lot like the balanced predictions from the Blanco paper. 

#replicate blanco figure 3
ggplot(st_balanced_MSE_long, aes(x = method, y = value, color = method)) +
  geom_boxplot() +
  geom_point(data = data.frame(method = c("a", "b"), value = gold_MSE[[1]], metric = "RMSE"), aes(x = method, y = value, color = method), size = 3, shape = 5) +
  geom_point(data = data.frame(method = c("a", "b"), value = gold_MSE[[2]], metric = "R2"), aes(x = method, y = value, color = method), size = 3, shape = 5) +
  facet_wrap(~metric) +
  labs(title = "RMSE and MSE_R2 Comparison",
       caption = "Method A = Campaign Predictions Compared to Gold Standard Obs\n
       Method B = Campaign Predictions Compared to Campaign Observations\n
       Square points shows the long-term campaigns performance") +
  theme_bw()

#This is also not bad. It will need some fancification, as well as adjustment when we start to add other campaigns, but it's an ok start. 

#Need to also add the in-sample R2, which I can do later.

#probably want to turn some of the above stuff into functions, because it is going to be a lot of cut and paste to do this for different data sets multiple times. 

# get range for plot
r <- st_balanced_CV %>% dplyr::select(log_avg, cvpreds) %>% range()


ggplot(data = st_balanced_CV, aes(log_avg, cvpreds)) +
    geom_point(shape = "o", alpha = 0.8) +
    lims(x= r, y = r) +
    coord_fixed() +
    geom_smooth(method = 'lm', se = F, aes(color = "red")) +
    geom_abline(aes(slope = 1, intercept = 0, color = "blue")) +
    labs(title = "Figure 4.0: Model predictions vs. observed ln(NOx)\n Cross-validated ", 
         subtitle = "Short-Term Balanced Data",
         x = "Observed ln(NOx) (ln(ppb))",
         y = "Predicted ln(NOx) (ln(ppb))",
         caption = "RMSE = 0.545 MSE-based R2 = 0.207") + 
    scale_colour_manual(name='',
                      labels = c("1:1 Line", "Best-Fit Line"), 
                      values=c("blue", "red")) +
    theme_bw()


```

#------------Season-Specific Prediction Models----------------------------------

**Seasonal Stratification of NOx for season-specific model generation**

``` {r seasonal strata}
#----------create season-specific strata---------------

summer <- nox_only[nox_only$season == "Summer",]

fall <- nox_only[nox_only$season == "Fall",]

winter <- nox_only[nox_only$season == "Winter",] 

spring <- nox_only[nox_only$season == "Spring",]



```


#Writing a function to aggregate data to the seasonal level -seasonal dataset
```{r seasonal.avg, echo = FALSE}
#function to take the annual average concentration of any dataframe
seasonal_avg <- function(data) {
  seasonal <- data %>% group_by(native_id) %>% summarise(seasonal_avg = mean(sample_measurement), log_avg = log(mean(sample_measurement)))
  return(seasonal)
}


```


#creating short term seasonal data sets
```{r seasonal datasets2}


#random_sample() (data, size, seed)
#----------summer----------------------------------------------
# Initialize the short-term balanced data frame
st_summer <- data.frame()

n_reps <- 30  # number of random df simulations to run

for (i in 1:n_reps) {
  seed <- i * 4  # generating a different seed for each simulation
  sampled_data <- random_sample(summer, 28, seed)
  summer_avg_data <- seasonal_avg(sampled_data)
  
  # Add seed and rep_number columns
  summer_avg_data$seed <- seed
  summer_avg_data$rep_number <- i
  
  # Append to st summer df
  st_summer <- rbind(st_summer, summer_avg_data)
}
ggplot(st_summer, aes(x = native_id, y = log_avg)) +
  geom_boxplot()+
  ggtitle("Figure X: Summer Short Term Sensor Log-transformed Annual Averages \n30 sets of 28 samples per sensor")


#----------fall----------------------------------------------
# Initialize the short-term balanced data frame
st_fall <- data.frame()

n_reps <- 30  # number of random df simulations to run

for (i in 1:n_reps) {
  seed <- i * 4  # generating a different seed for each simulation
  sampled_data <- random_sample(fall, 28, seed)
  fall_avg_data <- seasonal_avg(sampled_data)
  
  # Add seed and rep_number columns
  fall_avg_data$seed <- seed
  fall_avg_data$rep_number <- i
  
  # Append to st fall df
  st_fall <- rbind(st_fall, fall_avg_data)
}
ggplot(st_fall, aes(x = native_id, y = log_avg)) +
  geom_boxplot()+
  ggtitle("Figure X: Fall Short Term Sensor Log-transformed Annual Averages \n30 sets of 28 samples per sensor")

#------------Winter--------------------------------------------
st_winter <- data.frame()

n_reps <- 30  # number of random df simulations to run

for (i in 1:n_reps) {
  seed <- i * 4  # generating a different seed for each simulation
  sampled_data <- random_sample(winter, 28, seed)
  winter_avg_data <- seasonal_avg(sampled_data)
  
  # Add seed and rep_number columns
  winter_avg_data$seed <- seed
  winter_avg_data$rep_number <- i
  
  # Append to st winter df
  st_winter <- rbind(st_winter, winter_avg_data)
}
ggplot(st_winter, aes(x = native_id, y = log_avg)) +
  geom_boxplot()+
  ggtitle("Figure X: Winter Short Term Sensor Log-transformed Annual Averages \n30 sets of 28 samples per sensor")


#------------Spring--------------------------------------------
st_spring <- data.frame()

n_reps <- 30  # number of random df simulations to run

for (i in 1:n_reps) {
  seed <- i * 4  # generating a different seed for each simulation
  sampled_data <- random_sample(spring, 28, seed)
  spring_avg_data <- seasonal_avg(sampled_data)
  
  # Add seed and rep_number columns
  spring_avg_data$seed <- seed
  spring_avg_data$rep_number <- i
  
  # Append to st spring df
  st_spring <- rbind(st_spring, spring_avg_data)
}

ggplot(st_spring, aes(x = native_id, y = log_avg)) +
  geom_boxplot()+
  ggtitle("Figure X:Spring Short Term Sensor Log-transformed Annual Averages \n30 sets of 28 samples per sensor")


# this should be the same length as the st_balanced dataframe
#True - AG

#Looks good! Only suggestion is that we just use the annual_avg() function and not add a season_avg() function, because other than the name, they produce the same results. KW

```



#quick summary short-term seasonal datasets
```{r}

#summary stats for st_summer
st_summer_sum <- st_summer %>% summarise(min = min(seasonal_avg), 
                                            Q25 = quantile(seasonal_avg, probs = 0.25), 
                                            Q50 = quantile(seasonal_avg, probs = 0.5),
                                            Q75 = quantile(seasonal_avg, probs = 0.75),
                                            max = max(seasonal_avg),
                                            SD = sd(seasonal_avg)
                                            )
#st_summer_sum

st_fall_sum <- st_fall %>% summarise(min = min(seasonal_avg), 
                                            Q25 = quantile(seasonal_avg, probs = 0.25), 
                                            Q50 = quantile(seasonal_avg, probs = 0.5),
                                            Q75 = quantile(seasonal_avg, probs = 0.75),
                                            max = max(seasonal_avg),
                                            SD = sd(seasonal_avg)
                                            )
#st_fall_sum 

st_winter_sum <- st_winter %>% summarise(min = min(seasonal_avg), 
                                            Q25 = quantile(seasonal_avg, probs = 0.25), 
                                            Q50 = quantile(seasonal_avg, probs = 0.5),
                                            Q75 = quantile(seasonal_avg, probs = 0.75),
                                            max = max(seasonal_avg),
                                            SD = sd(seasonal_avg)
                                            )
#st_winter_sum 

st_spring_sum <- st_spring %>% summarise(min = min(seasonal_avg), 
                                            Q25 = quantile(seasonal_avg, probs = 0.25), 
                                            Q50 = quantile(seasonal_avg, probs = 0.5),
                                            Q75 = quantile(seasonal_avg, probs = 0.75),
                                            max = max(seasonal_avg),
                                            SD = sd(seasonal_avg)
                                            )
#st_spring_sum



names2 <- c("summer", "fall", "winter", "spring", "balanced")
kable(cbind(names2 , rbind(st_summer_sum, st_fall_sum, st_winter_sum, st_spring_sum, st_balanced_sum)), digits = 2,
      caption = " Table 2.0: Short-term Sampling Campaign NOx Summaries") %>%
  kable_styling()

```
Table 2.0 summarizes the spread and range of the data randomly sampled for each season data set. The Fall and Winter data sets have nearly double the variation as Spring and Summer data sets. As we would expect, the balanced data set has vatriability in the middle between these four seasonal data sets. 

The figures above show that across all four seasons, the within-sensor variability looks similar, except for spring. The spring data set has less variability within sensor, not just across sensors. 

```{r season.convariates2}
#--------------------------Common Model on Seasonal Data sets------------------#
#adding the covariates to the gold standard data to do some predictions
st_summer <-  left_join(st_summer,common_model_cov, by = "native_id" )
st_fall <-  left_join(st_fall,common_model_cov, by = "native_id" )
st_winter <-  left_join(st_winter,common_model_cov, by = "native_id" )
st_spring <-  left_join(st_spring,common_model_cov, by = "native_id" ) #testing the spring data set without the inf. value data point


```

#running a regression model using common model covariates and seasonal data sets
```{r}

# in-sample predictions and fit summary
summary(lm_st_summer <- lm(frml, data = st_summer))
summary(lm_st_fall <- lm(frml, data = st_fall))
summary(lm_st_winter <- lm(frml, data = st_winter))
summary(lm_st_spring <- lm(frml, data = st_spring)) #infinitive log_avg value in spring data. a seasonal average is exactly zero. 


```


#----------comparing seasonal short term campaigns with Gold Standard data----------------

```{r summer vs. gold}
#------------seasonal comparison with gold standard-------------

#-----------------------------------SUMMER--------------------------------------
#need to group the st_summer data by "campaign" before doing regression

#apply the save CV groups as used in the gold standard model
st_summer <- st_summer %>% group_by(rep_number) %>% mutate(CV_grp = CV_grp) %>% ungroup()

#in sample-predictions
fitted_models = st_summer %>% group_by(rep_number) %>% do(model = lm(frml, data = .))

#cross-validated predictions treating each sample "campaign" separately
st_summer_CV <- st_summer %>% group_by(rep_number) %>% do(model = do_CV(data = ., id = "native_id", group = "CV_grp", formula = frml))
#produces a df for each result

#next step is to restructure the result back into a single long_df

st_summer_CV <- st_summer_CV %>% rename(model_run = rep_number) #change column name so there isn't a conflict when I unnest it

st_summer_CV <- st_summer_CV %>%
  unnest(cols = c(model))

#---compare st_summer with gold-----------------------------------------#
#appending the predictions from the balanced short term campaign to the st_summer_CV df for comparison
gold_preds <- gold_CV %>%
  dplyr::select("native_id", "cvpreds") %>% rename(gold_preds = cvpreds)  #select "model run" as well as native_id
st_summer_CV <- left_join(st_summer_CV, gold_preds, by = "native_id" ) #to have unique identifiers we need both native id and model run. AG


#calculating the RMSE and R2 two ways
  #a) comparing the campaign predictions to the observations from the gold standard 
  #b) comparing the predictions from each campaign to the observations from that campaign

#need to append the observations from the balanced short term campaign to do method a

gold_obs <- gold_CV %>%
  dplyr::select("native_id", "log_avg") %>% rename(gold_obs = log_avg)
st_summer_CV <- left_join(st_summer_CV, gold_obs, by = "native_id" ) #to have unique identifiers we need both native id and model run. AG

# Initialize empty lists to store the results for both methods
results_a <- list()
results_b <- list()

# Loop through each unique model_run
for (run in unique(st_summer_CV$model_run)) {
  # Filter the data for the current model_run
  data_subset <- st_summer_CV %>% filter(model_run == run)
  
  # Calculate the MSE for method A
  mse_result_a <- get_MSE(data_subset$gold_obs, data_subset$cvpreds)
  results_a[[run]] <- mse_result_a
  
  # Calculate the MSE for method B
  mse_result_b <- get_MSE(data_subset$log_avg, data_subset$cvpreds)
  results_b[[run]] <- mse_result_b
}

# Convert the lists to a data frame with separate columns for each method
st_summer_MSE <- data.frame(
  model_run = unique(st_summer_CV$model_run),
  RMSE_a = sapply(results_a, `[`, 1),
  R2_a = sapply(results_a, `[`, 2),
  RMSE_b = sapply(results_b, `[`, 1),
  R2_b = sapply(results_b, `[`, 2)
)

#pivot longer for plotting
st_summer_MSE_long <- st_summer_MSE %>%
  pivot_longer(
    cols = c(RMSE_a, R2_a, RMSE_b, R2_b),
    names_to = c("metric", "method"),
    names_sep = "_",
    values_to = "value"
  ) 

#Plotting (replicate figure 1 in blanco paper)
# get range for plot
r <- st_summer_CV %>% dplyr::select(gold_preds, cvpreds) %>% range()


ggplot(data = st_summer_CV, aes(x = gold_preds, y = cvpreds, color = as.factor(model_run))) +
    geom_point(shape = "o", alpha = 0.7) +
    lims(x= r, y = r) +
    coord_fixed() +
    geom_smooth(method = 'lm', se = F, alpha = 0.5) +
    geom_abline(slope = 1, intercept = 0, color = "darkgray", linetype = "dashed") +
    labs(title = "Summer Campaign vs. Gold Standard Cross-validated Predictions", 
         subtitle = "Balanced Campaigns",
         x = "Gold Standard predicted ln(NOx) (ln(ppb))",
         y = "Summer Campaign Predicted ln(NOx) (ln(ppb))",
         caption = "1:1 line is dashed") +
    theme_bw()


#This needs some aesthetic adjustments: choose between dots or lines, pick a color scale that's helpful, get rid of the funky legend, but it's cool!!
    #looks a lot like the balanced predictions from the Blanco paper. 

#replicate blanco figure 3
ggplot(st_summer_MSE_long, aes(x = method, y = value, color = method)) +
  geom_boxplot() +
  geom_point(data = data.frame(method = c("a", "b"), value = gold_MSE[[1]], metric = "RMSE"), aes(x = method, y = value, color = method), size = 3, shape = 5) +
  geom_point(data = data.frame(method = c("a", "b"), value = gold_MSE[[2]], metric = "R2"), aes(x = method, y = value, color = method), size = 3, shape = 5) +
  facet_wrap(~metric) +
  labs(title = "RMSE and MSE_R2 Comparison",
       caption = "Method A = Summer Campaign Predictions Compared to Gold Standard Obs\n
       Method B = Summer Campaign Predictions Compared to Summer Campaign Observations\n
       Square points shows the long-term campaigns performance") +
  theme_bw()


  


```
Comparing the summer short term sampling campaign to the Gold Standard sampling campaign, we see that the summer model under-predicts NOx concentrations.  



```{r fall vs gold}
#------------seasonal comparison with st balanced and gold standard-------------

#-----------------------------------SUMMER--------------------------------------
#need to group the st_summer data by "campaign" before doing regression

#apply the save CV groups as used in the gold standard model
st_summer <- st_summer %>% group_by(rep_number) %>% mutate(CV_grp = CV_grp) %>% ungroup()

#in sample-predictions
fitted_models = st_summer %>% group_by(rep_number) %>% do(model = lm(frml, data = .))

#cross-validated predictions treating each sample "campaign" separately
st_summer_CV <- st_summer %>% group_by(rep_number) %>% do(model = do_CV(data = ., id = "native_id", group = "CV_grp", formula = frml))
#produces a df for each result

#next step is to restructure the result back into a single long_df

st_summer_CV <- st_summer_CV %>% rename(model_run = rep_number) #change column name so there isn't a conflict when I unnest it

st_summer_CV <- st_summer_CV %>%
  unnest(cols = c(model))

#---compare st_summer with gold-----------------------------------------#
#appending the predictions from the balanced short term campaign to the st_summer_CV df for comparison
gold_preds <- gold_CV %>%
  dplyr::select("native_id", "cvpreds") %>% rename(gold_preds = cvpreds)  #select "model run" as well as native_id
st_summer_CV <- left_join(st_summer_CV, gold_preds, by = "native_id" ) #to have unique identifiers we need both native id and model run. AG


#calculating the RMSE and R2 two ways
  #a) comparing the campaign predictions to the observations from the gold standard 
  #b) comparing the predictions from each campaign to the observations from that campaign

#need to append the observations from the balanced short term campaign to do method a

gold_obs <- gold_CV %>%
  dplyr::select("native_id", "log_avg") %>% rename(gold_obs = log_avg)
st_summer_CV <- left_join(st_summer_CV, gold_obs, by = "native_id" ) #to have unique identifiers we need both native id and model run. AG

# Initialize empty lists to store the results for both methods
results_a <- list()
results_b <- list()

# Loop through each unique model_run
for (run in unique(st_summer_CV$model_run)) {
  # Filter the data for the current model_run
  data_subset <- st_summer_CV %>% filter(model_run == run)
  
  # Calculate the MSE for method A
  mse_result_a <- get_MSE(data_subset$gold_obs, data_subset$cvpreds)
  results_a[[run]] <- mse_result_a
  
  # Calculate the MSE for method B
  mse_result_b <- get_MSE(data_subset$log_avg, data_subset$cvpreds)
  results_b[[run]] <- mse_result_b
}

# Convert the lists to a data frame with separate columns for each method
st_summer_MSE <- data.frame(
  model_run = unique(st_summer_CV$model_run),
  RMSE_a = sapply(results_a, `[`, 1),
  R2_a = sapply(results_a, `[`, 2),
  RMSE_b = sapply(results_b, `[`, 1),
  R2_b = sapply(results_b, `[`, 2)
)

#pivot longer for plotting
st_summer_MSE_long <- st_summer_MSE %>%
  pivot_longer(
    cols = c(RMSE_a, R2_a, RMSE_b, R2_b),
    names_to = c("metric", "method"),
    names_sep = "_",
    values_to = "value"
  ) 

#Plotting (replicate figure 1 in blanco paper)
# get range for plot
r <- st_summer_CV %>% dplyr::select(gold_preds, cvpreds) %>% range()


ggplot(data = st_summer_CV, aes(x = gold_preds, y = cvpreds, color = as.factor(model_run))) +
    geom_point(shape = "o", alpha = 0.7) +
    lims(x= r, y = r) +
    coord_fixed() +
    geom_smooth(method = 'lm', se = F, alpha = 0.5) +
    geom_abline(slope = 1, intercept = 0, color = "darkgray", linetype = "dashed") +
    labs(title = "Summer Campaign vs. Gold Standard Cross-validated Predictions", 
         subtitle = "Balanced Campaigns",
         x = "Gold Standard predicted ln(NOx) (ln(ppb))",
         y = "Summer Campaign Predicted ln(NOx) (ln(ppb))",
         caption = "1:1 line is dashed") +
    theme_bw()


#This needs some aesthetic adjustments: choose between dots or lines, pick a color scale that's helpful, get rid of the funky legend, but it's cool!!
    #looks a lot like the balanced predictions from the Blanco paper. 

#replicate blanco figure 3
ggplot(st_summer_MSE_long, aes(x = method, y = value, color = method)) +
  geom_boxplot() +
  geom_point(data = data.frame(method = c("a", "b"), value = gold_MSE[[1]], metric = "RMSE"), aes(x = method, y = value, color = method), size = 3, shape = 5) +
  geom_point(data = data.frame(method = c("a", "b"), value = gold_MSE[[2]], metric = "R2"), aes(x = method, y = value, color = method), size = 3, shape = 5) +
  facet_wrap(~metric) +
  labs(title = "RMSE and MSE_R2 Comparison",
       caption = "Method A = Summer Campaign Predictions Compared to Gold Standard Obs\n
       Method B = Summer Campaign Predictions Compared to Summer Campaign Observations\n
       Square points shows the long-term campaigns performance") +
  theme_bw()


  

```
Comparing the fall short term sampling campaign to the Gold Standard sampling campaign, we see that the fall model slightly over-predicts NOx concentrations.  However, this plot shows fairly close predictions to the Gold Standard model. 

```{r winter v gold}
#------------seasonal comparison with st balanced and gold standard-------------

#-----------------------------------winter--------------------------------------
#need to group the st_winter data by "campaign" before doing regression

#apply the save CV groups as used in the gold standard model
st_winter <- st_winter %>% group_by(rep_number) %>% mutate(CV_grp = CV_grp) %>% ungroup()

#in sample-predictions
fitted_models = st_winter %>% group_by(rep_number) %>% do(model = lm(frml, data = .))

#cross-validated predictions treating each sample "campaign" separately
st_winter_CV <- st_winter %>% group_by(rep_number) %>% do(model = do_CV(data = ., id = "native_id", group = "CV_grp", formula = frml))
#produces a df for each result

#next step is to restructure the result back into a single long_df

st_winter_CV <- st_winter_CV %>% rename(model_run = rep_number) #change column name so there isn't a conflict when I unnest it

st_winter_CV <- st_winter_CV %>%
  unnest(cols = c(model))

#---compare st_winter with gold-----------------------------------------#
#appending the predictions from the balanced short term campaign to the st_winter_CV df for comparison
gold_preds <- gold_CV %>%
  dplyr::select("native_id", "cvpreds") %>% rename(gold_preds = cvpreds)  #select "model run" as well as native_id
st_winter_CV <- left_join(st_winter_CV, gold_preds, by = "native_id" ) #to have unique identifiers we need both native id and model run. AG


#calculating the RMSE and R2 two ways
  #a) comparing the campaign predictions to the observations from the gold standard 
  #b) comparing the predictions from each campaign to the observations from that campaign

#need to append the observations from the balanced short term campaign to do method a

gold_obs <- gold_CV %>%
  dplyr::select("native_id", "log_avg") %>% rename(gold_obs = log_avg)
st_winter_CV <- left_join(st_winter_CV, gold_obs, by = "native_id" ) #to have unique identifiers we need both native id and model run. AG

# Initialize empty lists to store the results for both methods
results_a <- list()
results_b <- list()

# Loop through each unique model_run
for (run in unique(st_winter_CV$model_run)) {
  # Filter the data for the current model_run
  data_subset <- st_winter_CV %>% filter(model_run == run)
  
  # Calculate the MSE for method A
  mse_result_a <- get_MSE(data_subset$gold_obs, data_subset$cvpreds)
  results_a[[run]] <- mse_result_a
  
  # Calculate the MSE for method B
  mse_result_b <- get_MSE(data_subset$log_avg, data_subset$cvpreds)
  results_b[[run]] <- mse_result_b
}

# Convert the lists to a data frame with separate columns for each method
st_winter_MSE <- data.frame(
  model_run = unique(st_winter_CV$model_run),
  RMSE_a = sapply(results_a, `[`, 1),
  R2_a = sapply(results_a, `[`, 2),
  RMSE_b = sapply(results_b, `[`, 1),
  R2_b = sapply(results_b, `[`, 2)
)

#pivot longer for plotting
st_winter_MSE_long <- st_winter_MSE %>%
  pivot_longer(
    cols = c(RMSE_a, R2_a, RMSE_b, R2_b),
    names_to = c("metric", "method"),
    names_sep = "_",
    values_to = "value"
  ) 

#Plotting (replicate figure 1 in blanco paper)
# get range for plot
r <- st_winter_CV %>% dplyr::select(gold_preds, cvpreds) %>% range()


ggplot(data = st_winter_CV, aes(x = gold_preds, y = cvpreds, color = as.factor(model_run))) +
    geom_point(shape = "o", alpha = 0.7) +
    lims(x= r, y = r) +
    coord_fixed() +
    geom_smooth(method = 'lm', se = F, alpha = 0.5) +
    geom_abline(slope = 1, intercept = 0, color = "darkgray", linetype = "dashed") +
    labs(title = "winter Campaign vs. Gold standard Cross-validated Predictions", 
         subtitle = "Balanced Campaigns",
         x = "Gold Standard predicted ln(NOx) (ln(ppb))",
         y = "winter Campaign Predicted ln(NOx) (ln(ppb))",
         caption = "1:1 line is dashed") +
    theme_bw()


#This needs some aesthetic adjustments: choose between dots or lines, pick a color scale that's helpful, get rid of the funky legend, but it's cool!!
    #looks a lot like the balanced predictions from the Blanco paper. 

#replicate blanco figure 3
ggplot(st_winter_MSE_long, aes(x = method, y = value, color = method)) +
  geom_boxplot() +
  geom_point(data = data.frame(method = c("a", "b"), value = gold_MSE[[1]], metric = "RMSE"), aes(x = method, y = value, color = method), size = 3, shape = 5) +
  geom_point(data = data.frame(method = c("a", "b"), value = gold_MSE[[2]], metric = "R2"), aes(x = method, y = value, color = method), size = 3, shape = 5) +
  facet_wrap(~metric) +
  labs(title = "RMSE and MSE_R2 Comparison",
       caption = "Method A = winter Campaign Predictions Compared to  Gold standard Obs\n
       Method B = winter Campaign Predictions Compared to winter Campaign Observations\n
       Square points shows the long-term campaigns performance") +
  theme_bw()



```
Comparing the winter short term sampling campaign to the Gold Standard sampling campaign, we see that the winter model slightly over-predicts NOx concentrations.


```{r spring v gold}
#------------seasonal comparison with st balanced and gold standard-------------

#-----------------------------------spring--------------------------------------
#need to group the st_spring data by "campaign" before doing regression

#apply the save CV groups as used in the gold standard model
st_spring <- st_spring %>% group_by(rep_number) %>% mutate(CV_grp = CV_grp) %>% ungroup()

#in sample-predictions
fitted_models = st_spring %>% group_by(rep_number) %>% do(model = lm(frml, data = .))

#cross-validated predictions treating each sample "campaign" separately
st_spring_CV <- st_spring %>% group_by(rep_number) %>% do(model = do_CV(data = ., id = "native_id", group = "CV_grp", formula = frml))
#produces a df for each result

#next step is to restructure the result back into a single long_df

st_spring_CV <- st_spring_CV %>% rename(model_run = rep_number) #change column name so there isn't a conflict when I unnest it

st_spring_CV <- st_spring_CV %>%
  unnest(cols = c(model))

#---compare st_spring with gold-----------------------------------------#
#appending the predictions from the balanced short term campaign to the st_spring_CV df for comparison
gold_preds <- gold_CV %>%
  dplyr::select("native_id", "cvpreds") %>% rename(gold_preds = cvpreds)  #select "model run" as well as native_id
st_spring_CV <- left_join(st_spring_CV, gold_preds, by = "native_id" ) #to have unique identifiers we need both native id and model run. AG


#calculating the RMSE and R2 two ways
  #a) comparing the campaign predictions to the observations from the gold standard 
  #b) comparing the predictions from each campaign to the observations from that campaign

#need to append the observations from the balanced short term campaign to do method a

gold_obs <- gold_CV %>%
  dplyr::select("native_id", "log_avg") %>% rename(gold_obs = log_avg)
st_spring_CV <- left_join(st_spring_CV, gold_obs, by = "native_id" )

# Initialize empty lists to store the results for both methods
results_a <- list()
results_b <- list()

# Loop through each unique model_run
for (run in unique(st_spring_CV$model_run)) {
  # Filter the data for the current model_run
  data_subset <- st_spring_CV %>% filter(model_run == run)
  
  # Calculate the MSE for method A
  mse_result_a <- get_MSE(data_subset$gold_obs, data_subset$cvpreds)
  results_a[[run]] <- mse_result_a
  
  # Calculate the MSE for method B
  mse_result_b <- get_MSE(data_subset$log_avg, data_subset$cvpreds)
  results_b[[run]] <- mse_result_b
}

# Convert the lists to a data frame with separate columns for each method
st_spring_MSE <- data.frame(
  model_run = unique(st_spring_CV$model_run),
  RMSE_a = sapply(results_a, `[`, 1),
  R2_a = sapply(results_a, `[`, 2),
  RMSE_b = sapply(results_b, `[`, 1),
  R2_b = sapply(results_b, `[`, 2)
)

#pivot longer for plotting
st_spring_MSE_long <- st_spring_MSE %>%
  pivot_longer(
    cols = c(RMSE_a, R2_a, RMSE_b, R2_b),
    names_to = c("metric", "method"),
    names_sep = "_",
    values_to = "value"
  ) 

#Plotting (replicate figure 1 in blanco paper)
# get range for plot
r <- st_spring_CV %>% dplyr::select(gold_preds, cvpreds) %>% range()


ggplot(data = st_spring_CV, aes(x = gold_preds, y = cvpreds, color = as.factor(model_run))) +
    geom_point(shape = "o", alpha = 0.7) +
    lims(x= r, y = r) +
    coord_fixed() +
    geom_smooth(method = 'lm', se = F, alpha = 0.5) +
    geom_abline(slope = 1, intercept = 0, color = "darkgray", linetype = "dashed") +
    labs(title = "spring Campaign vs. Gold Standard Cross-validated Predictions", 
         subtitle = "Balanced Campaigns",
         x = "short term balanced predicted ln(NOx) (ln(ppb))",
         y = "spring Campaign Predicted ln(NOx) (ln(ppb))",
         caption = "1:1 line is dashed") +
    theme_bw()


#This needs some aesthetic adjustments: choose between dots or lines, pick a color scale that's helpful, get rid of the funky legend, but it's cool!!
    #looks a lot like the balanced predictions from the Blanco paper. 

#replicate blanco figure 3
ggplot(st_spring_MSE_long, aes(x = method, y = value, color = method)) +
  geom_boxplot() +
  geom_point(data = data.frame(method = c("a", "b"), value = gold_MSE[[1]], metric = "RMSE"), aes(x = method, y = value, color = method), size = 3, shape = 5) +
  geom_point(data = data.frame(method = c("a", "b"), value = gold_MSE[[2]], metric = "R2"), aes(x = method, y = value, color = method), size = 3, shape = 5) +
  facet_wrap(~metric) +
  labs(title = "RMSE and MSE_R2 Comparison",
       caption = "Method A = spring Campaign Predictions Compared to Gold Standard Obs\n
       Method B = spring Campaign Predictions Compared to spring Campaign Observations\n
       Square points shows the long-term campaigns performance") +
  theme_bw()

```
Comparing the spring short term sampling campaign to the Gold Standard sampling campaign, we see that the spring model under-predicts NOx concentrations. The spring model's predictions appear to be the farthest form the gold-standard predictions.



#--------comparing seasonal short term campaigns with short term balanced campaign--------------------



```{r summer vs balanced}
#------------seasonal comparison with st balanced and gold standard-------------

#-----------------------------------SUMMER--------------------------------------
#need to group the st_summer data by "campaign" before doing regression

#apply the save CV groups as used in the gold standard model
st_summer <- st_summer %>% group_by(rep_number) %>% mutate(CV_grp = CV_grp) %>% ungroup()

#in sample-predictions
fitted_models = st_summer %>% group_by(rep_number) %>% do(model = lm(frml, data = .))

#cross-validated predictions treating each sample "campaign" separately
st_summer_CV <- st_summer %>% group_by(rep_number) %>% do(model = do_CV(data = ., id = "native_id", group = "CV_grp", formula = frml))
#produces a df for each result

#next step is to restructure the result back into a single long_df

st_summer_CV <- st_summer_CV %>% rename(model_run = rep_number) #change column name so there isn't a conflict when I unnest it

st_summer_CV <- st_summer_CV %>%
  unnest(cols = c(model))

#---compare st_summer with st_balanced-----------------------------------------#
#appending the predictions from the balanced short term campaign to the st_summer_CV df for comparison
st_balanced_preds <- st_balanced_CV %>%
  dplyr::select("native_id", "cvpreds", "model_run") %>% rename(st_balanced_preds = cvpreds)  #select "model run" as well as native_id
st_summer_CV <- left_join(st_summer_CV, st_balanced_preds, by = c("native_id", "model_run") ) #to have unique identifiers we need both native id and model run. AG


#calculating the RMSE and R2 two ways
  #a) comparing the campaign predictions to the observations from the gold standard 
  #b) comparing the predictions from each campaign to the observations from that campaign

#need to append the observations from the balanced short term campaign to do method a

st_balanced_obs <- st_balanced_CV %>%
  dplyr::select("native_id", "log_avg", "model_run") %>% rename(st_balanced_obs = log_avg)
st_summer_CV <- left_join(st_summer_CV, st_balanced_obs, by = c("native_id", "model_run") ) #to have unique identifiers we need both native id and model run. AG

# Initialize empty lists to store the results for both methods
results_a <- list()
results_b <- list()

# Loop through each unique model_run
for (run in unique(st_summer_CV$model_run)) {
  # Filter the data for the current model_run
  data_subset <- st_summer_CV %>% filter(model_run == run)
  
  # Calculate the MSE for method A
  mse_result_a <- get_MSE(data_subset$st_balanced_obs, data_subset$cvpreds)
  results_a[[run]] <- mse_result_a
  
  # Calculate the MSE for method B
  mse_result_b <- get_MSE(data_subset$log_avg, data_subset$cvpreds)
  results_b[[run]] <- mse_result_b
}

# Convert the lists to a data frame with separate columns for each method
st_summer_MSE <- data.frame(
  model_run = unique(st_summer_CV$model_run),
  RMSE_a = sapply(results_a, `[`, 1),
  R2_a = sapply(results_a, `[`, 2),
  RMSE_b = sapply(results_b, `[`, 1),
  R2_b = sapply(results_b, `[`, 2)
)

#pivot longer for plotting
st_summer_MSE_long <- st_summer_MSE %>%
  pivot_longer(
    cols = c(RMSE_a, R2_a, RMSE_b, R2_b),
    names_to = c("metric", "method"),
    names_sep = "_",
    values_to = "value"
  ) 

#Plotting (replicate figure 1 in blanco paper)
# get range for plot
r <- st_summer_CV %>% dplyr::select(st_balanced_preds, cvpreds) %>% range()


ggplot(data = st_summer_CV, aes(x = st_balanced_preds, y = cvpreds, color = as.factor(model_run))) +
    geom_point(shape = "o", alpha = 0.7) +
    lims(x= r, y = r) +
    coord_fixed() +
    geom_smooth(method = 'lm', se = F, alpha = 0.5) +
    geom_abline(slope = 1, intercept = 0, color = "darkgray", linetype = "dashed") +
    labs(title = "Summer Campaign vs. short term balanced Cross-validated Predictions", 
         subtitle = "Balanced Campaigns",
         x = "short term balanced predicted ln(NOx) (ln(ppb))",
         y = "Summer Campaign Predicted ln(NOx) (ln(ppb))",
         caption = "1:1 line is dashed") +
    theme_bw()


#This needs some aesthetic adjustments: choose between dots or lines, pick a color scale that's helpful, get rid of the funky legend, but it's cool!!
    #looks a lot like the balanced predictions from the Blanco paper. 

#replicate blanco figure 3
ggplot(st_summer_MSE_long, aes(x = method, y = value, color = method)) +
  geom_boxplot() +
  geom_point(data = data.frame(method = c("a", "b"), value = gold_MSE[[1]], metric = "RMSE"), aes(x = method, y = value, color = method), size = 3, shape = 5) +
  geom_point(data = data.frame(method = c("a", "b"), value = gold_MSE[[2]], metric = "R2"), aes(x = method, y = value, color = method), size = 3, shape = 5) +
  facet_wrap(~metric) +
  labs(title = "RMSE and MSE_R2 Comparison",
       caption = "Method A = Summer Campaign Predictions Compared to short term balanced Obs\n
       Method B = Summer Campaign Predictions Compared to Summer Campaign Observations\n
       Square points shows the long-term campaigns performance") +
  theme_bw()


```


```{r fall vs balanced}
#------------seasonal comparison with st balanced and gold standard-------------

#-----------------------------------fall--------------------------------------
#need to group the st_fall data by "campaign" before doing regression

#apply the save CV groups as used in the gold standard model
st_fall <- st_fall %>% group_by(rep_number) %>% mutate(CV_grp = CV_grp) %>% ungroup()

#in sample-predictions
fitted_models = st_fall %>% group_by(rep_number) %>% do(model = lm(frml, data = .))

#cross-validated predictions treating each sample "campaign" separately
st_fall_CV <- st_fall %>% group_by(rep_number) %>% do(model = do_CV(data = ., id = "native_id", group = "CV_grp", formula = frml))
#produces a df for each result

#next step is to restructure the result back into a single long_df

st_fall_CV <- st_fall_CV %>% rename(model_run = rep_number) #change column name so there isn't a conflict when I unnest it

st_fall_CV <- st_fall_CV %>%
  unnest(cols = c(model))

#---compare st_fall with st_balanced-----------------------------------------#
#appending the predictions from the balanced short term campaign to the st_fall_CV df for comparison
st_balanced_preds <- st_balanced_CV %>%
  dplyr::select("native_id", "cvpreds", "model_run") %>% rename(st_balanced_preds = cvpreds)  #select "model run" as well as native_id
st_fall_CV <- left_join(st_fall_CV, st_balanced_preds, by = c("native_id", "model_run") ) #to have unique identifiers we need both native id and model run. AG


#calculating the RMSE and R2 two ways
  #a) comparing the campaign predictions to the observations from the gold standard 
  #b) comparing the predictions from each campaign to the observations from that campaign

#need to append the observations from the balanced short term campaign to do method a

st_balanced_obs <- st_balanced_CV %>%
  dplyr::select("native_id", "log_avg", "model_run") %>% rename(st_balanced_obs = log_avg)
st_fall_CV <- left_join(st_fall_CV, st_balanced_obs, by = c("native_id", "model_run") ) #to have unique identifiers we need both native id and model run. AG

# Initialize empty lists to store the results for both methods
results_a <- list()
results_b <- list()

# Loop through each unique model_run
for (run in unique(st_fall_CV$model_run)) {
  # Filter the data for the current model_run
  data_subset <- st_fall_CV %>% filter(model_run == run)
  
  # Calculate the MSE for method A
  mse_result_a <- get_MSE(data_subset$st_balanced_obs, data_subset$cvpreds)
  results_a[[run]] <- mse_result_a
  
  # Calculate the MSE for method B
  mse_result_b <- get_MSE(data_subset$log_avg, data_subset$cvpreds)
  results_b[[run]] <- mse_result_b
}

# Convert the lists to a data frame with separate columns for each method
st_fall_MSE <- data.frame(
  model_run = unique(st_fall_CV$model_run),
  RMSE_a = sapply(results_a, `[`, 1),
  R2_a = sapply(results_a, `[`, 2),
  RMSE_b = sapply(results_b, `[`, 1),
  R2_b = sapply(results_b, `[`, 2)
)

#pivot longer for plotting
st_fall_MSE_long <- st_fall_MSE %>%
  pivot_longer(
    cols = c(RMSE_a, R2_a, RMSE_b, R2_b),
    names_to = c("metric", "method"),
    names_sep = "_",
    values_to = "value"
  ) 

#Plotting (replicate figure 1 in blanco paper)
# get range for plot
r <- st_fall_CV %>% dplyr::select(st_balanced_preds, cvpreds) %>% range()


ggplot(data = st_fall_CV, aes(x = st_balanced_preds, y = cvpreds, color = as.factor(model_run))) +
    geom_point(shape = "o", alpha = 0.7) +
    lims(x= r, y = r) +
    coord_fixed() +
    geom_smooth(method = 'lm', se = F, alpha = 0.5) +
    geom_abline(slope = 1, intercept = 0, color = "darkgray", linetype = "dashed") +
    labs(title = "Fall Campaign vs. short term balanced Cross-validated Predictions", 
         subtitle = "Balanced Campaigns",
         x = "short term balanced predicted ln(NOx) (ln(ppb))",
         y = "Fall Campaign Predicted ln(NOx) (ln(ppb))",
         caption = "1:1 line is dashed") +
    theme_bw()


#This needs some aesthetic adjustments: choose between dots or lines, pick a color scale that's helpful, get rid of the funky legend, but it's cool!!
    #looks a lot like the balanced predictions from the Blanco paper. 

#replicate blanco figure 3
ggplot(st_fall_MSE_long, aes(x = method, y = value, color = method)) +
  geom_boxplot() +
  geom_point(data = data.frame(method = c("a", "b"), value = gold_MSE[[1]], metric = "RMSE"), aes(x = method, y = value, color = method), size = 3, shape = 5) +
  geom_point(data = data.frame(method = c("a", "b"), value = gold_MSE[[2]], metric = "R2"), aes(x = method, y = value, color = method), size = 3, shape = 5) +
  facet_wrap(~metric) +
  labs(title = "RMSE and MSE_R2 Comparison",
       caption = "Method A = fall Campaign Predictions Compared to short term balanced Obs\n
       Method B = Fall Campaign Predictions Compared to fall Campaign Observations\n
       Square points shows the long-term campaigns performance") +
  theme_bw()

```
The fall and balanced short term sampling models seem to have decent agreement in their NOx predictions. 


```{r winter v balanced}
#------------seasonal comparison with st balanced and gold standard-------------

#-----------------------------------winter--------------------------------------
#need to group the st_winter data by "campaign" before doing regression

#apply the save CV groups as used in the gold standard model
st_winter <- st_winter %>% group_by(rep_number) %>% mutate(CV_grp = CV_grp) %>% ungroup()

#in sample-predictions
fitted_models = st_winter %>% group_by(rep_number) %>% do(model = lm(frml, data = .))

#cross-validated predictions treating each sample "campaign" separately
st_winter_CV <- st_winter %>% group_by(rep_number) %>% do(model = do_CV(data = ., id = "native_id", group = "CV_grp", formula = frml))
#produces a df for each result

#next step is to restructure the result back into a single long_df

st_winter_CV <- st_winter_CV %>% rename(model_run = rep_number) #change column name so there isn't a conflict when I unnest it

st_winter_CV <- st_winter_CV %>%
  unnest(cols = c(model))

#---compare st_winter with st_balanced-----------------------------------------#
#appending the predictions from the balanced short term campaign to the st_winter_CV df for comparison
st_balanced_preds <- st_balanced_CV %>%
  dplyr::select("native_id", "cvpreds", "model_run") %>% rename(st_balanced_preds = cvpreds)  #select "model run" as well as native_id
st_winter_CV <- left_join(st_winter_CV, st_balanced_preds, by = c("native_id", "model_run") ) #to have unique identifiers we need both native id and model run. AG


#calculating the RMSE and R2 two ways
  #a) comparing the campaign predictions to the observations from the gold standard 
  #b) comparing the predictions from each campaign to the observations from that campaign

#need to append the observations from the balanced short term campaign to do method a

st_balanced_obs <- st_balanced_CV %>%
  dplyr::select("native_id", "log_avg", "model_run") %>% rename(st_balanced_obs = log_avg)
st_winter_CV <- left_join(st_winter_CV, st_balanced_obs, by = c("native_id", "model_run") ) #to have unique identifiers we need both native id and model run. AG

# Initialize empty lists to store the results for both methods
results_a <- list()
results_b <- list()

# Loop through each unique model_run
for (run in unique(st_winter_CV$model_run)) {
  # Filter the data for the current model_run
  data_subset <- st_winter_CV %>% filter(model_run == run)
  
  # Calculate the MSE for method A
  mse_result_a <- get_MSE(data_subset$st_balanced_obs, data_subset$cvpreds)
  results_a[[run]] <- mse_result_a
  
  # Calculate the MSE for method B
  mse_result_b <- get_MSE(data_subset$log_avg, data_subset$cvpreds)
  results_b[[run]] <- mse_result_b
}

# Convert the lists to a data frame with separate columns for each method
st_winter_MSE <- data.frame(
  model_run = unique(st_winter_CV$model_run),
  RMSE_a = sapply(results_a, `[`, 1),
  R2_a = sapply(results_a, `[`, 2),
  RMSE_b = sapply(results_b, `[`, 1),
  R2_b = sapply(results_b, `[`, 2)
)

#pivot longer for plotting
st_winter_MSE_long <- st_winter_MSE %>%
  pivot_longer(
    cols = c(RMSE_a, R2_a, RMSE_b, R2_b),
    names_to = c("metric", "method"),
    names_sep = "_",
    values_to = "value"
  ) 

#Plotting (replicate figure 1 in blanco paper)
# get range for plot
r <- st_winter_CV %>% dplyr::select(st_balanced_preds, cvpreds) %>% range()


ggplot(data = st_winter_CV, aes(x = st_balanced_preds, y = cvpreds, color = as.factor(model_run))) +
    geom_point(shape = "o", alpha = 0.7) +
    lims(x= r, y = r) +
    coord_fixed() +
    geom_smooth(method = 'lm', se = F, alpha = 0.5) +
    geom_abline(slope = 1, intercept = 0, color = "darkgray", linetype = "dashed") +
    labs(title = "winter Campaign vs. short term balanced Cross-validated Predictions", 
         subtitle = "Balanced Campaigns",
         x = "short term balanced predicted ln(NOx) (ln(ppb))",
         y = "winter Campaign Predicted ln(NOx) (ln(ppb))",
         caption = "1:1 line is dashed") +
    theme_bw()


#This needs some aesthetic adjustments: choose between dots or lines, pick a color scale that's helpful, get rid of the funky legend, but it's cool!!
    #looks a lot like the balanced predictions from the Blanco paper. 

#replicate blanco figure 3
ggplot(st_winter_MSE_long, aes(x = method, y = value, color = method)) +
  geom_boxplot() +
  geom_point(data = data.frame(method = c("a", "b"), value = gold_MSE[[1]], metric = "RMSE"), aes(x = method, y = value, color = method), size = 3, shape = 5) +
  geom_point(data = data.frame(method = c("a", "b"), value = gold_MSE[[2]], metric = "R2"), aes(x = method, y = value, color = method), size = 3, shape = 5) +
  facet_wrap(~metric) +
  labs(title = "RMSE and MSE_R2 Comparison",
       caption = "Method A = winter Campaign Predictions Compared to short term balanced Obs\n
       Method B = winter Campaign Predictions Compared to winter Campaign Observations\n
       Square points shows the long-term campaigns performance") +
  theme_bw()



```
Winter and balanced predictions seem to have pretty decent agreemeent.


```{r spring v balanced}
#------------seasonal comparison with st balanced and gold standard-------------

#-----------------------------------spring--------------------------------------
#need to group the st_spring data by "campaign" before doing regression

#apply the save CV groups as used in the gold standard model
st_spring <- st_spring %>% group_by(rep_number) %>% mutate(CV_grp = CV_grp) %>% ungroup()

#in sample-predictions
fitted_models = st_spring %>% group_by(rep_number) %>% do(model = lm(frml, data = .))

#cross-validated predictions treating each sample "campaign" separately
st_spring_CV <- st_spring %>% group_by(rep_number) %>% do(model = do_CV(data = ., id = "native_id", group = "CV_grp", formula = frml))
#produces a df for each result

#next step is to restructure the result back into a single long_df

st_spring_CV <- st_spring_CV %>% rename(model_run = rep_number) #change column name so there isn't a conflict when I unnest it

st_spring_CV <- st_spring_CV %>%
  unnest(cols = c(model))

#---compare st_spring with st_balanced-----------------------------------------#
#appending the predictions from the balanced short term campaign to the st_spring_CV df for comparison
st_balanced_preds <- st_balanced_CV %>%
  dplyr::select("native_id", "cvpreds", "model_run") %>% rename(st_balanced_preds = cvpreds)  #select "model run" as well as native_id
st_spring_CV <- left_join(st_spring_CV, st_balanced_preds, by = c("native_id", "model_run") ) #to have unique identifiers we need both native id and model run. AG


#calculating the RMSE and R2 two ways
  #a) comparing the campaign predictions to the observations from the gold standard 
  #b) comparing the predictions from each campaign to the observations from that campaign

#need to append the observations from the balanced short term campaign to do method a

st_balanced_obs <- st_balanced_CV %>%
  dplyr::select("native_id", "log_avg", "model_run") %>% rename(st_balanced_obs = log_avg)
st_spring_CV <- left_join(st_spring_CV, st_balanced_obs, by = c("native_id", "model_run") ) #to have unique identifiers we need both native id and model run. AG

# Initialize empty lists to store the results for both methods
results_a <- list()
results_b <- list()

# Loop through each unique model_run
for (run in unique(st_spring_CV$model_run)) {
  # Filter the data for the current model_run
  data_subset <- st_spring_CV %>% filter(model_run == run)
  
  # Calculate the MSE for method A
  mse_result_a <- get_MSE(data_subset$st_balanced_obs, data_subset$cvpreds)
  results_a[[run]] <- mse_result_a
  
  # Calculate the MSE for method B
  mse_result_b <- get_MSE(data_subset$log_avg, data_subset$cvpreds)
  results_b[[run]] <- mse_result_b
}

# Convert the lists to a data frame with separate columns for each method
st_spring_MSE <- data.frame(
  model_run = unique(st_spring_CV$model_run),
  RMSE_a = sapply(results_a, `[`, 1),
  R2_a = sapply(results_a, `[`, 2),
  RMSE_b = sapply(results_b, `[`, 1),
  R2_b = sapply(results_b, `[`, 2)
)

#pivot longer for plotting
st_spring_MSE_long <- st_spring_MSE %>%
  pivot_longer(
    cols = c(RMSE_a, R2_a, RMSE_b, R2_b),
    names_to = c("metric", "method"),
    names_sep = "_",
    values_to = "value"
  ) 

#Plotting (replicate figure 1 in blanco paper)
# get range for plot
r <- st_spring_CV %>% dplyr::select(st_balanced_preds, cvpreds) %>% range()


ggplot(data = st_spring_CV, aes(x = st_balanced_preds, y = cvpreds, color = as.factor(model_run))) +
    geom_point(shape = "o", alpha = 0.7) +
    lims(x= r, y = r) +
    coord_fixed() +
    geom_smooth(method = 'lm', se = F, alpha = 0.5) +
    geom_abline(slope = 1, intercept = 0, color = "darkgray", linetype = "dashed") +
    labs(title = "spring Campaign vs. short term balanced Cross-validated Predictions", 
         subtitle = "Balanced Campaigns",
         x = "short term balanced predicted ln(NOx) (ln(ppb))",
         y = "spring Campaign Predicted ln(NOx) (ln(ppb))",
         caption = "1:1 line is dashed") +
    theme_bw()


#This needs some aesthetic adjustments: choose between dots or lines, pick a color scale that's helpful, get rid of the funky legend, but it's cool!!
    #looks a lot like the balanced predictions from the Blanco paper. 

#replicate blanco figure 3
ggplot(st_spring_MSE_long, aes(x = method, y = value, color = method)) +
  geom_boxplot() +
  geom_point(data = data.frame(method = c("a", "b"), value = gold_MSE[[1]], metric = "RMSE"), aes(x = method, y = value, color = method), size = 3, shape = 5) +
  geom_point(data = data.frame(method = c("a", "b"), value = gold_MSE[[2]], metric = "R2"), aes(x = method, y = value, color = method), size = 3, shape = 5) +
  facet_wrap(~metric) +
  labs(title = "RMSE and MSE_R2 Comparison",
       caption = "Method A = spring Campaign Predictions Compared to short term balanced Obs\n
       Method B = spring Campaign Predictions Compared to spring Campaign Observations\n
       Square points shows the long-term campaigns performance") +
  theme_bw()

```

#tabulate RMSE and R sqaured for all campaigns:
```{r basic obs vs preds plot}
ggplot( st_balanced_CV, aes(x = log_avg, cvpreds)) +
  geom_point() +
  geom_smooth(method = "lm") +
  ggtitle("observations vs predictions - short term balanced campaign") +
  xlab("Observed log(NOx) annual averages") +
  ylab("Predicted log(NOx) annual averages ") +
  geom_abline(slope = 1.0) 

ggplot( st_summer_CV, aes(x = log_avg, cvpreds)) +
  geom_point() +
  geom_smooth(method = "lm") +
  ggtitle("observations vs predictions - short term Summer campaign") +
  xlab("Observed log(NOx) seasonal averages") +
  ylab("Predicted log(NOx) seasonal averages ") +
  geom_abline(slope = 1.0)

ggplot( st_fall_CV, aes(x = log_avg, cvpreds)) +
  geom_point() +
  geom_smooth(method = "lm") +
  ggtitle("observations vs predictions - short term Fall campaign") +
  xlab("Observed log(NOx) seasonal averages") +
  ylab("Predicted log(NOx) seasonal averages ") +
  geom_abline(slope = 1.0)

ggplot( st_winter_CV, aes(x = log_avg, cvpreds)) +
  geom_point() +
  geom_smooth(method = "lm") +
  ggtitle("observations vs predictions - short term Winter campaign") +
  xlab("Observed log(NOx) seasonal averages") +
  ylab("Predicted log(NOx) seasonal averages ") +
  geom_abline(slope = 1.0)

ggplot( st_spring_CV, aes(x = log_avg, cvpreds)) +
  geom_point() +
  geom_smooth(method = "lm") +
  ggtitle("observations vs predictions - short term Spring campaign") +
  xlab("Observed log(NOx) seasonal averages") +
  ylab("Predicted log(NOx) seasonal averages ") +
  geom_abline(slope = 1.0)

```


```{r table of prediction stats}
#averages the rmse and r squared across all 30 repeats of the random sampling for all campaigns

campaigns <- c("Summer", "Fall", "Winter", "Spring", "Balanced")
RMSE <- c(mean(st_summer_MSE$RMSE_a), mean(st_fall_MSE$RMSE_a), mean(st_winter_MSE$RMSE_a), mean(st_spring_MSE$RMSE_a), mean(st_balanced_MSE$RMSE_a)) %>% round(digits = 3)
R2 <- c(mean(st_summer_MSE$R2_a), mean(st_fall_MSE$R2_a), mean(st_winter_MSE$R2_a), mean(st_spring_MSE$R2_a), mean(st_balanced_MSE$R2_a)) %>% round(digits = 3)

kable(rbind(campaigns, RMSE, R2),
      digits = 3,
      caption = "Table 3.0: Performance Statistics of Seasonal and Balanced Sampling Campaigns - Method A") %>% kable_styling()

```
Based on the R squared and RMSE values in Table 3.0 we may conclude that the winter short term model performed better than the short term balanced model. 
**I think that the balanced was better than the winter because the balanced had a higher R2 and lower RMSE than the winter models. balanced R2 seems pretty decent (compared to what we saw previously) -ck**






#-------------------Sensitivity Analysis: Restricting data to LA county --------------------------------------

``` {r LA strat}
#----------create LA-specific dataset---------------

la <-cal_nox_covars %>% filter(parameter_name == "Oxides of nitrogen (NOx)") %>%
  filter(County=="Los Angeles")

head(la)

#NOTE: updated this since it wasn't loading with data without the "county" variable. it seems to look better now but we may want to move all the other extraneous covariates? -ck
```

# creating long term LA data set
```{r long term gold standard la dataset}

#Gold Standard Data
long_term_gold_la <- annual_avg(la)


```




#creating short term LA data set
```{r short term LA dataset}
#----------generate short-term LA dataset---------------

#random_sample() (data, size, seed)
# Initialize the short-term balanced data frame
st_la <- data.frame()

n_reps <- 30  # number of random df simulations to run

for (i in 1:n_reps) {
  seed <- i * 4  # generating a different seed for each simulation
  sampled_data <- random_sample(la, 28, seed)
  la_avg_data <- seasonal_avg(sampled_data)
  
  # Add seed and rep_number columns
  la_avg_data$seed <- seed
  la_avg_data$rep_number <- i
  
  # Append to st summer df
  st_la <- rbind(st_la, la_avg_data)
}
ggplot(st_la, aes(x = native_id, y = log_avg)) +
  geom_boxplot()+
  ggtitle("Figure X:Los Angeles County Short Term Sensor Log-transformed Annual Averages \n30 sets of 28 samples per sensor")

```

#quick summary of long-term and short-term LA datasets
```{r summarize LA short term dataset}

#summary stats for st_la
st_la_sum <- st_la %>% summarise(min = min(seasonal_avg), 
                                            Q25 = quantile(seasonal_avg, probs = 0.25), 
                                            Q50 = quantile(seasonal_avg, probs = 0.5),
                                            Q75 = quantile(seasonal_avg, probs = 0.75),
                                            max = max(seasonal_avg),
                                            SD = sd(seasonal_avg)
                                            )

#summary stats for lt_gold_sum_la
lt_gold_sum_la <- long_term_gold_la %>% summarise(min = min(annual_avg), 
                                            Q25 = quantile(annual_avg, probs = 0.25), 
                                            Q50 = quantile(annual_avg, probs = 0.5),
                                            Q75 = quantile(annual_avg, probs = 0.75),
                                            max = max(annual_avg),
                                            SD = sd(annual_avg)
                                            )


#combined table short and long term LA (balanced datasets)

names_la <- c("short-term balanced", "long-term balanced")
kable(cbind(names_la , rbind(st_la_sum, lt_gold_sum_la)), digits = 2,
      caption = " Short- and Long-Term Balanced Sampling Campaigns - NOx Summaries (Los Angeles) ") %>%
  kable_styling()

```
```{r la.convariates}
#--------------------------Common Model on LA datasets------------------#

#adding the covariates to do some predictions
st_la<-  left_join(st_la,common_model_cov, by = "native_id")

long_term_gold_la <- left_join(long_term_gold_la,common_model_cov, by = "native_id")

```

#running a regression model using common model covariates and LA data sets
```{r}

# in-sample predictions and fit summary
summary(lm_st_la <- lm(frml, data = st_la)) #short term balanced data set

```
#Create cross-validation groups that can be used for all the data
# trying leave-one-out to avoid too small of groups
```{r LOO cross validation groups}

CV_grp_la <- rep(1:12, length.out = nrow(long_term_gold_la)) %>% 
  sample(replace = FALSE)

# now append it to the cal_nox_clean data frame
long_term_gold_la <- mutate(long_term_gold_la, CV_grp_la = CV_grp_la)


#doing gold cv within LA dataset for long term gold standard model
gold_CV_la <- do_CV(long_term_gold_la, id = "native_id", group = "CV_grp_la", formula = frml)

gold_MSE_la <- get_MSE(gold_CV_la$log_avg, gold_CV_la$cvpreds)


# now make groups for short term data sets
CV_grp_st_la <- rep(1:360, length.out = nrow(st_la)) %>% 
  sample(replace = FALSE)




```



#st balanced vs gold standard - LA county using 
```{r la short-term balanced versus gold standard}
#------------LAshort-term balanced versus gold standard-------------

#need to group the st_la data by "campaign" before doing regression

#apply the save CV groups as used in the gold standard model
st_la <- st_la %>% #group_by(rep_number) %>% #abbie had to remove this to have the leave one out groups append. 
  mutate(CV_grp_st_la = CV_grp_st_la) %>% 
  ungroup()

#in sample-predictions
fitted_models = st_la %>% group_by(rep_number) %>% 
  do(model = lm(frml, data = .))

#cross-validated predictions treating each sample "campaign" separately
st_la_CV <- st_la %>% group_by(rep_number) %>%
  do(model = do_CV(data = ., id = "native_id", group = "CV_grp_st_la", formula = frml))
#produces a df for each result

#next step is to restructure the result back into a single long_df

st_la_CV <- st_la_CV %>% rename(model_run = rep_number)

st_la_CV <- st_la_CV %>%
  unnest(cols = c(model))

#---compare st_la with gold la--------

#appending the predictions from the balanced short term campaign to the st_la_CV df for comparison
gold_preds_la <- gold_CV_la %>%
  dplyr::select("native_id", "cvpreds") %>% rename(gold_preds_la = cvpreds)  
st_la_CV <- left_join(st_la_CV, gold_preds_la, by = "native_id" ) 


#calculating the RMSE and R2 two ways
  #a) comparing the campaign predictions to the observations from the gold standard 
  #b) comparing the predictions from each campaign to the observations from that campaign

#need to append the observations from the balanced short term campaign to do method a

gold_obs_la <- gold_CV_la %>%
  dplyr::select("native_id", "log_avg") %>% rename(gold_obs_la = log_avg)
st_la_CV <- left_join(st_la_CV, gold_obs_la, by = "native_id" )

# Initialize empty lists to store the results for both methods
results_a <- list()
results_b <- list()

# Loop through each unique model_run
for (run in unique(st_la_CV$model_run)) {
  # Filter the data for the current model_run
  data_subset <- st_la_CV %>% filter(model_run == run)
  
  # Calculate the MSE for method A
  mse_result_a <- get_MSE(data_subset$gold_obs_la, data_subset$cvpreds)
  results_a[[run]] <- mse_result_a
  
  # Calculate the MSE for method B
  mse_result_b <- get_MSE(data_subset$log_avg, data_subset$cvpreds)
  results_b[[run]] <- mse_result_b
}

# Convert the lists to a data frame with separate columns for each method
st_la_MSE <- data.frame(
  model_run = unique(st_la_CV$model_run),
  RMSE_a = sapply(results_a, `[`, 1),
  R2_a = sapply(results_a, `[`, 2),
  RMSE_b = sapply(results_b, `[`, 1),
  R2_b = sapply(results_b, `[`, 2)
)

#pivot longer for plotting
st_la_MSE_long <- st_la_MSE %>%
  pivot_longer(
    cols = c(RMSE_a, R2_a, RMSE_b, R2_b),
    names_to = c("metric", "method"),
    names_sep = "_",
    values_to = "value"
  ) 

#Plotting (replicate figure 1 in blanco paper)
# get range for plot
r <- st_la_CV %>% dplyr::select(gold_preds_la, cvpreds) %>% range()


ggplot(data = st_la_CV, aes(x = gold_preds_la, y = cvpreds, color = as.factor(model_run))) +
    geom_point(shape = "o", alpha = 0.7) +
    lims(x= r, y = r) +
    coord_fixed() +
    geom_smooth(method = 'lm', se = F, alpha = 0.5) +
    geom_abline(slope = 1, intercept = 0, color = "darkgray", linetype = "dashed") +
    labs(title = "Long vs. short term balanced Cross-validated Predictions in LA", 
         subtitle = "Balanced Campaigns",
         x = "short term balanced predicted ln(NOx) (ln(ppb))",
         y = "Long term Campaign Predicted ln(NOx) (ln(ppb))",
         caption = "1:1 line is dashed") +
    theme_bw()

#replicate blanco figure 3
ggplot(st_la_MSE_long, aes(x = method, y = value, color = method)) +
  geom_boxplot() +
  geom_point(data = data.frame(method = c("a", "b"), value = gold_MSE[[1]], metric = "RMSE"), aes(x = method, y = value, color = method), size = 3, shape = 5) +
  geom_point(data = data.frame(method = c("a", "b"), value = gold_MSE[[2]], metric = "R2"), aes(x = method, y = value, color = method), size = 3, shape = 5) +
  facet_wrap(~metric) +
  labs(title = "RMSE and MSE_R2 Comparison",
       caption = "Method A = Short-term Balanced Predictions Compared to Gold Standard Obs\n
       Method B = Short-Term Campaign Predictions Compared to Short-term Campaign Observations\n
       Square points shows the long-term campaigns performance") +
  theme_bw()

#NOTE: can someone please confirm that I got the captions right here? -ck

```



#------------Seasonal campaigns restricted to LA county------------------------

``` {r seasonal strat}
#----------create season-specific strata---------------
summer_la <- la[la$season == "Summer",]

fall_la <- la[la$season == "Fall",]

winter_la <- la[la$season == "Winter",] 

spring_la <- la[la$season == "Spring",]



```


#creating short term seasonal data sets
```{r seasonal datasets}


#random_sample() (data, size, seed)
#----------summer_la----------------------------------------------
# Initialize the short-term balanced data frame
st_summer_la <- data.frame()

n_reps <- 30  # number of random df simulations to run

for (i in 1:n_reps) {
  seed <- i * 4  # generating a different seed for each simulation
  sampled_data <- random_sample(summer_la, 28, seed)
  summer_la_avg_data <- seasonal_avg(sampled_data)
  
  # Add seed and rep_number columns
  summer_la_avg_data$seed <- seed
  summer_la_avg_data$rep_number <- i
  
  # Append to st summer_la df
  st_summer_la <- rbind(st_summer_la, summer_la_avg_data)
}
ggplot(st_summer_la, aes(x = native_id, y = log_avg)) +
  geom_boxplot()+
  ggtitle("Figure X: summer_la Short Term Sensor Log-transformed Annual Averages \n30 sets of 28 samples per sensor")


#----------fall_la----------------------------------------------
# Initialize the short-term balanced data frame
st_fall_la <- data.frame()

n_reps <- 30  # number of random df simulations to run

for (i in 1:n_reps) {
  seed <- i * 4  # generating a different seed for each simulation
  sampled_data <- random_sample(fall_la, 28, seed)
  fall_la_avg_data <- seasonal_avg(sampled_data)
  
  # Add seed and rep_number columns
  fall_la_avg_data$seed <- seed
  fall_la_avg_data$rep_number <- i
  
  # Append to st fall_la df
  st_fall_la <- rbind(st_fall_la, fall_la_avg_data)
}
ggplot(st_fall_la, aes(x = native_id, y = log_avg)) +
  geom_boxplot()+
  ggtitle("Figure X: fall_la Short Term Sensor Log-transformed Annual Averages \n30 sets of 28 samples per sensor")

#------------winter_la--------------------------------------------
st_winter_la <- data.frame()

n_reps <- 30  # number of random df simulations to run

for (i in 1:n_reps) {
  seed <- i * 4  # generating a different seed for each simulation
  sampled_data <- random_sample(winter_la, 28, seed)
  winter_la_avg_data <- seasonal_avg(sampled_data)
  
  # Add seed and rep_number columns
  winter_la_avg_data$seed <- seed
  winter_la_avg_data$rep_number <- i
  
  # Append to st winter_la df
  st_winter_la <- rbind(st_winter_la, winter_la_avg_data)
}
ggplot(st_winter_la, aes(x = native_id, y = log_avg)) +
  geom_boxplot()+
  ggtitle("Figure X: winter_la Short Term Sensor Log-transformed Annual Averages \n30 sets of 28 samples per sensor")


#------------spring_la--------------------------------------------
st_spring_la <- data.frame()

n_reps <- 30  # number of random df simulations to run

for (i in 1:n_reps) {
  seed <- i * 4  # generating a different seed for each simulation
  sampled_data <- random_sample(spring_la, 28, seed)
  spring_la_avg_data <- seasonal_avg(sampled_data)
  
  # Add seed and rep_number columns
  spring_la_avg_data$seed <- seed
  spring_la_avg_data$rep_number <- i
  
  # Append to st spring_la df
  st_spring_la <- rbind(st_spring_la, spring_la_avg_data)
}

ggplot(st_spring_la, aes(x = native_id, y = log_avg)) +
  geom_boxplot()+
  ggtitle("Figure X:spring_la Short Term Sensor Log-transformed Annual Averages \n30 sets of 28 samples per sensor")




#Looks good! Only suggestion is that we just use the annual_avg() function and not add a season_avg() function, because other than the name, they produce the same results. KW

```


# adding covariates to the season data sets for LA county only
```{r season.convariates}
#--------------------------Common Model on Seasonal Data sets------------------#
#adding the covariates to the gold standard data to do some predictions
st_summer_la <-  left_join(st_summer_la,common_model_cov, by = "native_id" )
st_fall_la <-  left_join(st_fall_la,common_model_cov, by = "native_id" )
st_winter_la <-  left_join(st_winter_la,common_model_cov, by = "native_id" )
st_spring_la <-  left_join(st_spring_la,common_model_cov, by = "native_id" )


```

#running a regression model using common model covariates and seasonal data sets
```{r}

# in-sample predictions and fit summary
summary(lm_st_summer_la <- lm(frml, data = st_summer_la))
summary(lm_st_fall_la <- lm(frml, data = st_fall_la))
summary(lm_st_winter_la <- lm(frml, data = st_winter_la))
summary(lm_st_spring_la <- lm(frml, data = st_spring_la)) 


```

#----------comparing seasonal short term campaigns with Gold Standard data----------------

```{r summer vs gold}
#------------seasonal comparison with gold standard in LA county-------------

#-----------------------------------SUMMER--------------------------------------
#need to group the st_summer data by "campaign" before doing regression

#apply the save CV groups as used in the gold standard model
sst_summer_la <- st_summer_la %>%
  mutate(CV_grp_st_la = CV_grp_st_la) %>% ungroup()

#in sample-predictions
fitted_models = sst_summer_la %>% group_by(rep_number) %>% do(model = lm(frml, data = .))

#cross-validated predictions treating each sample "campaign" separately
st_summer_CV_la <- sst_summer_la %>% group_by(rep_number) %>% do(model = do_CV(data = ., id = "native_id", group = "CV_grp_st_la", formula = frml))
#produces a df for each result

#next step is to restructure the result back into a single long_df

st_summer_CV_la <- st_summer_CV_la %>% rename(model_run = rep_number) #change column name so there isn't a conflict when I unnest it

st_summer_CV_la <- st_summer_CV_la %>%
  unnest(cols = c(model))

#---compare st_summer with gold-----------------------------------------#
#appending the predictions from the balanced short term campaign to the st_summer_CV df for comparison
gold_preds_la <- gold_CV_la %>%
  dplyr::select("native_id", "cvpreds") %>% rename(gold_preds_la = cvpreds)  #select "model run" as well as native_id
st_summer_CV_la <- left_join(st_summer_CV_la, gold_preds_la, by = "native_id" ) 


#calculating the RMSE and R2 two ways
  #a) comparing the campaign predictions to the observations from the gold standard 
  #b) comparing the predictions from each campaign to the observations from that campaign

#need to append the observations from the balanced short term campaign to do method a

gold_obs_la <- gold_CV_la %>%
  dplyr::select("native_id", "log_avg") %>% rename(gold_obs_la = log_avg)
st_summer_CV_la <- left_join(st_summer_CV_la, gold_obs_la, by = "native_id" ) 
# Initialize empty lists to store the results for both methods
results_a <- list()
results_b <- list()

# Loop through each unique model_run
for (run in unique(st_summer_CV_la$model_run)) {
  # Filter the data for the current model_run
  data_subset <- st_summer_CV_la %>% filter(model_run == run)
  
  # Calculate the MSE for method A
  mse_result_a <- get_MSE(data_subset$gold_obs_la, data_subset$cvpreds)
  results_a[[run]] <- mse_result_a
  
  # Calculate the MSE for method B
  mse_result_b <- get_MSE(data_subset$log_avg, data_subset$cvpreds)
  results_b[[run]] <- mse_result_b
}

# Convert the lists to a data frame with separate columns for each method
st_summer_MSE_la <- data.frame(
  model_run = unique(st_summer_CV_la$model_run),
  RMSE_a = sapply(results_a, `[`, 1),
  R2_a = sapply(results_a, `[`, 2),
  RMSE_b = sapply(results_b, `[`, 1),
  R2_b = sapply(results_b, `[`, 2)
)

#pivot longer for plotting
st_summer_MSE_long_la <- st_summer_MSE_la %>%
  pivot_longer(
    cols = c(RMSE_a, R2_a, RMSE_b, R2_b),
    names_to = c("metric", "method"),
    names_sep = "_",
    values_to = "value"
  ) 

#Plotting (replicate figure 1 in blanco paper)
# get range for plot
r <- st_summer_CV_la %>% dplyr::select(gold_preds_la, cvpreds) %>% range()


ggplot(data = st_summer_CV_la, aes(x = gold_preds_la, y = cvpreds, color = as.factor(model_run))) +
    geom_point(shape = "o", alpha = 0.7) +
    lims(x= r, y = r) +
    coord_fixed() +
    geom_smooth(method = 'lm', se = F, alpha = 0.5) +
    geom_abline(slope = 1, intercept = 0, color = "darkgray", linetype = "dashed") +
    labs(title = "Summer Campaign vs. short term balanced Cross-validated Predictions", 
         subtitle = "Balanced Campaigns",
         x = "short term balanced predicted ln(NOx) (ln(ppb))",
         y = "Summer Campaign Predicted ln(NOx) (ln(ppb))",
         caption = "1:1 line is dashed") +
    theme_bw()


#This needs some aesthetic adjustments: choose between dots or lines, pick a color scale that's helpful, get rid of the funky legend, but it's cool!!
    #looks a lot like the balanced predictions from the Blanco paper. 

#replicate blanco figure 3
ggplot(st_summer_MSE_long_la, aes(x = method, y = value, color = method)) +
  geom_boxplot() +
  geom_point(data = data.frame(method = c("a", "b"), value = gold_MSE_la[[1]], metric = "RMSE"), aes(x = method, y = value, color = method), size = 3, shape = 5) +
  geom_point(data = data.frame(method = c("a", "b"), value = gold_MSE_la[[2]], metric = "R2"), aes(x = method, y = value, color = method), size = 3, shape = 5) +
  facet_wrap(~metric) +
  labs(title = "RMSE and MSE_R2 Comparison",
       caption = "Method A = LA county Summer Campaign Predictions Compared to Gold Standard Obs\n
       Method B = LA county Summer Campaign Predictions Compared to Summer Campaign Observations\n
       Square points shows the long-term campaigns performance") +
  theme_bw()


  


```

#Fall LA county LOO and Model comparison
```{r}
#------------seasonal comparison with gold standard in LA county-------------

#-----------------------------------FALL--------------------------------------
#need to group the st_summer data by "campaign" before doing regression

#apply the save CV groups as used in the gold standard model
sst_fall_la <- st_fall_la %>%
  mutate(CV_grp_st_la = CV_grp_st_la) %>% ungroup()

#in sample-predictions
fitted_models = sst_fall_la %>% group_by(rep_number) %>% do(model = lm(frml, data = .))

#cross-validated predictions treating each sample "campaign" separately
st_fall_CV_la <- sst_fall_la %>% group_by(rep_number) %>% do(model = do_CV(data = ., id = "native_id", group = "CV_grp_st_la", formula = frml))
#produces a df for each result

#next step is to restructure the result back into a single long_df

st_fall_CV_la <- st_fall_CV_la %>% rename(model_run = rep_number) #change column name so there isn't a conflict when I unnest it

st_fall_CV_la <- st_fall_CV_la %>%
  unnest(cols = c(model))

#---compare st_summer with gold-----------------------------------------#
#appending the predictions from the balanced short term campaign to the st_summer_CV df for comparison
gold_preds_la <- gold_CV_la %>%
  dplyr::select("native_id", "cvpreds") %>% rename(gold_preds_la = cvpreds)  #select "model run" as well as native_id
st_fall_CV_la <- left_join(st_fall_CV_la, gold_preds_la, by = "native_id" ) 


#calculating the RMSE and R2 two ways
  #a) comparing the campaign predictions to the observations from the gold standard 
  #b) comparing the predictions from each campaign to the observations from that campaign

#need to append the observations from the balanced short term campaign to do method a

gold_obs_la <- gold_CV_la %>%
  dplyr::select("native_id", "log_avg") %>% rename(gold_obs_la = log_avg)
st_fall_CV_la <- left_join(st_fall_CV_la, gold_obs_la, by = "native_id" ) 
# Initialize empty lists to store the results for both methods
results_a <- list()
results_b <- list()

# Loop through each unique model_run
for (run in unique(st_fall_CV_la$model_run)) {
  # Filter the data for the current model_run
  data_subset <- st_fall_CV_la %>% filter(model_run == run)
  
  # Calculate the MSE for method A
  mse_result_a <- get_MSE(data_subset$gold_obs_la, data_subset$cvpreds)
  results_a[[run]] <- mse_result_a
  
  # Calculate the MSE for method B
  mse_result_b <- get_MSE(data_subset$log_avg, data_subset$cvpreds)
  results_b[[run]] <- mse_result_b
}

# Convert the lists to a data frame with separate columns for each method
st_fall_MSE_la <- data.frame(
  model_run = unique(st_fall_CV_la$model_run),
  RMSE_a = sapply(results_a, `[`, 1),
  R2_a = sapply(results_a, `[`, 2),
  RMSE_b = sapply(results_b, `[`, 1),
  R2_b = sapply(results_b, `[`, 2)
)

#pivot longer for plotting
st_fall_MSE_long_la <- st_fall_MSE_la %>%
  pivot_longer(
    cols = c(RMSE_a, R2_a, RMSE_b, R2_b),
    names_to = c("metric", "method"),
    names_sep = "_",
    values_to = "value"
  ) 

#Plotting (replicate figure 1 in blanco paper)
# get range for plot
r <- st_fall_CV_la %>% dplyr::select(gold_preds_la, cvpreds) %>% range()


ggplot(data = st_fall_CV_la, aes(x = gold_preds_la, y = cvpreds, color = as.factor(model_run))) +
    geom_point(shape = "o", alpha = 0.7) +
    lims(x= r, y = r) +
    coord_fixed() +
    geom_smooth(method = 'lm', se = F, alpha = 0.5) +
    geom_abline(slope = 1, intercept = 0, color = "darkgray", linetype = "dashed") +
    labs(title = "Fall Campaign vs. short term balanced Cross-validated Predictions", 
         subtitle = "Balanced Campaigns",
         x = "short term balanced predicted ln(NOx) (ln(ppb))",
         y = "Fall Campaign Predicted ln(NOx) (ln(ppb))",
         caption = "1:1 line is dashed") +
    theme_bw()


#This needs some aesthetic adjustments: choose between dots or lines, pick a color scale that's helpful, get rid of the funky legend, but it's cool!!
    #looks a lot like the balanced predictions from the Blanco paper. 

#replicate blanco figure 3
ggplot(st_fall_MSE_long_la, aes(x = method, y = value, color = method)) +
  geom_boxplot() +
  geom_point(data = data.frame(method = c("a", "b"), value = gold_MSE_la[[1]], metric = "RMSE"), aes(x = method, y = value, color = method), size = 3, shape = 5) +
  geom_point(data = data.frame(method = c("a", "b"), value = gold_MSE_la[[2]], metric = "R2"), aes(x = method, y = value, color = method), size = 3, shape = 5) +
  facet_wrap(~metric) +
  labs(title = "RMSE and MSE_R2 Comparison",
       caption = "Method A = LA county Fall Campaign Predictions Compared to Gold Standard Obs\n
       Method B = LA county Fall Campaign Predictions Compared to Summer Campaign Observations\n
       Square points shows the long-term campaigns performance") +
  theme_bw()


```
#Winter LA County LOO and Model Comparison
```{r}

#------------seasonal comparison with gold standard in LA county-------------

#-----------------------------------WINTER--------------------------------------
#need to group the st_summer data by "campaign" before doing regression

#apply the save CV groups as used in the gold standard model
sst_winter_la <- st_winter_la %>%
  mutate(CV_grp_st_la = CV_grp_st_la) %>% ungroup()

#in sample-predictions
fitted_models = sst_winter_la %>% group_by(rep_number) %>% do(model = lm(frml, data = .))

#cross-validated predictions treating each sample "campaign" separately
st_winter_CV_la <- sst_winter_la %>% group_by(rep_number) %>% do(model = do_CV(data = ., id = "native_id", group = "CV_grp_st_la", formula = frml))
#produces a df for each result

#next step is to restructure the result back into a single long_df

st_winter_CV_la <- st_winter_CV_la %>% rename(model_run = rep_number) #change column name so there isn't a conflict when I unnest it

st_winter_CV_la <- st_winter_CV_la %>%
  unnest(cols = c(model))

#---compare st_summer with gold-----------------------------------------#
#appending the predictions from the balanced short term campaign to the st_summer_CV df for comparison
gold_preds_la <- gold_CV_la %>%
  dplyr::select("native_id", "cvpreds") %>% rename(gold_preds_la = cvpreds)  #select "model run" as well as native_id
st_winter_CV_la <- left_join(st_winter_CV_la, gold_preds_la, by = "native_id" ) 


#calculating the RMSE and R2 two ways
  #a) comparing the campaign predictions to the observations from the gold standard 
  #b) comparing the predictions from each campaign to the observations from that campaign

#need to append the observations from the balanced short term campaign to do method a

gold_obs_la <- gold_CV_la %>%
  dplyr::select("native_id", "log_avg") %>% rename(gold_obs_la = log_avg)
st_winter_CV_la <- left_join(st_winter_CV_la, gold_obs_la, by = "native_id" ) 
# Initialize empty lists to store the results for both methods
results_a <- list()
results_b <- list()

# Loop through each unique model_run
for (run in unique(st_winter_CV_la$model_run)) {
  # Filter the data for the current model_run
  data_subset <- st_winter_CV_la %>% filter(model_run == run)
  
  # Calculate the MSE for method A
  mse_result_a <- get_MSE(data_subset$gold_obs_la, data_subset$cvpreds)
  results_a[[run]] <- mse_result_a
  
  # Calculate the MSE for method B
  mse_result_b <- get_MSE(data_subset$log_avg, data_subset$cvpreds)
  results_b[[run]] <- mse_result_b
}

# Convert the lists to a data frame with separate columns for each method
st_winter_MSE_la <- data.frame(
  model_run = unique(st_winter_CV_la$model_run),
  RMSE_a = sapply(results_a, `[`, 1),
  R2_a = sapply(results_a, `[`, 2),
  RMSE_b = sapply(results_b, `[`, 1),
  R2_b = sapply(results_b, `[`, 2)
)

#pivot longer for plotting
st_winter_MSE_long_la <- st_winter_MSE_la %>%
  pivot_longer(
    cols = c(RMSE_a, R2_a, RMSE_b, R2_b),
    names_to = c("metric", "method"),
    names_sep = "_",
    values_to = "value"
  ) 

#Plotting (replicate figure 1 in blanco paper)
# get range for plot
r <- st_winter_CV_la %>% dplyr::select(gold_preds_la, cvpreds) %>% range()


ggplot(data = st_winter_CV_la, aes(x = gold_preds_la, y = cvpreds, color = as.factor(model_run))) +
    geom_point(shape = "o", alpha = 0.7) +
    lims(x= r, y = r) +
    coord_fixed() +
    geom_smooth(method = 'lm', se = F, alpha = 0.5) +
    geom_abline(slope = 1, intercept = 0, color = "darkgray", linetype = "dashed") +
    labs(title = "Winter Campaign vs. short term balanced Cross-validated Predictions", 
         subtitle = "Balanced Campaigns",
         x = "short term balanced predicted ln(NOx) (ln(ppb))",
         y = "Winter Campaign Predicted ln(NOx) (ln(ppb))",
         caption = "1:1 line is dashed") +
    theme_bw()


#This needs some aesthetic adjustments: choose between dots or lines, pick a color scale that's helpful, get rid of the funky legend, but it's cool!!
    #looks a lot like the balanced predictions from the Blanco paper. 

#replicate blanco figure 3
ggplot(st_winter_MSE_long_la, aes(x = method, y = value, color = method)) +
  geom_boxplot() +
  geom_point(data = data.frame(method = c("a", "b"), value = gold_MSE_la[[1]], metric = "RMSE"), aes(x = method, y = value, color = method), size = 3, shape = 5) +
  geom_point(data = data.frame(method = c("a", "b"), value = gold_MSE_la[[2]], metric = "R2"), aes(x = method, y = value, color = method), size = 3, shape = 5) +
  facet_wrap(~metric) +
  labs(title = "RMSE and MSE_R2 Comparison",
       caption = "Method A = LA county Winter Campaign Predictions Compared to Gold Standard Obs\n
       Method B = LA county Winter Campaign Predictions Compared to Summer Campaign Observations\n
       Square points shows the long-term campaigns performance") +
  theme_bw()


```



# Spring LA county model comparison
```{r}
#------------seasonal comparison with gold standard in LA county-------------

#-----------------------------------SPRING--------------------------------------
#need to group the st_summer data by "campaign" before doing regression

#apply the save CV groups as used in the gold standard model
sst_spring_la <- st_spring_la %>%
  mutate(CV_grp_st_la = CV_grp_st_la) %>% ungroup()

#in sample-predictions
fitted_models = sst_spring_la %>% group_by(rep_number) %>% do(model = lm(frml, data = .))

#cross-validated predictions treating each sample "campaign" separately
st_spring_CV_la <- sst_spring_la %>% group_by(rep_number) %>% do(model = do_CV(data = ., id = "native_id", group = "CV_grp_st_la", formula = frml))
#produces a df for each result

#next step is to restructure the result back into a single long_df

st_spring_CV_la <- st_spring_CV_la %>% rename(model_run = rep_number) #change column name so there isn't a conflict when I unnest it

st_spring_CV_la <- st_spring_CV_la %>%
  unnest(cols = c(model))

#---compare st_summer with gold-----------------------------------------#
#appending the predictions from the balanced short term campaign to the st_summer_CV df for comparison
gold_preds_la <- gold_CV_la %>%
  dplyr::select("native_id", "cvpreds") %>% rename(gold_preds_la = cvpreds)  #select "model run" as well as native_id
st_spring_CV_la <- left_join(st_spring_CV_la, gold_preds_la, by = "native_id" ) 


#calculating the RMSE and R2 two ways
  #a) comparing the campaign predictions to the observations from the gold standard 
  #b) comparing the predictions from each campaign to the observations from that campaign

#need to append the observations from the balanced short term campaign to do method a

gold_obs_la <- gold_CV_la %>%
  dplyr::select("native_id", "log_avg") %>% rename(gold_obs_la = log_avg)
st_spring_CV_la <- left_join(st_spring_CV_la, gold_obs_la, by = "native_id" ) 
# Initialize empty lists to store the results for both methods
results_a <- list()
results_b <- list()

# Loop through each unique model_run
for (run in unique(st_spring_CV_la$model_run)) {
  # Filter the data for the current model_run
  data_subset <- st_spring_CV_la %>% filter(model_run == run)
  
  # Calculate the MSE for method A
  mse_result_a <- get_MSE(data_subset$gold_obs_la, data_subset$cvpreds)
  results_a[[run]] <- mse_result_a
  
  # Calculate the MSE for method B
  mse_result_b <- get_MSE(data_subset$log_avg, data_subset$cvpreds)
  results_b[[run]] <- mse_result_b
}

# Convert the lists to a data frame with separate columns for each method
st_spring_MSE_la <- data.frame(
  model_run = unique(st_spring_CV_la$model_run),
  RMSE_a = sapply(results_a, `[`, 1),
  R2_a = sapply(results_a, `[`, 2),
  RMSE_b = sapply(results_b, `[`, 1),
  R2_b = sapply(results_b, `[`, 2)
)

#pivot longer for plotting
st_spring_MSE_long_la <- st_spring_MSE_la %>%
  pivot_longer(
    cols = c(RMSE_a, R2_a, RMSE_b, R2_b),
    names_to = c("metric", "method"),
    names_sep = "_",
    values_to = "value"
  ) 

#Plotting (replicate figure 1 in blanco paper)
# get range for plot
r <- st_spring_CV_la %>% dplyr::select(gold_preds_la, cvpreds) %>% range()


ggplot(data = st_spring_CV_la, aes(x = gold_preds_la, y = cvpreds, color = as.factor(model_run))) +
    geom_point(shape = "o", alpha = 0.7) +
    lims(x= r, y = r) +
    coord_fixed() +
    geom_smooth(method = 'lm', se = F, alpha = 0.5) +
    geom_abline(slope = 1, intercept = 0, color = "darkgray", linetype = "dashed") +
    labs(title = "Spring Campaign vs. short term balanced Cross-validated Predictions", 
         subtitle = "Balanced Campaigns",
         x = "short term balanced predicted ln(NOx) (ln(ppb))",
         y = "Spring Campaign Predicted ln(NOx) (ln(ppb))",
         caption = "1:1 line is dashed") +
    theme_bw()


#This needs some aesthetic adjustments: choose between dots or lines, pick a color scale that's helpful, get rid of the funky legend, but it's cool!!
    #looks a lot like the balanced predictions from the Blanco paper. 

#replicate blanco figure 3
ggplot(st_spring_MSE_long_la, aes(x = method, y = value, color = method)) +
  geom_boxplot() +
  geom_point(data = data.frame(method = c("a", "b"), value = gold_MSE_la[[1]], metric = "RMSE"), aes(x = method, y = value, color = method), size = 3, shape = 5) +
  geom_point(data = data.frame(method = c("a", "b"), value = gold_MSE_la[[2]], metric = "R2"), aes(x = method, y = value, color = method), size = 3, shape = 5) +
  facet_wrap(~metric) +
  labs(title = "RMSE and MSE_R2 Comparison",
       caption = "Method A = LA county Spring Campaign Predictions Compared to Gold Standard Obs\n
       Method B = LA county Spring Campaign Predictions Compared to Summer Campaign Observations\n
       Square points shows the long-term campaigns performance") +
  theme_bw()
```



# Discussion and Conclusion

**Model Selection** 

We used forward stepwise regression to select model covariates from among the seven used by Mercer et al. (2011) in their common model describing all seasons. We chose to use stepwise regression to select covariates from among the common model covariates rather than incorporating all seven used by Mercer et al. (2011) due to the limited data available in our LA county-specific analysis, which included just 12 sensors. The Mercer et al. (2011) common model covariates were used because of their efficacy in predicting seasonal NOx concentrations in California, as determined through a robust selection process that determined which covariates performed well across all seasons. Although the Mercer et al. (2011) covariates performed reasonably well in the Mercer et al. (2011) analysis, similar covariates in our model explained just small proportion of the variability in NOx predictions, as demonstrated by our small-to-moderate coefficients of determination generated for cross-validated predictions across all seasons. It is likely that some of the covariates not selected for inclusion into our model may have explained a greater portion of the variability in NOx concentrations; this is particularly important to consider, given that our primary analysis focused on the entire state of California, whereas the Mercer et al. (2011) analysis evaluated only Los Angeles. The geographic differences, including potential differences in spatial correlation, could therefore contribute to the varied performance of the covariates across analyses. 

Our use of forward selection stepwise regression for covariate selection was beneficial in providing information on the bias-variance-tradeoff as covariates were systematically incorporated into the model and assessed. Through this process, we were able to objectively select three of the seven Mercer et al. (2011) covariates based on their contributions to model accuracy and variability. Although forward selection was a reasonable choice for model selection based on our goal (i.e. narrowing down covariates from those in the Mercer et al. common model) and the tools available for us, it is possible that another covariate selection method would have provided a stronger prediction model in the present analysis. For example, in Blanco et al. (2022), authors used partial least squares (PLS) regression models to summarize hundreds of geographic covariates, rather than a priori selecting a handful of covariates based on previous findings published in the literature. The use of PLS across hundreds of covariates enhanced the ability of the model to capture variability in NOx concentrations and likely accounted for their improved prediction performance when compared against the present analysis. 




**Statewide Short-Term Dataset Performance**




**Los Angeles County Short-Term Dataset Performance** 




**Comparison of Findings to Literature** 





**Strengths and Limitations** 



**Broader Implications and Future Research** 



**Conclusion** 




# Author Contribution Statement (**NOTE add in your contributions below**) 

**Core Contributions** 

Analysis: 

*Callan:* Met with professors at office hours to discuss analytical approach; attended all group meetings to discuss analytical approach; drafted first iteration of day-of-week  vs. weekday/weekend predictions with cross-validation and ANOVA (not incorporated: later revised plan); conducted sensitivity analysis for Los Angeles county, including creation of short- and long-term temporally balanced datasets specific to Los Angeles, summarizing short- and long-term temporally balanced datasets within LA, generating LA-specific NOx predictions and taking the first pass at cross-validation, including making a new cross-validation group; calculated R2 and RMSE values for LA-specific NOx predictions

*Katie:*

*Abbie:*

Writing: 

*Callan:* Drafted and revised analysis plans (3 in total); wrote and revised methods section; wrote and revised introduction section; wrote portions of results section; wrote portions of discussion section; wrote portions of conclusion section 

*Katie:*


*Abbie:*


**Additional Contributions**

*Callan:* reviewed literature for introduction section; reviewed and critiqued content; final editing and proofreading

*Katie:*

*Abbie:*








*points to be added into discussion section later (originally earlier in code):*


[popping the below from the code to the limitations section for later reference when we write the discussion]
The data provided the date and week of the year that each sample was taken. To look at seasonal changes in the data, we need to separate the weeks of the year into four seasons. We did this by breaking up the roughly 52 weeks of the year into 4, ~13-week segments. 

    Weeks 13 - 25 of the year are coded as spring
    weeks 26 - 38 are coded as summer
    weeks 39 - 50 are coded as autumn
    and week 51,52 and 1-12 are coded as winter.
    
  ** Limitations of this approach. This is based only on diving the year into roughly equal portions based on numerical week. There is no distinct scientific or meteorological significance to the cutoff. Could look at other ways of defining season for more accuracy --- could use specific calendrical definitions, or could look at cut points based on temp or precip. Could also use social cutoffs (e.g. summer is when school is out), as any seasonal differences could be a combination of both social and meteorological phenomena.
  
  
  


# Works Cited 

Blanco, M. N., Gassett, A., Gould, T., Doubleday, A., Slager, D. L., Austin, E., ... & Sheppard, L. (2022). Characterization of annual average traffic-related air pollution concentrations in the Greater Seattle Area from a year-long mobile monitoring campaign. Environmental science & technology, 56(16), 11460-11472.

Colorado Department of Public Health & Environment. 2024. Air toxics: monitoring. https://cdphe.colorado.gov/air-toxics/monitoring#:~:text=Mobile%20monitoring%20can%20capture%20%E2%80%9Cspatial,wind%20compared%20to%20stationary%20monitoring Accessed 12/3/2024.

Mercer, L. D., Szpiro, A. A., Sheppard, L., Lindström, J., Adar, S. D., Allen, R. W., ... & Kaufman, J. D. (2011). Comparing universal kriging and land-use regression for predicting concentrations of gaseous oxides of nitrogen (NOx) for the Multi-Ethnic Study of Atherosclerosis and Air Pollution (MESA Air). Atmospheric Environment, 45(26), 4412-4420.

United States Census. 2023. QuickFacts Los Angeles County, California. https://www.census.gov/quickfacts/fact/table/losangelescountycalifornia,losangelescitycalifornia,CA/PST045223 Accessed 11/30/2024.



## Appendix 

#---------------------------Reference Code-------------------------------------#


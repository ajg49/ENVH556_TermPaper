---
title: "CA NOx Term Project"
authors: "Callan, Abbie, and Katie - ENV H 556"
format:
  html:
    df_print: "paged"
    fig_caption: yes
    toc: true
    toc_depth: 3
    number_sections: true
    self-contained: true #save images etc. in this file (vs folders)
execute:
  echo: false
  cache: false
  echo.comments: false
  message: false
  warning: false
  
---

This document was rendered on `r format(Sys.time(), '%B %d, %Y')`.
---

```{r setup, include=FALSE}

#-----setup-----

# clear work space of all objects and unload all extra (non-base) packages
rm(list = ls(all = TRUE))
if (!is.null(sessionInfo()$otherPkgs)) {
    res <- suppressWarnings(
        lapply(paste('package:', names(sessionInfo()$otherPkgs), sep=""),
               detach, character.only=TRUE, unload=TRUE, force=TRUE))
   
}

```

```{r load.libraries.pacman, echo=FALSE, include=FALSE, eval=TRUE}

#-----load libraries pacman-----

# Load pacman into memory, installing as needed
my_repo <- 'http://cran.r-project.org'
if (!require("pacman")) {install.packages("pacman", repos = my_repo)}

# Load the other packages, installing as needed.
pacman::p_load(knitr, kableExtra, tidyverse, lubridate, egg, multcomp, modelr, broom, EnvStats, Hmisc,
               dplyr, tidyr, purrr, ggplot2, stringr, sf, lme4, VCA)


```


```{r read.data, echo=FALSE, include=FALSE}
#-----read data from a website--------

# create data directory if it does not exist
dir.create(file.path("Datasets"),
           showWarnings = FALSE,
           recursive = TRUE)

# read in ca nox air pollution concentrations from mobile monitoring
# download it from the web and save locallyif the file does not already exist
# read data; download if necessary
cal_nox <- read_rds(file.path("https://zenodo.org/records/14166411/files/nox_hourly.rda?download=1", 
                                 output_file_path = file.path("Datasets", "nox_hourly.rda"))) %>% rename_with(~ tolower(gsub(".","_", .x, fixed=TRUE)))

#view the data
glimpse(cal_nox)
summary(cal_nox)

length(unique(cal_nox$native_id)) #73 locations
                              
ca_covariates <- read_rds(file.path("https://zenodo.org/records/14166411/files/site_covariates.rda?download=1")) 

ca_covariates <- st_as_sf(ca_covariates, coords = c("longitude", "latitude"))

#glimpse(ca_covariates) #too long!
  
# combine files
cal_nox <- left_join(cal_nox, ca_covariates, by="native_id")

unique(cal_nox$parameter_name)  
```




```{r create.strata, echo =F, include = F}

cal_nox <- cal_nox %>%
  mutate(day_time = case_when(
    hour(date) > 4 & hour(date) <= 9 ~ "Morning",
    hour(date) > 9 & hour(date) <= 16 ~ "Midday",
    hour(date) > 16 & hour(date) <= 21 ~ "Evening",
    hour(date) > 21 | hour(date) <= 4 ~ "Night"
  ), .after = hour) %>%
  mutate(season = factor(season, levels = c("Morning", "Midday", "Evening", "Night")))

#I made these cuts off very rough estimates based on data distribution. We probably want to fine tune our scientific rationale for doing so and readjust if warranted. KW

#fixing the season variable because there were a lot of NA

cal_nox <- cal_nox %>%
  mutate(season = case_when(
    week(date) > 12 & week(date) <= 25 ~ "Spring",
    week(date) > 25 & week(date) <= 38 ~ "Summer",
    week(date) > 38 & week(date) <= 50 ~ "Fall",
    week(date) > 50 | week(date) <= 12 ~ "Winter"
  )) %>%
  mutate(season = factor(season, levels = c("Spring", "Summer", "Fall", "Winter")))

```

```{r, echo = FALSE, warning = FALSE, message = FALSE}
#Cal_NOx data set
kable(cal_nox%>%
  group_by(parameter_name) %>%
  summarise(
    count = n(),
    min = min(sample_measurement),
    max = max(sample_measurement),
    median = median(sample_measurement),
    GM = geoMean(sample_measurement, na.rm = TRUE),
    GSD = geoSD(sample_measurement, na.rm = TRUE),
    AM = mean(sample_measurement, na.rm = TRUE),
    ASD = sd(sample_measurement, na.rm = TRUE)),
  digit = 2,
  caption = "Table 1.0: Summary Statistics for Nitric Oxides (NOx)") %>%
  kable_styling()

#data is going to need a little cleaning. There are monitors with negative values recorded. 

#filter out the negative values and repeat the table. Justify that it's scientifically impossible to have neg. values on native scale.
cal_nox_clean <- cal_nox %>% filter(sample_measurement > 0) #29,521 rows removed!!

kable(cal_nox_clean%>%
  group_by(parameter_name) %>%
  summarise(
    count = n(),
    min = min(sample_measurement),
    max = max(sample_measurement),
    median = median(sample_measurement),
    GM = geoMean(sample_measurement, na.rm = TRUE),
    GSD = geoSD(sample_measurement, na.rm = TRUE),
    AM = mean(sample_measurement, na.rm = TRUE),
    ASD = sd(sample_measurement, na.rm = TRUE)),
  digit = 2,
  caption = "Table 1.0: Summary Statistics for Nitric Oxides (NOx)") %>%
  kable_styling()


```
```{r histogram}
#histograms with smoother

#native scale
ggplot(cal_nox_clean, aes(sample_measurement)) +
  geom_histogram(aes(y = ..density..), bins = 40, color = "lightblue", fill = "lightblue", alpha = 0.8) +
  geom_density(color = "purple") +
  facet_wrap(~parameter_name, scales = "free") +
  labs(title = "Distribution of NOx",
       x = "Concentration on native scale",
       y = "density"
       )
  
#log transformed
ggplot(cal_nox_clean, aes(log(sample_measurement))) +
  geom_histogram(aes(y = ..density..), bins = 40, color = "darkgreen", fill = "darkgreen", alpha = 0.4) +
  geom_density(color = "green") +
  facet_wrap(~parameter_name, scales = "free") +
  labs(title = "Distribution of log(NOx)",
       x = "Concentration on log scale",
       y = "density"
       )

#Log transformed is definitely better.

```
```{r log transform, echo = F}

#create a new variable called "log_conc" for the log transformed concentrations
cal_nox_clean <- cal_nox_clean %>%
  mutate(log_conc = log(sample_measurement)) %>%
  relocate(log_conc, .after = sample_measurement)

```


```{r}
#---Q-Q plot---#


#plotting first on a native scale
ggplot(cal_nox_clean, aes(sample = sample_measurement)) +
  stat_qq() + 
  stat_qq_line() +
  labs(title = "Figure 1: Normal Q-Q Plot of NOx",
       x = "Theoretical Quantiles from a Std Normal Distribution",
       y = "Concentration - native scale"
       ) +
  facet_wrap(~parameter_name, scales = "free")

#qq plots using log transformed values
ggplot(cal_nox_clean, aes(sample = log_conc)) +
  stat_qq() + 
  stat_qq_line() +
  labs(title = "Figure 1: Normal Q-Q Plot of NOx",
       x = "Theoretical Quantiles from a Std Normal Distribution",
       y = "Concentration - native scale"
       ) +
  facet_wrap(~parameter_name, scales = "free")


```
The Q-Q plots show a huge benefit to log-transformation of the data, no matter which NOx parameter you choose to work with. WE do still see some much more attenuated deviations form the normal line, but overall the data distribution is clearly in need of this transformation before statistical tests or models are built. 

# Seasonal Characterization of NOx concentrations:

The data provided the date and week of the year that each sample was taken. To look at seasonal changes in the data, we need to separate the weeks of the year into four seasons. We did this by breaking up the roughly 52 weeks of the year into 4, ~13-week segments. 

    Weeks 13 - 25 of the year are coded as spring
    weeks 26 - 38 are coded as summer
    weeks 39 - 50 are coded as autumn
    and week 51,52 and 1-12 are coded as winter.
    
  ** Limitations of this approach. This is based only on diving the year into roughly equal portions based on numerical week. There is no distinct scientific or meteorological significance to the cutoff. Could look at other ways of defining season for more accuracy --- could use specific calendrical definitions, or could look at cut points based on temp or precip. Could also use social cutoffs (e.g. summer is when school is out), as any seasonal differences could be a combination of both social and meteorological phenomena.

  We also needed to take the hours of the day at which samples were collected and bin them into categories to assess whether time of day is a predictor of NOx concentrations. To do this we binned the 24 hours of the day into four categories:

    hours > 4 & hours <= 9 are coded as Morning
    hours > 9 & hours <= 16 are coded as Midday
    hours > 16 & hours  <= 21 are coded as Evening
    hours > 21 or hours  <= 4 are coded as Night

**Hourly and Day Time Summary of NOx**

```{r timetrends, echo = F}

#summarizing by time

cal_nox_clean %>% filter(parameter_name == "Oxides of nitrogen (NOx)") %>% ggplot(aes(x = as.factor(hour), y = log_conc)) +
         geom_boxplot() +
  ggtitle( label = "Ln(NOx) Distributions by hour")

cal_nox_clean %>% filter(parameter_name == "Oxides of nitrogen (NOx)") %>% 
  ggplot(aes(x = day_time, y = log_conc)) +
         geom_boxplot() +
  ggtitle( label = "Ln(NOx) Distributions by Day Time")

kable(cal_nox_clean %>%
        filter(parameter_name ==  "Oxides of nitrogen (NOx)") %>%
  group_by(day_time) %>%
  summarise(
    n = sum(!is.na(sample_measurement)),
    AM = mean(sample_measurement, na.rm = TRUE),
    ASD = sd(sample_measurement, na.rm = TRUE)),
  digit = 1,
  caption = "Table 3.0: Time of Day Summary of NOx - Native scale ") %>%
  kable_styling()

```

# Weekly Characterization of the NOx variables concentrations:

**Weekday Stratification and Summary of NOx**

```{r}
#summary by day of the week

cal_nox_clean %>% filter(parameter_name == "Oxides of nitrogen (NOx)") %>% ggplot(aes(x = dow, y = log_conc)) +
         geom_boxplot() +
  ggtitle( label = "Ln(NOx) Distributions by Day of the Week")

kable(cal_nox_clean %>%
        filter(parameter_name ==  "Oxides of nitrogen (NOx)") %>%
  group_by(dow) %>%
  summarise(
    n = sum(!is.na(sample_measurement)),
    AM = mean(sample_measurement, na.rm = TRUE),
    ASD = sd(sample_measurement, na.rm = TRUE)),
  digit = 1,
  caption = "Table 3.0: Day of the Week Summary of NOX - Native Scale ") %>%
  kable_styling()
```

**DAILY Summary of NOx**

```{r weekdays}

#summarizing by Day of the Week
cal_nox_clean<- cal_nox_clean %>%
  mutate(weekday2 = as.character(weekday) %>%
           str_replace(pattern = "TRUE",
                       replacement = "Weekday") %>%
         str_replace(pattern = "FALSE",
                     replacement = "Weekend"), .after = weekday)

cal_nox_clean %>% filter(parameter_name == "Oxides of nitrogen (NOx)") %>% ggplot(aes(x = as.factor(weekday2), y = log_conc)) +
         geom_boxplot() +
  ggtitle( label = " Weekly Ln(NOx) Distributions")

kable(cal_nox_clean %>%
        filter(parameter_name ==  "Oxides of nitrogen (NOx)") %>%
  group_by(weekday2) %>%
  summarise(
    n = sum(!is.na(sample_measurement)),
    AM = mean(sample_measurement, na.rm = TRUE),
    ASD = sd(sample_measurement, na.rm = TRUE)),
  digit = 1,
  caption = "Table 3.0: Week Summary of NOX - Native Scale ") %>%
  kable_styling()

```



**Seasonal Stratification and Summary of NOx**

``` {r seasonal strat}

#summarizing by season
#focusing on NOx - all species

unique(cal_nox_clean$season)

cal_nox_clean %>% filter(parameter_name == "Oxides of nitrogen (NOx)") %>% ggplot(aes(x = season, y = log_conc)) +
         geom_boxplot()+
  ggtitle( label = "NOx Distributions by Season")


kable(cal_nox_clean %>%
        filter(parameter_name ==  "Oxides of nitrogen (NOx)") %>%
  group_by(season) %>%
  summarise(
    n = sum(!is.na(sample_measurement)),
    AM = mean(sample_measurement, na.rm = TRUE),
    ASD = sd(sample_measurement, na.rm = TRUE)),
  digit = 1,
  caption = "Table 3.0: Seasonal Summary of NOx - Native Scale ") %>%
  kable_styling()
#can we report an arithmetic mean on a log-transformed variable?
# I think so - I think we call it the mean log concentration...


```




``` {r} 
# creating a formula with temporal variables
cal_nox_clean <- cal_nox_clean %>%      #filter to only one species category
  filter(parameter_name == "Oxides of nitrogen (NOx)") %>%
  mutate(dow = as.factor(dow),
         weekday2 = as.factor(weekday2))

#CALLAN CHANGED: taking out dow and leaving just weekday2 to prevent overfitting. adding in a sensitivity analysis with dow below. 
Full <- as.formula(log_conc ~ day_time + weekday2 + season )

```


```{r}
# linear regression model of full model (weekday2)

full_temporal <- lm(Full, data = cal_nox_clean)

#---spatial distribution---
cal_nox_clean %>%
  group_by(County) %>%
  summarise(
             n = sum(!is.na(County)))

cal_nox_clean %>%
  group_by(County) %>%
  summarise(
             n = sum(!is.na(County)))


cal_nox_clean %>%
  group_by(County) %>%
  summarise(
             n = length(unique(native_id)))

#26 total counties. use this as the clustering variable? Code below makes random clusters but this could be a clustering variable

```
This plot allows you to see trends in the data without artificially stratifying the data by season, day, or hour.


# Regression for Prediction: Seasonal Models

1. create cross validation functions
2. cross validation for out-of-sample statistics
  for summer, fall, winter, and spring models
3. select best-fit model 
  for summer, fall, winter, and spring models
  
```{r, echo = FALSE, message = FALSE}

#-----manual 10-fold CV---------------------------------------------------
#--generate random groups--

# set the seed to make reproducible
set.seed(283)

# create vector of CV groups
  # first generate a vector with values 1:10 equal to the number of dataset rows
  # then randomize (with argument replace = FALSE)
CV_grp <- rep(1:10, length.out = nrow(cal_nox_clean)) %>% 
  sample(replace = FALSE)

# now append it to the cal_nox_clean data frame
cal_nox_clean <- mutate(cal_nox_clean, CV_grp = CV_grp)

# create a numeric variable for CV predictions (using -999 as a placeholder)
cv_pred <- cal_nox_clean %>% mutate(preds = -999)

# loop over the 10 clusters
for (i in 1:10){

    # define the current cluster variable as a logical vector
    is_cluster <- cv_pred$CV_grp == i

    # fit the "common" model to the training set by omitting cluster i
    CV_lm <- lm(Full, data = cv_pred, subset = !is_cluster)

    # generate predictions using CV_lm
    preds <- predict(CV_lm, cal_nox_clean)

    # add results to cv_pred dataframe
    cv_pred$preds[is_cluster] <- preds[is_cluster]
}

# now calculate the MSE, RMSE, MSE-based R2

# mean of observations
log_conc_avg <- mean(cv_pred$log_conc)

# MSE of predictions
MSE_pred <- mean((cv_pred$log_conc - cv_pred$preds)^2)

# MSE of observations (for R2 denominator)
MSE_obs <- mean((cv_pred$log_conc - log_conc_avg)^2)

# print the results not rounded
kable(rbind(paste("RMSE:  ", round(sqrt(MSE_pred),3)),
paste("MSE-based R2:  ", round(max(1 - MSE_pred/MSE_obs, 0),3))),
  caption = "10-fold CV with random groups") %>%
  kable_styling()


```
#These are not very good performance statistics!! Do we want to try and figure out a different model with some geographic covariates?

# ANOVA components of Variance analysis
Now lets look to see what covariates explain the most temporal variation


```{r}
#-----VCA implementation-----

# different ways to estimate variance components
fit_MOM <- anovaVCA(Full, Data = as.data.frame(cal_nox_clean) )
kable(fit_MOM$aov.tab,
      caption = "Table of MOM ANOVA",
      digits = 2) %>%
  kable_styling()

#fit_REML <- fitVCA(Full, Data = as.data.frame(cal_nox_clean), method = "REML")
#kable(fit_REML$aov.tab,
#      caption = "Table of REML ANOVA",
#      digits = 2) %>%
#  kable_styling()


```


# SENSITIVITY ANALYSIS: DAY OF THE WEEK 

```{r full model sensitivity analysis}

#creating a formula to check that the simpler version of weekday vs. weekend (rather than day of the week) is appropriate in our model: 

Full2 <-as.formula(log_conc~day_time + dow + season)

```

```{r linear regression model for dow sensitivity analysis}
#creating linear regression model for model with day of the week rather than weekend vs. weekday

full_temporal2 <- lm(Full2, data = cal_nox_clean)

```

```{r comparison of dow vs. weekend or weekday models}

#model with weekday2 covariate: 
summary(full_temporal)


#model with dow covariate: 
summary(full_temporal2)

#these appear reasonably comparable, though notably (and predictably) the day of the week model is more complex than the weekend/weekday model

#calculate AIC for both models

AIC(full_temporal)

AIC(full_temporal2)


#AIC looks to be considerably lower for the full DOW model than for the simpler model. Let's check BIC: 

BIC(full_temporal)

BIC(full_temporal2)

#BIC also looks better for the full DOW model. 


#let's check the deviance for both models:
deviance(full_temporal)

deviance(full_temporal2)

#deviance also looks better for full DOW model. 


#NOTE: could make a table with all these values (plus rmse and r2) comparing the model results. will need to decide how to balance complexity and performance. 
```

```{r repeat cross-validation for sensitivity analysis}
 
```
```{r anova for sensitivity analysis}

#-----VCA implementation-----

# different ways to estimate variance components
fit_MOM <- anovaVCA(Full2, Data = as.data.frame(cal_nox_clean) )
kable(fit_MOM$aov.tab,
      caption = "Table of MOM ANOVA",
      digits = 2) %>%
  kable_styling()


```

``` {r sensitivity analysis 2: cross-validation by county clusters}


# create vector of CV groups by county
cal_nox_clean <- cal_nox_clean %>%
  group_by(County) %>%
  mutate(CV_grp_co = cur_group_id()) %>%
  ungroup()



# create a numeric variable for CV predictions (using -999 as a placeholder)
cv_pred1 <- cal_nox_clean %>% mutate(preds = -999)

num_counties <- length(unique(cal_nox_clean$CV_grp_co))

print(num_counties)

# loop over the 10 clusters
for (i in 1:num_counties){

    # define the current cluster variable as a logical vector
    is_cluster <- cv_pred1$CV_grp_co == i

    # fit the "common" model to the training set by omitting cluster i
    CV_lm <- lm(Full, data = cv_pred1, subset = !is_cluster)

    # generate predictions using CV_lm
    preds1 <- predict(CV_lm, cal_nox_clean)

    # add results to cv_pred dataframe
    cv_pred1$preds[is_cluster] <- preds1[is_cluster]
}

# now calculate the MSE, RMSE, MSE-based R2

# mean of observations
log_conc_avg <- mean(cv_pred1$log_conc)

# MSE of predictions
MSE_pred <- mean((cv_pred1$log_conc - cv_pred1$preds)^2)

# MSE of observations (for R2 denominator)
MSE_obs <- mean((cv_pred1$log_conc - log_conc_avg)^2)

# print the results not rounded
kable(rbind(paste("RMSE:  ", round(sqrt(MSE_pred),3)),
paste("MSE-based R2:  ", round(max(1 - MSE_pred/MSE_obs, 0),3))),
  caption = "10-fold CV with random groups") %>%
  kable_styling()



```

## Appendix 

#---------------------------Reference Code-------------------------------------#

```{r forward select, warning = FALSE, message = FALSE}

#-----model order and CV-----

# apply along length of the vector of names from forward selection
res1 <- lapply(seq_along(covars_forward), function(i){
    
    # define the formula, updated to add a term each time
    fmla <- as.formula(
        paste("ln_no2 ~ 1 + ", paste(covars_forward[seq_len(i)], collapse = "+"))) 
    
    # in-sample model and estimates
    in_model <- lm(fmla, data = summer) 
    
    # out-of sample model and estimates
    out_ests <- do_CV(data = summer, id = "stop_id", group = "location", fmla) # I don't have a great sense of what the location variable represents in this data, but there probably is some spacial value to it. Also it's way more than 10 groups, and not random. KW
    out_results <- get_MSE(out_ests$ln_no2, out_ests$cvpreds)
    
    # compile results
   tibble(n_pred = i,
           covar = covars_forward[i],
           in_RMSE = sqrt(mean(in_model$residuals^2)),
           in_R2 = summary(in_model)$r.squared,
           out_RMSE = out_results[["RMSE"]],
           out_R2 = out_results[["MSE_based_R2"]] 
           )
    
    }) %>% 
    bind_rows()

head(res1)

#length(unique(stop_data_primary$location)). # There are 309 CV groups using this method. (basically each stop location is a group as I understand it.

```

```{r bias.plots}
#-----bias-var combined plots-----


#there are 23 terms in the model. 
max(res1$out_RMSE[1:23])
min(res1$out_RMSE[1:23])

y_lim <- 0.8 #need to find what is actually useful for out data. Set at 0.8 for now just to include everything

# create temporary dataframe for plot
temp <- res1 %>% 
  
  # make long dataframe
  pivot_longer(cols = c(ends_with("_RMSE"), ends_with("_R2")), 
               names_to = "Source_Estimate", 
               values_to = "value" ) %>%
  
  # separate the "Source" column for in and out of sample
  separate(col = Source_Estimate, into = c("Source", "Estimate") ) %>% 

  # set high RMSE values to NA, then filter out these values before plotting
  mutate(value = ifelse(Estimate == "RMSE" & value > y_lim, NA, value)) %>%
  filter(!is.na(value)) 

#plot looked to be missing some stuff, so I filled it in - KW

combined_plot <- ggplot(data = temp) +
  geom_point(aes(x = n_pred, y = value, color = Source)) +
  geom_line(aes(x = n_pred, y = value, color = Source)) +
  xlab("Model Complexity (# of terms)") +
  ylab("") +
  scale_x_continuous(breaks = c(seq(0, 63, 5))) +
  facet_wrap(~ Estimate, scales = "free_y", ncol = 1, strip.position = "right") +
  theme_bw() 

#show plot
combined_plot
```
This is good to have as a template, but we should think about the CV groups as well as if we want season spefic or interaction model. 



```{r generate.random.cv.groups KW}
#-----generate groups-----

# set the seed to make reproducible
set.seed(123)

# create vector of CV groups
  # first generate a vector with values 1:10 equal to the number of dataset rows
  # then randomize (with argument replace = FALSE)
CV_grp <- rep(1:10, length.out = nrow(no2_only)) %>% 
  sample(replace = FALSE)
#Did this on the whole DF, which I think make sense if we use week_day or type_day as interaction terms in the model
#would not make sense if we end up doing seperate models for the types of days. 

# now append it to the fall data frame
no2_only <- mutate(no2_only, rando_CV_grp = CV_grp)

```

```{r}
#KW
#repeating the above selection and  cross-validation using the random groups for week_day (still using No2, but can switch out for pM2.5 later)

#-----forward selection using interaction-----#


null <- lm(ln_no2 ~ 1*day_type, data = no2_only) #I don't actually know if you can define a null model like this for interaction terms (got this warning on the results: Warning: variable 'day_type' is absent, its contrast will be ignored)


covars_all <- str_subset(names(no2_only),"pop_|int_|open_|D2|A1_|A23_|m_to_a1")

# B: now turn this into a formula for the full model in stepwise regression:
full <- as.formula(paste("ln_no2 ~ ", paste(covars_all, collapse= "+")))

# Using k=2 is comparable to standard AIC.
# Using log(n), where n is the number of observations, is comparable to BIC.
forwardreg_day <- step(null, 
                        scope = list(lower = null, upper = full), 
                        trace = 0, 
                        direction = "forward", 
                        k = 0)

covars_forward2 <- names(forwardreg_day$coefficients) %>%
  setdiff('(Intercept)')

covars_forward2 #different list than above when just run for summer

#forward selection is what we used the the lab, but she also said it wasn't how you would normally really do it. Do we want to try backwards selection or something else?

```

```{r fit for weekdays, warning = FALSE, message = FALSE}

#-----model order and CV-----

# apply along length of the vector of names from forward selection
res2 <- lapply(seq_along(covars_forward2), function(i){
    
    # define the formula, updated to add a term each time
    fmla <- as.formula(
        paste("ln_no2 ~ + ", paste(covars_forward2[seq_len(i)], collapse = "+"))) 
    
    # in-sample model and estimates
    in_model <- lm(fmla, data = no2_only) 
    
    # out-of sample model and estimates
    out_ests <- do_CV(data = no2_only, id = "location", group = "rando_CV_grp", fmla)  #changed the id to location, because that is the unique identifier for WHERE the stop is, which is what I think we want to be able to predict. STOP ID, is unique to the actual time that they stoped at that location each time around. Maybe I'm confused here, but it seems like we want to predict by location?
    out_results <- get_MSE(out_ests$ln_no2, out_ests$cvpreds)
    
    # compile results
   tibble(n_pred = i,
           covar = covars_forward2[i],
           in_RMSE = sqrt(mean(in_model$residuals^2)),
           in_R2 = summary(in_model)$r.squared,
           out_RMSE = out_results[["RMSE"]],
           out_R2 = out_results[["MSE_based_R2"]] 
           )
    
    }) %>% 
    bind_rows()

head(res2)



```

```{r bias.plots.weekday kw}
#-----bias-var combined plots-----


#there are 23 terms in the model. 
max(res2$out_RMSE[1:23])
min(res2$out_RMSE[1:23])

y_lim <- 0.8

# create temporary dataframe for plot
temp2 <- res2 %>% 
  
  # make long dataframe
  pivot_longer(cols = c(ends_with("_RMSE"), ends_with("_R2")), 
               names_to = "Source_Estimate", 
               values_to = "value" ) %>%
  
  # separate the "Source" column for in and out of sample
  separate(col = Source_Estimate, into = c("Source", "Estimate") ) %>% 

  # set high RMSE values to NA, then filter out these values before plotting
  mutate(value = ifelse(Estimate == "RMSE" & value > y_lim, NA, value)) %>%
  filter(!is.na(value)) 


combined_plot2 <- ggplot(data = temp2) +
  geom_point(aes(x = n_pred, y = value, color = Source)) +
  geom_line(aes(x = n_pred, y = value, color = Source)) +
  xlab("Model Complexity (# of terms)") +
  ylab("") +
  labs(title = "Bias-Variance TradeOff For Randomly Cross-Validated CV Groups",
       subtitle = "Type of Day Model") +
  scale_x_continuous(breaks = c(seq(0, 63, 5))) +
  facet_wrap(~ Estimate, scales = "free_y", ncol = 1, strip.position = "right") +
  theme_bw() 

#show plot
combined_plot2
```

```{r}
#KW experimenting with backwards selection


#-----backward selection-----#


null <- lm(ln_no2 ~ 1, data = no2_only) 


covars_all <- str_subset(names(no2_only),"pop_|int_|open_|D2|A1_|A23_|m_to_a1")

# B: now turn this into a formula for the full model in stepwise regression:
full <- as.formula(paste("ln_no2 ~ ", paste(covars_all, collapse= "+")))

# Fit the full model
full_model <- lm(full, data = no2_only)

# Using k=2 is comparable to standard AIC. (which is the default here)

backward_reg <- step(full_model, direction='backward', scope=formula(full_model), trace=0)

covars_backward <- names(backward_reg$coefficients) %>%
  setdiff('(Intercept)')

covars_backward 

backward_reg$anova

backward_reg$coefficients

# I don't really know much about how to interpret this. May not be that useful in its current state. 


```


```{r appendix, results='hide'}

#*Statistical Approach for regression for association*

#1. We aim to create a best-fit model of NOx/TRAP pollutant to assess the association of NOx concentrations across days of the week. 

#1b. We will characterize variation in NOx/TRAP across days of the week - within week day and between weekday

#2. We aim to create a best fit model of NOx TRAP pollutant to assess NOx concentrations across the four seasons of the year. 

#2b. We will characterize variation in NOx/TRAP across season - within and between season variability


## Methods & Statistical Approach

#Descriptive statistics

#  We will characterize the distribution of PM2.5 concentrations according to season (Fall, Winter, Spring, Summer) and day of the week using descriptive summary statistics, box plots and/or histograms. 

 # We will use ANOVA models to compare the mean concentration of log-transformed air pollutants across (1) seasons (i.e. Fall, Winter, Spring, and Summer); and (2) days of the week. 
  

#As part of our day-of-the-week assessment, we will additionally test whether TRAP is associated more broadly with day type (i.e. week day or weekend) using a land-use regression model adjusted for season and distance to major roadways, which we anticipate could be precision variables.



```

```{r}

season_summary <- stop_data_primary %>% group_by(season, location) %>% summarise(count = length(median_value))

range(season_summary$count)

day_summary <- stop_data_primary %>% group_by(day_type, location) %>% summarise(count = length(median_value))

range(day_summary$count)

weekday_summary <- stop_data_primary %>% group_by(week_day, location) %>% summarise(count = length(median_value))

range(weekday_summary$count)


```



